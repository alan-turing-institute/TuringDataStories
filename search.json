[
  {
    "objectID": "stories/2023-02-01-Overwatch-League/OWL_story.html",
    "href": "stories/2023-02-01-Overwatch-League/OWL_story.html",
    "title": "Esport predictions: Overwatch League",
    "section": "",
    "text": "Overwatch League Logo courtesy of Blizzard Entertainment."
  },
  {
    "objectID": "stories/2023-02-01-Overwatch-League/OWL_story.html#overwatch-league-owl",
    "href": "stories/2023-02-01-Overwatch-League/OWL_story.html#overwatch-league-owl",
    "title": "Esport predictions: Overwatch League",
    "section": "Overwatch League (OWL)",
    "text": "Overwatch League (OWL)\nOverwatch is an online team-based multiplayer first-person shooter (FPS) game developed and published by Blizzard Entertainment. It features different modes designed around combat between two opposing teams of six players each. It was first released in 2016 and has been highly popular among casual players, selling over 50 million copies, and as a professional esport.\nMatches between two teams consist of several games and each game is played on one of 21 possible maps. There are four different types of maps with varying objectives, such as controlling key locations on the map or capturing them from the opponent. Each match includes games with different map types, and typically the losing team gets to choose the next map. An individual game may further subdivide into rounds, depending on the map type. A game ends in a victory for one of the two teams or a draw.\nThe Overwatch League (OWL) is the highest professional esports league for Overwatch, and is owned and run by Blizzard Entertainment. The 2021 OWL featured four midseason tournaments throughout the regular season which used a point system for season playoff seeding. OWL 2021 consisted of 20 teams split into two geographical regions: North America (NA) and Asia (APAC).\n\n\n\n\n\n\nOverwatch gameplay from the player’s point of view. Image credit Blizzard Entertainment."
  },
  {
    "objectID": "stories/2023-02-01-Overwatch-League/OWL_story.html#data-story",
    "href": "stories/2023-02-01-Overwatch-League/OWL_story.html#data-story",
    "title": "Esport predictions: Overwatch League",
    "section": "Data Story",
    "text": "Data Story\nThis Data Story will look at the data produced during the OWL 2021 season to determine whether it is possible to predict the result of a game between two teams. We started writing this story mid way through the OWL 2022 season and thus chose to use the OWL 2021 data set because of its comprehensivity. Since then, OWL 2022 has concluded and a sequel - Overwatch 2 - has been released. We hope and believe this analysis will carry through to the new game given the great similarity between them.\nAs one of the authors - Tim Powell - follows the OWL, we knew that it was possible to obtain the OWL data, and a discussion took place at the SeptembRSE conference on what could be done with it. We were curious to understand which factors influence the outcome of matches and whether past performance was strongly correlated with future outcomes. The main question this data story aims to answer is: Is it possible to use a team’s historic data to predict their future performance?”"
  },
  {
    "objectID": "stories/2023-02-01-Overwatch-League/OWL_story.html#outline",
    "href": "stories/2023-02-01-Overwatch-League/OWL_story.html#outline",
    "title": "Esport predictions: Overwatch League",
    "section": "Outline",
    "text": "Outline\nThe story is divided into the following parts.\n\nPart 1. Data ingestion\n\nGetting the source data\n\nPart 2. Data cleaning\n\nRefining and filtering the data to the scope of our analysis\n\nPart 3. Initial data exploration\n\nTeams’ win rates per map\nVisualisation\nTeams’ records against each other per map\n\nPart 4. A benchmark predictor\n\nThe pure guess predictor\nA framework for training and testing models\n\nPart 5. Higher win rate predictor\n\nPredicting winners based on past history on the given map\n\nPart 6. Skill based predictor - Elo ratings\n\nThe Elo rating system\nPredicting winners based on Elo ratings\nCombining Elo ratings and map win rates for better predictions?\n\nPart 7. Conclusions\n\nWhat we learned\nWhat we would like to do in the future\n\n\n # Part 1. Data ingestion\n\nData source\nThe official OWL website includes a stats tab that contains various data on players, heroes (characters players can choose), and matches. For this analysis, we will be using the match data as it includes the results of match ups between different teams in the league. Blizzard provides the data for anyone to analyse, but unfortunately it does not come with an explicit free data license that would allow us to redistribute it.\n\n\nIngestion\nWe start by importing all packages we’ll need in the whole story. This includes some common Python libraries to do the following: - data manipulation (pandas, numpy) - data extraction (zipfile, requests) - data visualisation (pyplot, seaborn) - machine learning and statistical modelling (sklearn) - notebook widgets (widgets, IPython.display) - utilities (io, itertools, random)\n\nimport io\nimport itertools\nimport random\nimport requests\nfrom zipfile import ZipFile\n\nfrom IPython.display import clear_output\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import svm, linear_model, preprocessing\n\nWe download a zip file containing all of the match data over the last few years, and then ingest this as a pandas dataframe, the usual python object for holding tabular data, provided by the pandas data science package.\n\n# Download zip file and unzip\nurl = \"https://assets.blz-contentstack.com/v3/assets/blt321317473c90505c/bltb4a6fe3cc2efaa02/634732b68cdace44d7b8efc4/2022_Week24_match_map_stats.zip\"\nr = requests.get(url)\nwith ZipFile(io.BytesIO(r.content)) as z:\n    first_file = z.namelist()[0]\n    with z.open(first_file) as f:\n        content = f.read()\n\nHere’s what the raw data looks like.\n\n# Convert files to csv and a pandas dataframe\ndata = io.StringIO(str(content, \"utf-8\"))\ndf = pd.read_csv(data)\ndf\n\n\n\n\n\n\n\n\nround_start_time\nround_end_time\nstage\nmatch_id\ngame_number\nmatch_winner\nmap_winner\nmap_loser\nmap_name\nmap_round\n...\nteam_one_name\nteam_two_name\nattacker_payload_distance\ndefender_payload_distance\nattacker_time_banked\ndefender_time_banked\nattacker_control_perecent\ndefender_control_perecent\nattacker_round_end_score\ndefender_round_end_score\n\n\n\n\n0\n01/11/18 00:12\n01/11/18 00:20\n2018: Stage 1\n10223\n1\nLos Angeles Valiant\nLos Angeles Valiant\nSan Francisco Shock\nDorado\n1\n...\nLos Angeles Valiant\nSan Francisco Shock\n75.615050\n0.000000\n0.000000\n240.00000\nNaN\nNaN\n2\n0\n\n\n1\n01/11/18 00:22\n01/11/18 00:27\n2018: Stage 1\n10223\n1\nLos Angeles Valiant\nLos Angeles Valiant\nSan Francisco Shock\nDorado\n2\n...\nLos Angeles Valiant\nSan Francisco Shock\n75.649600\n75.615050\n125.750570\n0.00000\nNaN\nNaN\n3\n2\n\n\n2\n01/11/18 00:34\n01/11/18 00:38\n2018: Stage 1\n10223\n2\nLos Angeles Valiant\nLos Angeles Valiant\nSan Francisco Shock\nTemple of Anubis\n1\n...\nLos Angeles Valiant\nSan Francisco Shock\n0.000000\n0.000000\n250.492000\n240.00000\nNaN\nNaN\n2\n0\n\n\n3\n01/11/18 00:40\n01/11/18 00:44\n2018: Stage 1\n10223\n2\nLos Angeles Valiant\nLos Angeles Valiant\nSan Francisco Shock\nTemple of Anubis\n2\n...\nLos Angeles Valiant\nSan Francisco Shock\n0.000000\n0.000000\n225.789030\n250.49200\nNaN\nNaN\n2\n2\n\n\n4\n01/11/18 00:46\n01/11/18 00:49\n2018: Stage 1\n10223\n2\nLos Angeles Valiant\nLos Angeles Valiant\nSan Francisco Shock\nTemple of Anubis\n3\n...\nLos Angeles Valiant\nSan Francisco Shock\n0.000000\n0.000000\n36.396057\n250.49200\nNaN\nNaN\n4\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13596\n10/09/22 22:47\n10/09/22 22:53\n2022: Countdown Cup: Qualifiers\n39321\n2\nLos Angeles Gladiators\nLos Angeles Gladiators\nBoston Uprising\nParaíso\n1\n...\nBoston Uprising\nLos Angeles Gladiators\n92.450140\n0.000000\n114.112010\n0.00000\nNaN\nNaN\n3\n0\n\n\n13597\n10/09/22 22:55\n10/09/22 22:59\n2022: Countdown Cup: Qualifiers\n39321\n2\nLos Angeles Gladiators\nLos Angeles Gladiators\nBoston Uprising\nParaíso\n2\n...\nBoston Uprising\nLos Angeles Gladiators\n0.000000\n92.450140\n0.000000\n114.11201\nNaN\nNaN\n0\n3\n\n\n13598\n10/09/22 23:07\n10/09/22 23:12\n2022: Countdown Cup: Qualifiers\n39321\n3\nLos Angeles Gladiators\nBoston Uprising\nLos Angeles Gladiators\nDorado\n1\n...\nBoston Uprising\nLos Angeles Gladiators\n68.543530\n0.000000\n0.000000\n0.00000\nNaN\nNaN\n0\n0\n\n\n13599\n10/09/22 23:13\n10/09/22 23:15\n2022: Countdown Cup: Qualifiers\n39321\n3\nLos Angeles Gladiators\nBoston Uprising\nLos Angeles Gladiators\nDorado\n2\n...\nBoston Uprising\nLos Angeles Gladiators\n68.549540\n68.543530\n137.413010\n0.00000\nNaN\nNaN\n1\n0\n\n\n13600\n10/09/22 23:25\n10/09/22 23:35\n2022: Countdown Cup: Qualifiers\n39321\n4\nLos Angeles Gladiators\nLos Angeles Gladiators\nBoston Uprising\nColosseo\n1\n...\nLos Angeles Gladiators\nBoston Uprising\n55.379028\n73.830414\n0.000000\n0.00000\nNaN\nNaN\n0\n1\n\n\n\n\n13601 rows × 25 columns\n\n\n\nEach row is a round in an Overwatch match. For instance, on the first row we can see the first round of the first game in a match between Los Angeles Valiant and San Francisco Shock, played on the map Dorado. Valiant went on to win both the game (called map_winner in the data) and the match. NaNs (not-a-number) mark missing values.\n # Part 2. Data cleaning\nNext we reduce the data to only the information that we need.\nLet’s start with the ‘Stage’ column, allowing us to filter based on the Overwatch season.\n\ndf[\"stage\"].unique()\n\narray(['2018: Stage 1', '2018: Stage 1 Title Matches', '2018: Stage 2',\n       '2018: Stage 2 Title Matches', '2018: Stage 3',\n       '2018: Stage 3 Title Matches', '2018: Stage 4',\n       '2018: Stage 4 Title Matches', '2018: Championship',\n       '2019: Stage 1', '2019: Stage 1 Title Matches', '2019: Stage 2',\n       '2019: Stage 2 Title Matches', '2019: Stage 3',\n       '2019: Stage 3 Title Matches', '2019: Stage 4',\n       '2019: Postseason Play-in', '2019: Playoffs & Grand Finals',\n       '2020: Regular Season', '2020: May Melee: North America Knockouts',\n       '2020: May Melee: Asia', '2020: May Melee: North America',\n       '2020: Summer Showdown: North America Knockouts',\n       '2020: Summer Showdown: Asia',\n       '2020: Summer Showdown: North America',\n       '2020: Countdown Cup: North America Knockouts',\n       '2020: Countdown Cup: Asia', '2020: Countdown Cup: North America',\n       '2020: North America Playoffs', '2020: Asia Playoffs',\n       '2020: Grand Finals', '2021: May Melee: Qualifiers',\n       '2021: May Melee: Tournament', '2021: June Joust: Qualifiers',\n       '2021: June Joust: Tournament',\n       '2021: Summer Showdown: Qualifiers',\n       '2021: Summer Showdown: Tournament',\n       '2021: Countdown Cup: Qualifiers',\n       '2021: Countdown Cup: Tournament', '2021: Postseason',\n       '2022: Kickoff Clash: Qualifiers',\n       '2022: Kickoff Clash: Tournament',\n       '2022: Midseason Madness: Qualifiers',\n       '2022: Midseason Madness: Tournament',\n       '2022: Summer Showdown: Qualifiers',\n       '2022: Summer Showdown: Tournament',\n       '2022: Countdown Cup: Qualifiers'], dtype=object)\n\n\nFor this analysis we’re going to use the 2021 season, as it was the most recent, complete data set at the time of writing. Focussing on a single season is useful to avoid having to think too much about changes in team rosters: typically players remain in a team for the entire season, but between seasons many players move teams. Trying to analyse the effect of such transfers would be very interesting, but with limited time, we focus on the team-level analysis, and thus constrain ourselves to a single season.\nThe columns of the dataframe include plenty of information we don’t care about: Recall that each match subdivides into games, which subdivide into rounds. We will keep our analysis on the level of games (which are always played on a single map), and thus can leave out all information about who won the match, or about the individual round, such as how far the payload (an object fought over on some map types) progressed.\n\n# Reduce data to only the OWL 2021 stage and only the relevant columns\nkept_columns = [\n    \"match_id\",\n    \"game_number\",\n    \"map_round\",\n    \"map_winner\",\n    \"map_loser\",\n    \"team_one_name\",\n    \"team_two_name\",\n    \"map_name\",\n    \"round_start_time\",\n]\n# We also encourage the reader to modify the string to run the analysis on a \n# different year.\nowl21_reduced = df.loc[df[\"stage\"].str.contains(\"2021\"), kept_columns] \nowl21_reduced\n\n\n\n\n\n\n\n\nmatch_id\ngame_number\nmap_round\nmap_winner\nmap_loser\nteam_one_name\nteam_two_name\nmap_name\nround_start_time\n\n\n\n\n9071\n37234\n1\n1\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nBusan\n04/16/21 19:08\n\n\n9072\n37234\n1\n2\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nBusan\n04/16/21 19:12\n\n\n9073\n37234\n1\n3\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nBusan\n04/16/21 19:18\n\n\n9074\n37234\n2\n1\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nKing's Row\n04/16/21 19:30\n\n\n9075\n37234\n2\n2\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nKing's Row\n04/16/21 19:39\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11220\n37441\n3\n2\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nKing's Row\n09/26/21 01:57\n\n\n11221\n37441\n3\n3\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nKing's Row\n09/26/21 02:05\n\n\n11222\n37441\n3\n4\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nKing's Row\n09/26/21 02:09\n\n\n11223\n37441\n4\n1\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHavana\n09/26/21 02:53\n\n\n11224\n37441\n4\n2\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHavana\n09/26/21 03:03\n\n\n\n\n2154 rows × 9 columns\n\n\n\nFor the purpose of this analysis round start times are not required except for the purpose of sorting the rounds to be chronologically ordeded. Note also that there are multiple rows for each game, because there are typically multiple rounds. As we are only considering game level results, round results are removed from the data. We then reset the index to have continuous numbering of the games starting from 0.\n\nowl21_reduced = owl21_reduced.sort_values(\"round_start_time\")\nkept_columns = [\n    \"match_id\",\n    \"map_winner\",\n    \"map_loser\",\n    \"team_one_name\",\n    \"team_two_name\",\n    \"map_name\",\n]\nowl21_reduced = (\n    owl21_reduced[kept_columns].drop_duplicates().reset_index(drop=True)\n)\nowl21_reduced\n\n\n\n\n\n\n\n\nmatch_id\nmap_winner\nmap_loser\nteam_one_name\nteam_two_name\nmap_name\n\n\n\n\n0\n37234\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nBusan\n\n\n1\n37234\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nKing's Row\n\n\n2\n37234\nHouston Outlaws\nDallas Fuel\nDallas Fuel\nHouston Outlaws\nHavana\n\n\n3\n37234\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nVolskaya Industries\n\n\n4\n37234\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nIlios\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n889\n37442\nAtlanta Reign\nDallas Fuel\nDallas Fuel\nAtlanta Reign\nDorado\n\n\n890\n37441\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nIlios\n\n\n891\n37441\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHanamura\n\n\n892\n37441\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nKing's Row\n\n\n893\n37441\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHavana\n\n\n\n\n894 rows × 6 columns\n\n\n\nWe are left with a data set of 894 games to study.\n # Part 3. Initial data exploration\nOur eventual goal is to predict the outcomes of future games based on past history of the teams involved. For example, a predictor should be able to use information from the first 200 games to predict the result of the 201st game.\nBefore we do that however, let’s start with exploring the data to get an initial idea of what it looks like and how teams perform. We start by collecting lists of all maps and teams.\n\n# Get list of maps\nmap_list = owl21_reduced[\"map_name\"].unique()\nmap_list\n\narray(['Busan', \"King's Row\", 'Havana', 'Volskaya Industries', 'Ilios',\n       'Eichenwalde', 'Watchpoint: Gibraltar', 'Hanamura',\n       'Lijiang Tower', 'Blizzard World', 'Dorado', 'Temple of Anubis',\n       'Oasis', 'Nepal', 'Numbani', 'Rialto', 'Hollywood', 'Junkertown',\n       'Route 66'], dtype=object)\n\n\n\n# Get list of teams\nteam_winners = set(owl21_reduced[\"map_winner\"])\nteam_losers = set(owl21_reduced[\"map_loser\"])\nteam_names = list(team_winners | team_losers)\nteam_names.remove(\"draw\")\nteam_names\n\n['Los Angeles Valiant',\n 'Shanghai Dragons',\n 'Guangzhou Charge',\n 'Los Angeles Gladiators',\n 'London Spitfire',\n 'Florida Mayhem',\n 'Boston Uprising',\n 'Vancouver Titans',\n 'Philadelphia Fusion',\n 'Chengdu Hunters',\n 'San Francisco Shock',\n 'New York Excelsior',\n 'Atlanta Reign',\n 'Toronto Defiant',\n 'Washington Justice',\n 'Hangzhou Spark',\n 'Paris Eternal',\n 'Dallas Fuel',\n 'Seoul Dynasty',\n 'Houston Outlaws']\n\n\nTo get an idea of the performance of various teams, we compute their win rate for each map: The number of times they’ve won on that map, divided by the number of times they’ve played the map.\n\ndef map_data(team_list, map_list, df):\n    \"\"\"Collect stats on wins, losses, and draws per map and per team.\"\"\"\n    # make dataframe\n    column_names = (\"team_name\", \"map\", \"win_%\", \"win\", \"draw\", \"lose\")\n    team_map_data_df = pd.DataFrame(columns=column_names)\n\n    # iterate through teams\n    for team in team_list:\n        # iterate through maps\n        for map_name in map_list:\n            # filter to specific team and map\n            map_filter = df[\"map_name\"] == map_name\n            team_filter = (df[\"team_one_name\"] == team) | (df[\"team_two_name\"] == team)\n            team_map_df = df[map_filter & team_filter]\n\n            # calculate data and add to list\n            num_win = (team_map_df.map_winner == team).sum()\n            num_lose = (team_map_df.map_loser == team).sum()\n            num_total = len(team_map_df)\n            num_draw = num_total - num_win - num_lose\n            win_rate = num_win / num_total if num_total &gt; 0 else np.nan\n            map_data_list = [team, map_name, round(win_rate, 4), num_win, num_draw, num_lose]\n\n            # append data list to dataframe\n            team_map_data_df.loc[len(team_map_data_df)] = map_data_list\n\n    team_map_data_df[\"games_played\"] = team_map_data_df[[\"win\", \"draw\", \"lose\"]].sum(\n        axis=1\n    )\n    return team_map_data_df\n\n\nall_teams_map_data = map_data(team_names, map_list, owl21_reduced)\nall_teams_map_data\n\n\n\n\n\n\n\n\nteam_name\nmap\nwin_%\nwin\ndraw\nlose\ngames_played\n\n\n\n\n0\nLos Angeles Valiant\nBusan\n0.0000\n0\n0\n2\n2\n\n\n1\nLos Angeles Valiant\nKing's Row\n0.3333\n1\n0\n2\n3\n\n\n2\nLos Angeles Valiant\nHavana\n0.0000\n0\n0\n2\n2\n\n\n3\nLos Angeles Valiant\nVolskaya Industries\n0.0000\n0\n1\n4\n5\n\n\n4\nLos Angeles Valiant\nIlios\n0.0000\n0\n0\n4\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n375\nHouston Outlaws\nNumbani\n0.5000\n1\n0\n1\n2\n\n\n376\nHouston Outlaws\nRialto\n0.5000\n1\n0\n1\n2\n\n\n377\nHouston Outlaws\nHollywood\n0.0000\n0\n0\n2\n2\n\n\n378\nHouston Outlaws\nJunkertown\n0.5000\n1\n0\n1\n2\n\n\n379\nHouston Outlaws\nRoute 66\n1.0000\n2\n0\n0\n2\n\n\n\n\n380 rows × 7 columns\n\n\n\nLet’s see what the distribution of win rates looks like over maps and teams. The win rate isn’t very meaningful if the team has only played the map a handful of times, so we also set a cut-off, where we only consider team-map combinations that occur at least four times during the season\n\ndef plot_win_rate_distribution(owl21_reduced, all_teams_map_data, min_games=4):\n    \"\"\"Plot a heatmap of win rates by team and map.\"\"\"\n    filtered_all_teams_map_data = all_teams_map_data.copy()\n    filtered_all_teams_map_data[\"win_%\"] = filtered_all_teams_map_data[\"win_%\"].where(\n        all_teams_map_data[\"games_played\"] &gt;= min_games, np.nan\n    )\n    # Visualise win rates of teams vs maps\n    # The pivot call turns the values in the map column into individual\n    # columns, one for each map.\n    team_map_matrix = filtered_all_teams_map_data.set_index(\"team_name\").pivot(\n        columns=\"map\"\n    )[\"win_%\"]\n    # Sort teams by total wins and maps by total match count.\n    team_order = owl21_reduced[\"map_winner\"].value_counts().drop(\"draw\")\n    map_order = owl21_reduced[\"map_name\"].value_counts()\n    team_map_matrix = team_map_matrix.loc[team_order.index, map_order.index]\n    ax = sns.heatmap(team_map_matrix, vmin=0, vmax=1, square=True, cmap=\"viridis\")\n    ax.set_xlabel('Map name')\n    ax.set_ylabel('Team name')\n    cbar = ax.collections[0].colorbar\n    cbar.set_label('Win rate')\n\n\nplot_win_rate_distribution(owl21_reduced, all_teams_map_data)\n\n\n\n\nOn the vertical axis here are the teams, ordered by total number of wins over the season. On the horizontal axis are maps, ordered by how many times they were played during the season. The colours code the win rates.\nSome observations: * Due to the season being quite short, there are many teams that didn’t play certain maps more than 4 times. This also varies between teams, because the good teams get to play more matches, and thus have fewer maps for which they have little data. This lack of data will cause us problems later. * The good teams seem to be good on almost all maps, the bad teams are bad on almost all maps. This suggests that there is not much map specialisation. * There are some exceptions to the above point. E.g. Dallas Fuel is the second best team by total win count, but has a very low win rate on Route 66. Conversely, Boston Uprising is one of the worst teams, but has a very good record on Ilios. * Some maps are much more popular than others. Almost all teams have played more than four game on Temple of Anubis, Volskaya Industries, and Hanamura, whereas only three teams have more than four games on Hollywood.\n\n\n\nA top-down schematic of one of the most popular maps, Temple of Anubis. This is an Assault type map, where the attacking team starts at the bottom right and tries to capture the two capture points, A and B, from the defenders. Image made by statbanana.\n\n\n\nIf you’re curious about a particular team’s performance, you can use the below widget to browse how they did on each map.\n\nout = widgets.Output()\n\n# See https://ipywidgets.readthedocs.io/en/latest/\n# for what widgets are.\ndef on_team_selection(change):\n    with out:\n        clear_output()\n        team = team_selector.value\n        team_filter = all_teams_map_data[\"team_name\"] == change[\"new\"]\n        print(all_teams_map_data[team_filter].to_string(index=False))\n\n\nteam_selector = widgets.Dropdown(\n    options=team_names, value=None, description=\"Team Name\"\n)\ndisplay(team_selector)\ndisplay(out)\nteam_selector.observe(on_team_selection, names=\"value\")\n\n\n\n\n\n\n\n # Part 4. A benchmark predictor\nOur goal is to create various predictors - models that can take in historical data to predict the result of a future game.\nTo assess the performance of our models fairly we split our data set into training and test sets. We choose an unusually large fraction, the last 50% of our data, to be the test set. This is because our models will require very little training, as we will see.\nWe considered using a separate validation set as well, that wouldn’t be used when comparing various models, but only used to check the final performance of our chosen best model at the very end. This would guard against overfitting in model selection and hyperparameter tuning. However, since we’ll be doing very little hyperparameter tuning, and will only deal with a handful of simple models, we chose against it. This helps make the most of our quite small data set.\nIn addition to guarding against overfitting as usual, the train/test split serves another purpose for us: our data is time series data, and our goal is to predict games late in the season, based on what we learned earlier in the season. By testing all our models on the test set that is the latter half of the season we ensure we don’t do something silly, like try to “predict” the first games based on what we learned from the last ones.\n\n# Set up test-training data sets.\ntest_fraction = 0.5\ntrain_fraction = 1 - test_fraction\nn_games = len(owl21_reduced)\nn_train = int(np.round(n_games * train_fraction))\ntrain_data = owl21_reduced.iloc[:n_train, :]\ntest_data = owl21_reduced.iloc[n_train:, :]\nn_test = len(test_data)\n\n\n\nFramework for evaluating predictors\nWe also set up a framework for evaluating different models and for defining predictors. This helps to reduce code duplication, and gives a neat interface for testing model performance.\n\ndef _get_model_rate(model, actual_winners, predictors):\n    \"\"\"Compute the success rate of a predictor model.\"\"\"\n    predicted_results = model.predict(predictors)\n    predicted_winners = predicted_results.loc[:, \"map_winner\"]\n    correct_predictions = (predicted_winners == actual_winners).sum()\n    rate = correct_predictions / len(actual_winners)\n    return rate\n\n\ndef train_and_test(train_data, test_data, model_class, *args, **kwargs):\n    \"\"\"Train and test a model of a given class.\n\n    The `model_class` argument should be a class with two methods with signatures\n    `train(self, train_data)` and `predict(self, predictors)`. `train` should return\n    `None` and modify the model object in-place to do the training.  `predict` should\n    return a DataFrame with the same index as `predictors`, and with a column\n    `\"map_winner\"` that includes the predictions for each game's winner.\n\n    Args:\n      train_data: Training data set.\n      test_data: Test data set to test model performance on.\n      model_class: A class with `train` and `predict` methods as described above.\n      *args, **kwargs: Additional arguments are passed to the constructor of\n        `model_class`. These could be e.g. parameters for the model\n\n    Returns:\n      A dictionary with the following keys:\n      test_rate: The proportion of games the model predicted correctly in the test set.\n      train_rate: The proportion of games the model predicted correctly in the training\n          set.\n      model: The trained model.\n    \"\"\"\n    model = model_class(*args, **kwargs)\n    model.train(train_data)\n    test_predictors = test_data.drop(\n        columns=[\"map_winner\", \"map_loser\"],\n        errors=\"ignore\",\n    )\n    test_winners = test_data.loc[:, \"map_winner\"]\n    test_rate = _get_model_rate(model, test_winners, test_predictors)\n    train_predictors = train_data.drop(\n        columns=[\"map_winner\", \"map_loser\"],\n        errors=\"ignore\",\n    )\n    train_winners = train_data.loc[:, \"map_winner\"]\n    train_rate = _get_model_rate(model, train_winners, train_predictors)\n    return {\"test rate\": test_rate, \"train rate\": train_rate, \"model\": model}\n\n\n\nGuessing predictors\nTo get a sense of how good our predictors are, it is useful to first develop a benchmark predictor. We start with the simplest predictor imaginable: random guessing. We should expect that our actual models will be much more accurate than this one, and that something is going wrong if they are not.\nGiven that we know the result of each game can either be a ‘team 1 wins’, ‘team 2 wins’, or ‘draw’, we can create a benchmark predictor that simply selects one of the above at random as the result of each game.\n\nclass PureRandomModel:\n    \"\"\"A model class that predicts the outcome of a game purely at random.\"\"\"\n\n    def train(self, train_data):\n        \"\"\"training does nothing as it is all random!\"\"\"\n        return None\n\n    def _random_predictions(self, team_one, team_two, map_name):\n        choices = [team_one, team_two, \"draw\"]\n        victor = random.choice(choices)\n        return victor\n\n    def predict(self, predictors):\n        \"\"\"Guess the outcome at random.\"\"\"\n        predictors[\"map_winner\"] = predictors.apply(\n            lambda x: self._random_predictions(\n                x[\"team_one_name\"], x[\"team_two_name\"], x[\"map_name\"]\n            ),\n            axis=1,\n        )\n        return predictors\n\n\ntrain_and_test(train_data, test_data, PureRandomModel)\n\n{'test rate': 0.31543624161073824,\n 'train rate': 0.3042505592841163,\n 'model': &lt;__main__.PureRandomModel at 0x17f8af220&gt;}\n\n\nTry running the above cell a few times - you’ll find that the accuracy for the test and training data seems to fluctuate a lot. This is because each time we run the cell, our model guesses randomly, and the data set isn’t quite large enough for the law of large numbers to kick in with force.\nThe main thing to note is how inaccurate the guessing model is. This is because the pure random distribition is unrealistic - very few games are actually drawn meaning a predictor that guesses a draw 1 in 3 times does not really come close. We retrospectively know that only around 2% of maps were drawn in the 2021 season.\nWe could improve on this by randomly picking between team 1 or team 2 winning, and never predict a draw, although this requires prior knowledge about Overwatch which we were trying to avoid with our simplest imaginable predictor. We could also count the actual proportion of draws in the data and weigh the probabilities proportionally, but this would use future knowledge: the predictor would somehow be using knowledge that there are, say 10 draws by the end of season, to predict whether a match in the middle of the season is a draw. Alternatively we could use the proportion of draws in the previous season.\nFor brevity we won’t provide code for the above predictors, but we encourage the reader to experiment with the notebook version of this story and try and write the above predictors themselves. For example, to try the first option you just need to remove 'draw' from victors. This leads to approximately 48% accuracy, which can be considered a lower bound for the modelling we do next: If our accuracy isn’t significantly better than that, we aren’t doing anything worthwhile.\n # Part 5. Higher win rate predictor\nNow let’s look at something marginally more intelligent by using prior data. Here we create a model that looks at previous wins on a given map: When predicting who will win between team A and team B on map C, we look at team A’s win rate on map C and compare it to team B’s win rate on map C. Whoever has the higher win rate will be predicted to win this game.\nNote that we could also consider the previous history of team A playing against team B in particular. We tried this approach but found that our data set was too small for that to yield interesting results. Hence we focus on each team’s win rate individually.\nWe have almost all the data we need for such a predictor in our all_teams_map_data dataframe that we used earlier for the exploratory plotting: We have the win rate for each team and map. However, we are dealing with time series data, progressing over the season, so we have to be a bit careful: We don’t want to use win rates based on the whole season, including the end part of it, to make predictions on the matches early in the season. Instead, we need the map win rates to be rolling, i.e. we need to know the win rate of each team on each map at each point during the season. We compute that below.\n\n# If the team hasn't played this map we set its win rate to be 0.5. This is a type of\n# uninformed prior.\nNO_INFO_PRIOR = 0.5\n\n\ndef _compute_win_rate(row, team_name, map_name):\n    wins = row.loc[[(team_name, map_name, \"wins\")]].iloc[0]\n    losses = row.loc[[(team_name, map_name, \"losses\")]].iloc[0]\n    num_played = wins + losses\n    rate = wins / num_played if num_played &gt; 0 else NO_INFO_PRIOR\n    return rate\n\n\ndef rolling_map_rates(df_full, team_names, map_names):\n    \"\"\"Make a dataframe of rolling per map, per team win rates.\"\"\"\n    columns_to_copy = [\n        \"match_id\",\n        \"map_name\",\n        \"map_winner\",\n        \"map_loser\",\n        \"team_one_name\",\n        \"team_two_name\",\n    ]\n    df = df_full.loc[:, columns_to_copy].copy()\n    N_games = len(df)\n    # We need a column for each 3-tuple of team, map, and win/loss, counting how many\n    # times that team has won/lost on that map, up to the point in the season indexed by\n    # the rows. If this seems a bit confusing, seeing what the output looks like below\n    # may clarify.\n    team_map_tuples = list(itertools.product(team_names, map_names, [\"wins\", \"losses\"]))\n    initial_column = pd.Series([np.nan] * N_games, index=df.index, dtype=np.float_)\n    initial_column.iloc[0] = 0\n    map_rate_columns = pd.concat(\n        [initial_column.copy() for _ in team_map_tuples],\n        axis=1,\n        keys=team_map_tuples,\n    )\n    df = pd.concat([df, map_rate_columns], axis=1)\n    # We also want columns for team1 and team2 map rates for the map that is\n    # being played.\n    df[\"team_one_winrate\"] = initial_column.copy()\n    df[\"team_two_winrate\"] = initial_column.copy()\n    # When the team has never played the map, we assume it has a 50-50 rate.\n    df.loc[0, \"team_one_winrate\"] = NO_INFO_PRIOR\n    df.loc[0, \"team_two_winrate\"] = NO_INFO_PRIOR\n\n    for i in df.index:\n        map_name = df.loc[i, \"map_name\"]\n        winner = df.loc[i, \"map_winner\"]\n        loser = df.loc[i, \"map_loser\"]\n        team1 = df.loc[i, \"team_one_name\"]\n        team2 = df.loc[i, \"team_two_name\"]\n        df.loc[i, \"team_one_winrate\"] = _compute_win_rate(df.loc[i, :], team1, map_name)\n        df.loc[i, \"team_two_winrate\"] = _compute_win_rate(df.loc[i, :], team2, map_name)\n        # The numbers of wins and losses for each team-map pair are the same as on\n        # the previous row, except that some get incremented by one.\n        df.loc[i + 1, team_map_tuples] = df.loc[i, team_map_tuples]\n        if winner != \"draw\":\n            df.loc[i + 1, [(winner, map_name, \"wins\")]] = (\n                df.loc[i, [(winner, map_name, \"wins\")]] + 1\n            )\n            df.loc[i + 1, [(loser, map_name, \"losses\")]] = (\n                df.loc[i, [(loser, map_name, \"losses\")]] + 1\n            )\n        elif winner == \"draw\":\n            # We count a draw as half a win and half a loss for both teams.\n            df.loc[i + 1, [(team1, map_name, \"wins\")]] = (\n                df.loc[i, [(team1, map_name, \"wins\")]] + 0.5\n            )\n            df.loc[i + 1, [(team1, map_name, \"losses\")]] = (\n                df.loc[i, [(team1, map_name, \"losses\")]] + 0.5\n            )\n            df.loc[i + 1, [(team2, map_name, \"wins\")]] = (\n                df.loc[i, [(team2, map_name, \"wins\")]] + 0.5\n            )\n            df.loc[i + 1, [(team2, map_name, \"losses\")]] = (\n                df.loc[i, [(team2, map_name, \"losses\")]] + 0.5\n            )\n\n    return df\n\n\ndf_maprates = rolling_map_rates(owl21_reduced, team_names, map_list)\ndf_maprates\n\n\n\n\n\n\n\n\nmatch_id\nmap_name\nmap_winner\nmap_loser\nteam_one_name\nteam_two_name\n(Los Angeles Valiant, Busan, wins)\n(Los Angeles Valiant, Busan, losses)\n(Los Angeles Valiant, King's Row, wins)\n(Los Angeles Valiant, King's Row, losses)\n...\n(Houston Outlaws, Rialto, wins)\n(Houston Outlaws, Rialto, losses)\n(Houston Outlaws, Hollywood, wins)\n(Houston Outlaws, Hollywood, losses)\n(Houston Outlaws, Junkertown, wins)\n(Houston Outlaws, Junkertown, losses)\n(Houston Outlaws, Route 66, wins)\n(Houston Outlaws, Route 66, losses)\nteam_one_winrate\nteam_two_winrate\n\n\n\n\n0\n37234.0\nBusan\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.500000\n0.500000\n\n\n1\n37234.0\nKing's Row\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.500000\n0.500000\n\n\n2\n37234.0\nHavana\nHouston Outlaws\nDallas Fuel\nDallas Fuel\nHouston Outlaws\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.500000\n0.500000\n\n\n3\n37234.0\nVolskaya Industries\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.500000\n0.500000\n\n\n4\n37234.0\nIlios\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.500000\n0.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n890\n37441.0\nIlios\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\n0.0\n2.0\n1.0\n2.0\n...\n1.0\n1.0\n0.0\n2.0\n1.0\n1.0\n2.0\n0.0\n0.200000\n0.769231\n\n\n891\n37441.0\nHanamura\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\n0.0\n2.0\n1.0\n2.0\n...\n1.0\n1.0\n0.0\n2.0\n1.0\n1.0\n2.0\n0.0\n0.687500\n0.600000\n\n\n892\n37441.0\nKing's Row\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\n0.0\n2.0\n1.0\n2.0\n...\n1.0\n1.0\n0.0\n2.0\n1.0\n1.0\n2.0\n0.0\n0.769231\n0.666667\n\n\n893\n37441.0\nHavana\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\n0.0\n2.0\n1.0\n2.0\n...\n1.0\n1.0\n0.0\n2.0\n1.0\n1.0\n2.0\n0.0\n0.600000\n0.333333\n\n\n894\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n2.0\n1.0\n2.0\n...\n1.0\n1.0\n0.0\n2.0\n1.0\n1.0\n2.0\n0.0\nNaN\nNaN\n\n\n\n\n895 rows × 768 columns\n\n\n\nIn this dataframe, on row i, the column (team_name, map_name, outcome) is the number of times the team team_name has won/lost (for outcome=\"win\" or outcome=\"loss\") on that map up to that point in the season. This win/loss-count does not include yet the result of the game played on row i, that will be included in the win/loss-counts on row i+1.\nWe needed to make a couple of non-trivial choices in creating this dataframe: * If a team has never played a map, we consider it to have a win rate of 0.5, as if it had won half of its games. * We count a draw as half a win and half a loss for both teams. This has the nice feature that the sum of wins and losses is the number of games played.\nWe now use this to create a LargerMapRate predictor that predicts that the team with better win rate on that map up to that point in the season will win. If the win rates are exactly equal, we guess either win or lose at random. The other reasonable choice would be guessing a draw, but that would result in predicting far too many draws.\n\nclass LargerMapRateModel:\n    \"\"\"A model class that predicts the winner of a game to be the one that had a larger\n    win rate on the given map. Note that we never predict draws. We could predict a draw\n    when the rates are exactly equal, but that happens quite often, especially early in\n    the season, and thus doing so would predict too many draws. It's better to just\n    guess a winner than predict a draw in those cases.\n    \"\"\"\n\n    def train(self, train_data):\n        # All the necessary information has been computed already, there isn't any\n        # training to do.\n        return None\n\n    def predict(self, predictors):\n        \"\"\"Predict the winner of each game to be the team with the higher win rate\n        on the given map.\n        \"\"\"\n        team1 = predictors[\"team_one_name\"]\n        team2 = predictors[\"team_two_name\"]\n        rate1 = predictors[\"team_one_winrate\"]\n        rate2 = predictors[\"team_two_winrate\"]\n        coin_flips = np.random.choice([True, False], size=len(team1))\n        random_winner = team1.where(coin_flips, other=team2)\n        predictors[\"map_winner\"] = team1.where(\n            rate1 &gt; rate2, other=team2.where(rate2 &gt; rate1, other=random_winner)\n        )\n        return predictors\n\n\ntrain_data = df_maprates.iloc[:n_train, :]\ntest_data = df_maprates.iloc[n_train:, :]\ntrain_and_test(train_data, test_data, LargerMapRateModel)\n\n{'test rate': 0.5334821428571429,\n 'train rate': 0.5190156599552572,\n 'model': &lt;__main__.LargerMapRateModel at 0x2851c4880&gt;}\n\n\nWe seem to be reaching an accuracy of a bit more than 50%. It’s hard to say at the moment whether the higher accuracy from this predictor is significant compared to our earlier random guessing. We might just be getting lucky. One way to study whether that might be the case is to resample our data, using a technique called bootstrapping. To do that, we take our N games that we are testing our model on, and random sample, with replacement, from it another set of N games. Some of the original games may feature in the new, sampled set several times, some may not feature at all. This emulates sampling again from the same probability distribution of games, as if e.g. another, independent season had been played. If we do this resampling procedure k times and look at the range of accuracies we get when we run our model, that gives us some idea of how much random variation there is in our results. Let’s write a function that does such bootstrapping that we can reuse.\n\ndef train_and_test_bootstrap(train_data, test_data, model_class, k=50):\n    \"\"\"Bootstrap k samples of test data, and test the trained model on them.\n    \n    Return descriptive statistics of accuracy over the sampled test sets.\n    \"\"\"\n    results = [\n        train_and_test(\n            train_data, test_data.sample(frac=1.0, replace=True), model_class\n        )\n        for _ in range(k)\n    ]\n    train_rate = results[0][\"train rate\"]\n    test_rates = [r[\"test rate\"] for r in results]\n    test_rate_mean = np.mean(test_rates)\n    test_rate_std = np.std(test_rates)\n    test_rate_percentiles = np.percentile(test_rates, [10, 90])\n    example_model = results[0][\"model\"]\n    return {\n        \"train rate\": train_rate,\n        \"test rate mean\": test_rate_mean,\n        \"test rate std\": test_rate_std,\n        \"test rate 90th percentiles\": test_rate_percentiles,\n        \"example model\": example_model,\n    }\n\n\ntrain_and_test_bootstrap(train_data, test_data, LargerMapRateModel)\n\n{'train rate': 0.5078299776286354,\n 'test rate mean': 0.530982142857143,\n 'test rate std': 0.019437030393303505,\n 'test rate 90th percentiles': array([0.50870536, 0.55401786]),\n 'example model': &lt;__main__.LargerMapRateModel at 0x17fcfacb0&gt;}\n\n\nWith 50 iterations of bootstrap the average accuracy we got was 53%, with a standard deviation of 2% and 10th and 90th percentile accuracies of 50% and 56%. This gives us some confidence that we are indeed doing something better than random guessing, though whether our true accuracy (at the limit of infinite data set size) is 51.5% or 55% or something else in that ballpark, we can not say.\nNote that our use of bootstrap here is quite crude and simple. For one, we are only resampling within the latter half of the season, our test set, mostly to avoid the problem that early in the season win rates are quite meaningless since few games have been played. We can not interpret these numbers directly as something like confidence intervals, but they do give some indication of the level of randomness in our results.\nNote also that at this point the whole train/test split is superficial: There’s no training happening, and thus no risk of overfitting. In fact, our model performs a bit better on the test set than the training set. This is because the training set includes the early season, when most teams haven’t played most maps yet, and we thus don’t have any information to base our predictions on.\n\n\n\n\n\n\nOverwatch players can choose from many characters, called heroes, with different abilities. We don’t use any data specific to choices of heroes, nor do we use information particular to the different game modes, but simply focus on the question of which team won against which team on which map. Image credit Blizzard Entertainment.\n\n\n\n # Part 6. Skill based predictor - Elo ratings\nThe map win rate predictor may have been a bit better than random guessing, but it certainly isn’t blowing our socks off. Let’s now try something more interesting.\nFor competitive games, it is natural to introduce some kind of skill based system, where higher skilled teams are considered more likely to win than lower skilled teams. One such system, originally developed for chess, is called Elo ratings. We will use it here.\nEach team will begin with an initial Elo rating - we have chosen to use 1500, but this is an arbitrary choice that doesn’t matter. Each time a team wins a game their Elo increases, and each time they lose it decreases. The clever bit is how the amount by which the Elo changes depends on who the game was played against.\nWhenever a team plays against another team, their respective Elos are compared to create an expectation for the match up, where the team with the higher Elo is expected to win. How much the two teams’ Elos differ will influence how much the teams will gain / lose Elo based on the result of the match. If team A has a much higher Elo than team B then they are heavily expected to win and will not gain much Elo if they beat team B. If however, they lose to team B in an upset then they will lose a lot of Elo. The Elo ratings are zero-sum, meaning team B will gain / lose the opposite amount.\nTo be explicit, if the ratings of the two teams are \\(R_A\\) and \\(R_B\\), then the expected “score” for team A is\n\\[ E_A = \\frac{1}{1+10^{(R_B - R_A)/400}} \\]\nand conversely for team B it is \\(E_B = 1 - E_A\\). The 400 is another arbitrary scale constant, like the starting value of 1500. The expected scores can be related to the players’ probability of winning, but we refer the reader to the Wikipedia article for the details.\nThe updated Elo ratings for teams A and B after the game are\n\\[ R'_A = R_A + k \\cdot (S_A - E_A) \\] \\[ R'_B = R_B + k \\cdot (S_B - E_B) = R_B - k \\cdot (S_A - E_A) \\]\nHere \\(S_A\\) is the outcome score of the game for team A (similarly for team B and \\(S_B\\)), which we choose to be 1 if team A won, 0 if they lost, and 0.5 in the case of a draw.\nThe parameter \\(k\\) in the update formula above is a free parameter that sets the variance or the “learning rate” of the system. Higher values of \\(k\\) will mean that teams’ Elo ratings change quickly based on how well they’ve done in the last few games, whereas with a low \\(k\\) value the ratings are quite “rigid” and change only slowly.\nTypically when data is low (e.g. with a new team), there is a lot of uncertainty about their skill level so a high \\(k\\) value may be preferable. On the other hand, once a team has played a lot, the model has a fairly accurate view of their skill and a lower \\(k\\) value might be used to avoid overfitting to the last few games. In chess for example, a high \\(k\\) value will be used in low rated tournaments, and a low \\(k\\) value for higher rated tournaments.\nAs it can be difficult to know at what point to vary \\(k\\) we will keep it static throughout this story.\n\nINITIAL_ELO = 1500\nELO_SCALE = 400\n\n\ndef expected(A, B):\n    \"\"\"Expected 'score' for the game, based on the Elo ratings of the participants A and\n    B. The score is in the range from 0 to 1, and relates to the probability of team A\n    or team B winning, with score of 0 meaning extreme confidence in A winning and score\n    of 1 meaning extreme confidence in B winning.\n    \"\"\"\n    return 1 / (1 + 10 ** ((B - A) / ELO_SCALE))\n\n\ndef elo(old, exp, score, k=32):\n    \"\"\"New Elo for a team based on their old Elo rating, and expected and actual outcome\n    of the game.\n    \"\"\"\n    return old + k * (score - exp)\n\nBelow we compute rolling Elo ratings for all teams at all points of the season, similarly to what we did with map win rates.\n\ndef rolling_elo(owl21_reduced, team_names):\n    \"\"\"Make a data frame with one column per team, with values of the ELO rating of each\n    team at each given moment in the season. The value on row i does not include the\n    changes to ELO ratings caused by the game played on row i, those will only be\n    included on row i+1.\n    \"\"\"\n    df_elo = owl21_reduced.copy().reset_index().drop(columns=\"index\")\n    N_games = len(df_elo)\n    initial_column = pd.Series([np.nan] * N_games, index=df_elo.index, dtype=np.float_)\n    initial_column.iloc[0] = INITIAL_ELO\n    for team in team_names:\n        df_elo[team] = initial_column.copy()\n    df[\"team_one_elo\"] = initial_column.copy()\n    df[\"team_two_elo\"] = initial_column.copy()\n\n    for i in df_elo.index:\n        team1 = df_elo.loc[i, \"team_one_name\"]\n        team2 = df_elo.loc[i, \"team_two_name\"]\n        elo1_pre = df_elo.loc[i, team1]\n        elo2_pre = df_elo.loc[i, team2]\n        df_elo.loc[i, \"team_one_elo\"] = elo1_pre\n        df_elo.loc[i, \"team_two_elo\"] = elo2_pre\n        exp1 = expected(elo1_pre, elo2_pre)\n        exp2 = 1 - exp1\n        winner = df_elo.loc[i, \"map_winner\"]\n        if team1 == winner:\n            score1 = 1\n        elif team2 == winner:\n            score1 = 0\n        elif winner == \"draw\":\n            score1 = 0.5\n        else:\n            raise RuntimeError(\"something went wrong\")\n        score2 = 1 - score1\n        elo1_post = elo(elo1_pre, exp1, score1)\n        elo2_post = elo(elo2_pre, exp2, score2)\n        df_elo.loc[i + 1, team_names] = df_elo.loc[i, team_names]\n        df_elo.loc[i + 1, team1] = elo1_post\n        df_elo.loc[i + 1, team2] = elo2_post\n    return df_elo\n\n\ndf_elo = rolling_elo(owl21_reduced, team_names)\ndf_elo\n\n\n\n\n\n\n\n\nmatch_id\nmap_winner\nmap_loser\nteam_one_name\nteam_two_name\nmap_name\nLos Angeles Valiant\nShanghai Dragons\nGuangzhou Charge\nLos Angeles Gladiators\n...\nAtlanta Reign\nToronto Defiant\nWashington Justice\nHangzhou Spark\nParis Eternal\nDallas Fuel\nSeoul Dynasty\nHouston Outlaws\nteam_one_elo\nteam_two_elo\n\n\n\n\n0\n37234.0\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nBusan\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n...\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n\n\n1\n37234.0\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nKing's Row\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n...\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1484.000000\n1500.000000\n1516.000000\n1516.000000\n1484.000000\n\n\n2\n37234.0\nHouston Outlaws\nDallas Fuel\nDallas Fuel\nHouston Outlaws\nHavana\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n...\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1501.469502\n1500.000000\n1498.530498\n1501.469502\n1498.530498\n\n\n3\n37234.0\nDallas Fuel\nHouston Outlaws\nHouston Outlaws\nDallas Fuel\nVolskaya Industries\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n...\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1485.334159\n1500.000000\n1514.665841\n1514.665841\n1485.334159\n\n\n4\n37234.0\nHouston Outlaws\nDallas Fuel\nHouston Outlaws\nDallas Fuel\nIlios\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n...\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1502.681733\n1500.000000\n1497.318267\n1497.318267\n1502.681733\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n890\n37441.0\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nIlios\n1183.216157\n1756.568699\n1453.558171\n1687.413399\n...\n1714.071836\n1508.428907\n1470.163957\n1441.604268\n1450.904232\n1585.810683\n1533.992033\n1496.310344\n1714.071836\n1756.568699\n\n\n891\n37441.0\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHanamura\n1183.216157\n1770.621348\n1453.558171\n1687.413399\n...\n1700.019187\n1508.428907\n1470.163957\n1441.604268\n1450.904232\n1585.810683\n1533.992033\n1496.310344\n1770.621348\n1700.019187\n\n\n892\n37441.0\nShanghai Dragons\nAtlanta Reign\nAtlanta Reign\nShanghai Dragons\nKing's Row\n1183.216157\n1783.414025\n1453.558171\n1687.413399\n...\n1687.226510\n1508.428907\n1470.163957\n1441.604268\n1450.904232\n1585.810683\n1533.992033\n1496.310344\n1687.226510\n1783.414025\n\n\n893\n37441.0\nShanghai Dragons\nAtlanta Reign\nShanghai Dragons\nAtlanta Reign\nHavana\n1183.216157\n1795.094231\n1453.558171\n1687.413399\n...\n1675.546304\n1508.428907\n1470.163957\n1441.604268\n1450.904232\n1585.810683\n1533.992033\n1496.310344\n1795.094231\n1675.546304\n\n\n894\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1183.216157\n1805.796297\n1453.558171\n1687.413399\n...\n1664.844237\n1508.428907\n1470.163957\n1441.604268\n1450.904232\n1585.810683\n1533.992033\n1496.310344\nNaN\nNaN\n\n\n\n\n895 rows × 28 columns\n\n\n\nThe columns in this dataframe that are the names of individual teams are their Elo ratings. To illustrate the development of Elo ratings over the season we plot them.\n\nplt.figure(figsize=(8, 6))\nplt.plot(df_elo.loc[:, team_names])\nplt.xlabel(\"Game number\")\nplt.ylabel(\"Elo rating\")\nplt.show()\n\n\n\n\nEach line is the Elo rating of one team. They all start at 1500, and as games are played teams gain/lose Elo as they win/lose games. The fact that the spread of the ratings is still growing late in the season is an indication that we are limited in the amount of data we have.\nWe split the dataframe of Elo ratings into training and test sets with the same ratio as before, and make a model that always predicts that the team with a higher rating will win.\n\nelo_train = df_elo.iloc[:n_train, :].copy()\nelo_test = df_elo.iloc[n_train:, :].copy()\n\n\nclass LargerELOModel:\n    \"\"\"A model class that predicts the winner of a game to be the one that had a larger\n    ELO rating.\n    \"\"\"\n\n    def train(self, train_data):\n        # All the necessary information has been computed already, there isn't any\n        # training to do.\n        pass\n\n    def predict(self, predictors):\n        \"\"\"Predict the winner of each game to be the team with the higher ELO.\"\"\"\n        team1 = predictors[\"team_one_name\"]\n        team2 = predictors[\"team_two_name\"]\n        elo1 = predictors[\"team_one_elo\"]\n        elo2 = predictors[\"team_two_elo\"]\n        predictors[\"map_winner\"] = team1.where(\n            elo1 &gt; elo2, other=team2.where(elo2 &gt; elo1, other=\"draw\")\n        )\n        return predictors\n\n\ntrain_and_test_bootstrap(elo_train, elo_test, LargerELOModel)\n\n{'train rate': 0.6062639821029083,\n 'test rate mean': 0.6234821428571429,\n 'test rate std': 0.02344255897782005,\n 'test rate 90th percentiles': array([0.60200893, 0.64866071]),\n 'example model': &lt;__main__.LargerELOModel at 0x2851e9db0&gt;}\n\n\nThis model clearly outperforms the earlier map win rate based model, reaching an accuracy roughly between 59% and 65%.\nAs an aside, we tried varying the \\(k\\) parameter for the Elo system, and couldn’t find a value that would have significantly improved the accuracy we see here. We leave this analysis out of the story, for brevity.\n\n\n\n\n\n\nProfessional Overwatch players in a tournament. We model skill using Elo ratings on the level of teams, not individual players. Image credit Blizzard Entertainment."
  },
  {
    "objectID": "stories/2023-02-01-Overwatch-League/OWL_story.html#combining-elo-and-map-win-rates",
    "href": "stories/2023-02-01-Overwatch-League/OWL_story.html#combining-elo-and-map-win-rates",
    "title": "Esport predictions: Overwatch League",
    "section": "Combining Elo and map win rates",
    "text": "Combining Elo and map win rates\nThe Elo system is simple, elegant, and evidently quite powerful. However, it feels crude in how it entirely disregards all data about the maps: It’s only concerned with who won against who. Perhaps we can improve on it by combining the Elo ratings with the map win rates, and use both for making our predictions? Let’s try.\n\ndef combine_elo_and_maprates(df_elo, df_maprates, team_names, map_names):\n    \"\"\"Combine rolling Elo and rolling map win rates into a single dataframe.\"\"\"\n    team_map_pairs = list(itertools.product(team_names, map_names, [\"wins\", \"losses\"]))\n    df_maprates = df_maprates[team_map_pairs + [\"team_one_winrate\", \"team_two_winrate\"]]\n    df_elo_maprates = pd.concat(\n        [df_elo, df_maprates],\n        axis=1,\n    )\n    return df_elo_maprates\n\n\ndef encode_map_winner(df):\n    \"\"\"Encode game outcome as +1, 0, -1.\"\"\"\n    team1 = df[\"team_one_name\"]\n    team2 = df[\"team_two_name\"]\n    winner = df[\"map_winner\"]\n    N_games = len(team1)\n    ones = pd.Series([1] * N_games)\n    minus_ones = pd.Series([-1] * N_games)\n    winner_number = ones.where(\n        winner == team1, other=minus_ones.where(winner == team2, other=0)\n    )\n    return winner_number\n\n\ndf_combined = combine_elo_and_maprates(df_elo, df_maprates, team_names, map_list)\ndf_combined[\"map_winner\"] = encode_map_winner(df_combined)\nprediction_columns = [\n    \"map_winner\",\n    \"team_one_winrate\",\n    \"team_two_winrate\",\n    \"team_one_elo\",\n    \"team_two_elo\",\n]\ncombined_train = df_combined.loc[:n_train, prediction_columns].copy()\ncombined_test = df_combined.loc[n_train : n_games - 1, prediction_columns].copy()\n\nWe have combined the rolling Elo numbers and map win rates into a single dataframe. We have also encoded the outcomes of games numerically into a single column, so that 1 means team one won, -1 means team two won, and 0 means the game was a draw. This allows using various statistical models meant for numerical rather than categorical data.\nPreviously we could simply predict that the team with the higher map win rate or higher Elo would win. Now that we use both as predictors, we have to decide how to combine them into a single prediction. Given how simple our predictors are, a natural starting point is a linear model, that models the numerically encoded (1,0, or -1) game winner as a linear combination of the win rates and the Elo scores of both teams. Note that this is the first time that any statistical modelling is happening in this story, and thus the first time that overfitting could in theory become a concern, and we need to actually use the trainining/test split we did in the beginning.\n\nclass LinearClassifier:\n    def __init__(self):\n        # See\n        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n        self.model = linear_model.RidgeClassifier()\n\n    def train(self, train_data):\n        self.model.fit(\n            train_data.drop(columns=\"map_winner\"),\n            train_data[\"map_winner\"],\n        )\n        return None\n\n    def predict(self, predictors):\n        predictors[\"map_winner\"] = self.model.predict(predictors)\n        return predictors\n\n\n# Predict who wins using ELO and map win/loss information\ntrain_and_test_bootstrap(combined_train, combined_test, LinearClassifier)\n\n{'train rate': 0.6160714285714286,\n 'test rate mean': 0.6223266219239374,\n 'test rate std': 0.021320696901629056,\n 'test rate 90th percentiles': array([0.59261745, 0.64451902]),\n 'example model': &lt;__main__.LinearClassifier at 0x28658d330&gt;}\n\n\nCombining the Elo and map win rates with a linear model gives a prediction accuracy somewhere around 60% to 66%. It is hard to say that this yields any improvement over only using the Elo data. This is somewhat disappointing; It seems our attempt at using more granular information than just plain Elo is of little help. This is compatible though with for instance the earlier observation that good teams do well an almost all maps and bad teams similarly do badly.\nOne could hypothesise that the issue is that our linear model is too crude and biased for this purpose. We did some experimentation around this, and it seems to not be the case: For instance some support vector machines perform no better.\n # Part 7: Conclusions\nWe started with the question of whether we can predict outcomes of Overwatch games using data from earlier that same season. The answer seems to be “yes, to a limited extent”.\nAfter trying a simple win rate based model, we settled on using the Elo ratings system, which assigns a skill rating to each team based on who they win and lose against, rewarding more points for winning against highly ranked opponents. This got us to a range where we could correctly predict the outcomes of about 60-65% of the games in the last half of the season. That’s substantially better than chance, but it’s not overwhelmingly impressive. We hoped to improve on that by adding some map specific information, that would take into account some teams being especially good or bad on particular maps, but failed to improve the prediction accuracy significantly.\nWe of course can not know if some other model or way of doing the predictions would yield better results. However, from toying around with various methods, some of which we left out of the final story, the feeling we were left with is that we are probably close to what can be achieved with our current approach. To improve further we would either try modelling on a more granular scale of individual players, taking into account player transfers within a season, or find another angle of how to utilise map specific data. There is, of course, also a natural limit to how good our predictions can ever be, because there is inherent variation in how individual games go. We may or may not be close to that limit.\nOverall, we were surprised by how general our analysis turned out to be: In the end the method that we got the most mileage out of were the Elo ratings, which can be applied to almost any game or sport. It’s entirely blind to any particular features of Overwatch as a game. This is bad in that it leaves us with the feeling that we didn’t understand anything very deep about Overwatch as a game through this analysis, but good in that our above code can be reused almost verbatim on other competitive games. Partially this might be because we didn’t try even try to utilise some very Overwatch-specific details about things like the various characters players can choose, but map specific expertise would be the most obvious feature to expect, and even that didn’t seem to be very useful for our predictions.\nWe wrote the bulk of this story in the middle of 2022 and thus ran our analysis on the 2021 season. By the time we were polishing the story for review the 2022 season had finished, so we ran the same analysis on that one too. It’s very easy to do by simply changing the file to download and read data from in the very beginning, and we encourage the reader to do it. The main conclusions from 2022 are the same as above with 2021: Map win rate based predictions are still a bit better than chance but not much; Elo ratings yield better predictions than that, though nothing much above 60%; and combining Elo and map win rate isn’t much better than just using Elo. The numbers do shift around a bit though, accuracies going up or down by a few percentage points.\nThe most interesting future exploration would be to move from the team-level to the player-level, and try to model the skill levels of the individuals that make up the teams. This would open whole new possibilities of using data from multiple seasons and tracking players across them as they may change teams. This would also enable us to for instance predict the performance of a rebuilt team at the beginning of a season based on who they’ve added to their roster, or model which teams seem to perform as more or less than the sum of their parts. That, though, is all work for another data story."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "",
    "text": "Jennifer Ding, Research Application Manager, The Alan Turing Institute\nEirini Zormpa, Community Manager of Open Collaboration, The Alan Turing Institute"
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#background",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#background",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Background",
    "text": "Background\nIn the summer of 2010, researchers from The International Group for Historic Aircraft Recovery excavated, in fragments, a small semi-opaque cosmetic jar from the Pacific island of Nikumaroro. They were searching for evidence to support their hypothesis that Amelia Earhart and her navigator Fred Noonan perished there in 1937, after very nearly succeeding in their pathbreaking mission to circumnavigate the globe by air at its widest point, the equator.\nMy interest in glass artifacts from the island ultimately led to a personal quest to become a resident glass expert of the team. In partnership with archaeologists Thomas King and Bill Lockhart, and chemist Greg George, we produced a paper that summarized our findings. By then, our work on the jar had already generated some press.\nAs we continued our research in 2013, Greg George and I worked with EAG Laboratories to determine the chemical composition of the Nikumaroro jar and a sibling glass jar, used as a control, that I had purchased on eBay and had named the clear facsimile. While the two jars were similar in most respects, interestingly, the clear facsimile jar was transparent, while the Nikumaroro jar was semi-opaque. Both jars were manufactured by the Hazel-Atlas Glass Company of Wheeling, West Virginia, but EAG Laboratories’ ICP-MS analysis revealed them to have considerably different chemical profiles."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#data",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#data",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Data",
    "text": "Data\nIn the social sciences, “data” often mean a collection of facts. In industry, however, we often consider data to be a collection of numbers and vectors, and we use computational tools such as statistics and machine learning to aid in their analysis. As part of an eCornell machine learning course, in 2022 I had the opportunity to revisit my work on the Nikumaroro jar as a learning exercise, and to use the data returned by EAG Laboratories to attempt to solve a novel machine learning classification problem I had proposed.\nThus was my interest in analyzing the Nikumaroro jar and comparing it with its sibling rekindled. Instead of using the jars’ data as a collection of facts, I now wanted to use them to study machine learning classification.\nBy analyzing the weight percent of the chemical elements of which each jar’s glass was made, plus refractive index, machine learning can build a model that will predict a pre-defined class to which it believes a given glass sample belongs. We may then assess the skill of the model by comparing all predictions to the actual correct classes for each sample. The correct class of the Nikumaroro jar and of the clear facsimile is that of a container.\nTwo sets of measurements for the purposes of machine learning are not of much value, however, because machine learning models require a full range of known examples upon which to ‘train’ a model to recognize examples it has never seen before.\nEAG Laboratories had been thorough, and for the purposes of our 2013 analysis, their data was ample enough. For the purposes of machine learning, however, the data, while eagerly anticipated by our team, was paltry. Where would I find the data upon which to train a machine learning model? I did not need to search very far for my answer. British forensic scientists Ian W. Evett and Ernest J. Spiehler assembled a database of glass samples in 1987 for the purposes of solving crimes, such as break-ins. Their paper was first presented at the 1987 conference of the KBS (Knowledge-Based Systems) in Goverment and is titled: “Rule Induction in Forensic Science.” http://gpbib.cs.ucl.ac.uk/gp-html/evett_1987_rifs.html. Their database is available at the University of California Machine Learning Repository here.\nThe weight percent of eight chemical elements and refractive index comprise the features of their database and each sample is represented by a target variable called Type. The types of glass represented in the sample are: Window Float, Window Non-Float, Vehicle Float, Container, Tableware, and Headlamp. There are no Vehicle Non-Float types represented in the data.\nMy two glass samples, as luck would have it, were independently measured for most of the same elements as found in the U.K. database, with a few caveats:\n\nBecause silicon was measured in my lab data as ‘Matrix,’ with no actual number stated, I estimated the silicon for both containers based on the average for containers in the database (72.37).\nBecause K (potassium) was measured at 980 ppm for the clear facsimile, rather than by wt%, I assigned it a value of 50% of the wt% of the Nikumaroro jar: .5 X .24 = .12.\nBecause Fe (iron) is at very low wt% levels in the 1987 database, and the levels in my data are listed at very low parts per million (ppm), I assigned to the Nikumaroro jar and to the clear facsimile a wt% of Fe of .02 and .01, respectively.\nRefractive index was not measured for either the Nikumaroro jar or the clear facsimile. Since the clear facsimile is completely transparent, I assigned it the minimum refractive index of containers from the 1987 dataset. Since the Nikumaroro jar is semi-opaque, I assigned it the maximum refractive index of containers from the 1987 dataset.\n\nThese educated guessed were necessary to ensure the ability of the machine learning algorithm to compare the data from EAG Laboratories with the training dataset from the U.K.*\n\n*Note: In a later section of this story, we will use a technique to evaluate whether or not the decision to supply a few of the values for the jars makes a significant difference in our machine learning algorithm’s ability to predict the class of the two jar samples."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#research-agenda",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#research-agenda",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Research Agenda",
    "text": "Research Agenda\nThere are three main research questions I wish to answer in my machine learning classification problem. I follow each of these questions with a rationale that explains the derived benefits in answering each question. 1) What can Python’s Matplotlib and Seaborn graphing capabilities reveal about the similarities and/or dissimilarities between the Nikumaroro jar, clear facsimile, and the glass samples in the 1987 database?\nRationale: Before tackling the machine learning problem, we need to learn whether and how the Nikumaroro jar and the clear facsimile, which precede the samples in the U.K. database by many decades, differ from the samples in that U.K. database. If the difference is too great, it may not be possible to use machine learning to classify the Nikumaroro jar and the clear facsimile at all.\n2) What do the correlations between elements for the different types of glass in the 1987 database reveal about late 20th century glassmaking, as compared with early 20th century glassmaking?\nRationale: The Nikumaroro jar and the clear facsimile precede the samples in the U.K. database by many decades. During these many decades, the recipes used in the glassmaking batches may have changed considerably. It is always fascinating to observe historical changes made manifest in data, but beyond this historical interest, the graphs that may be built from these correlations may also provide visual explanations for some difficulties a machine learning model may be having with classifying the Nikumaroro jar and the clear facsimile.\n3) Using machine learning to train a model on the 1987 database, could that model be used to classify the type (container) of one or both of the older samples unseen by the model?\nRationale: A successful classification of the Nikumaroro jar may suggest that it is not quite so unusual as we had supposed in our earlier research. The U.K. database was never built to classify highly unusual or historical glass samples. It was built to classify ordinary glass samples retrieved from break-ins that occurred in the 1980s. Successful classification would strengthen the argument, often expressed but without evidence, that the jar is not a rare item likely to be from the 1930s, but of far more recent provenance, too recent to have possibly been brought to the island in 1937 by Amelia Earhart.\nThe outcome of this classification problem has the potential to disverify or perhaps weaken a part of our hypothesis that Earhart and her navigator, Fred Noonan, perished on Nikumaroro. Efforts to disverify are an important element of scientific research, for as Richard Feynman once said, “The first principle is that you must not fool yourself — and you are the easiest person to fool.”\n\nFirst, before we start to analyze these questions, we will read in the data file and create a simple report.\nOur first task is to read in a copy of the 1987 glass.data file that has been uploaded to Zenodo.org. Then, we can assign the names of the variables in this file to a variable called names. Notice I am carefully avoiding the variable name ‘Type’ because it is a reserved word in Python. (This was obviously not true in 1987 when the database was built, because Python did not exist then.) Using that reserved word will co-opt the type function, thus disabling my ability to check the types of specific variables. Instead, I will use the variable name ‘GlassType’ to contain the various types of glass.\nI also include a docstring explaining what these variables mean and their general significance to glassmaking.\n\n\"\"\"\nID: an integer that identifies the 1987 sample number. This column should always be droppped in any analysis.\nRef_ix: Refractive index. There are many scientific definitions of refractive index, but in basic terms,\nrefractive index refers to how much the path of light is refracted when it enters the glass. The higher the \nrefractive index, the less transparent the glass is.\nNa: Sodium oxide reduces the temperature at which silica melts to around 1500-1650 Celsius. A lower temperature\nequates to a more efficient and less costly manufacturing process.\nMg: Magnesium oxide is used as a reducing agent in glassmaking. It reduces sulfates, which are used to fine the\nglass, and other impurities, such as iron.\nAl: Aluminum oxide is added to increase durability and heat resistance and to reduce crystallization.\nSi: Silicon oxide. The main constituent (matrix) of glass is silica sand.\nK: Potassium oxide is used for strengthening and fining the glass.\nCa: Calcium oxide is considered a stabilizer. It is used for increasing the chemical durability of glass, and \nalso to lower the melting point of the silica. Calcium oxide can also be used to raise refractive index.\nBa: Barium oxide. Barium was, and is, employed in glass manufacture to add luster or brilliance to glass, \nand also to raise the refractive index.[1] Museum glass, salt shakers, shot glasses, and microscopic slides \nare often high in barium. The Nikumaroro jar is also high in barium.\nFe: Iron oxide can be used as a colorant or it can exist by chance as an impurity.\nGlassType: This is the integer assigned in 1987 to signify the type of glass of the sample. This column should\nonly appear in the validation dataset; it should be dropped from the training dataset.\n\"\"\"\nimport urllib.request\nwith urllib.request.urlopen(\"https://zenodo.org/record/6913745/files/glass.data\") as f:\n    html=f.read().decode('utf-8')\n    with open('glass.csv', 'w') as out:\n        out.write(html)\n\nfilename='glass.csv'\n\nnames=['ID','Ref_ix','Na','Mg','Al','Si','K','Ca','Ba','Fe','GlassType']\n\n\nThe three questions of the research agenda (listed above) are quite different. It will be convenient to modify the dataset in different ways for each problem. Therefore, we will read in the file twice to create two Pandas dataframes of the glass data, thus allowing us to have separate data pipelines to keep the machine learning question separate from the other two. The pandas library in Python provides a convenient function, read_csv, for this purpose. One of the columns is imported as an object, which I will coax to be an integer for more reliable data processing.\n\nfrom pandas import read_csv\ndataset = read_csv(filename, header=None, sep=',',names=names)\ndataset_ml = read_csv(\n    filename, header=None, sep=',',names=names)\n# Convert clunky object formats to ints\ndataset['GlassType'] = dataset['GlassType'].astype('string')\ndataset['GlassType'] = dataset['GlassType'].astype('int')\ndataset_ml['GlassType'] = dataset['GlassType'].astype('string')\ndataset_ml['GlassType'] = dataset['GlassType'].astype('int')\n\nNext, we will provide the list of variables in the file with descriptions of their counts and data types. I could use the info() method for this purpose, but an anonymous user on StackOverflow has provided a function, which I have adopted, to give this information a more attractive display. It would be useful as well to have a simple statistical report of this dataset, which is provided by the describe method. Notice that the columns ID and GlassType, as supplied from the 1987 study, are integers, but they are also categorical variables. I will therefore drop these columns from my simple statistical report because they would be meaningless in terms of the measures of central tendency (mean, max, etc.). However, because GlassType will be very important to the rest of this code, I will take a copy of the dataset prior to making this report, to ensure I do not drop this variable from the original Pandas dataframe.\n\ndef infoOut(data,details=False):\n    dfInfo = data.columns.to_frame(name='Column')\n    dfInfo['Non-Null Count'] = data.notna().sum()\n    dfInfo['Dtype'] = data.dtypes\n    dfInfo.reset_index(drop=True,inplace=True)\n    if details:\n        rangeIndex = (dfInfo['Non-Null Count'].min(),dfInfo['Non-Null Count'].min())\n        totalColumns = dfInfo['Column'].count()\n        dtypesCount = dfInfo['Dtype'].value_counts()\n        totalMemory = dfInfo.memory_usage().sum()\n        return dfInfo, rangeIndex, totalColumns, dtypesCount, totalMemory\n    else:\n        return dfInfo\ndisplay(infoOut(dataset))\ntemp=dataset.copy()\ndisplay(temp.iloc[:,1:-1].describe())\n\n\n\n\n\n\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\n0\nID\n214\nint64\n\n\n1\nRef_ix\n214\nfloat64\n\n\n2\nNa\n214\nfloat64\n\n\n3\nMg\n214\nfloat64\n\n\n4\nAl\n214\nfloat64\n\n\n5\nSi\n214\nfloat64\n\n\n6\nK\n214\nfloat64\n\n\n7\nCa\n214\nfloat64\n\n\n8\nBa\n214\nfloat64\n\n\n9\nFe\n214\nfloat64\n\n\n10\nGlassType\n214\nint64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRef_ix\nNa\nMg\nAl\nSi\nK\nCa\nBa\nFe\n\n\n\n\ncount\n214.000000\n214.000000\n214.000000\n214.000000\n214.000000\n214.000000\n214.000000\n214.000000\n214.000000\n\n\nmean\n1.518365\n13.407850\n2.684533\n1.444907\n72.650935\n0.497056\n8.956963\n0.175047\n0.057009\n\n\nstd\n0.003037\n0.816604\n1.442408\n0.499270\n0.774546\n0.652192\n1.423153\n0.497219\n0.097439\n\n\nmin\n1.511150\n10.730000\n0.000000\n0.290000\n69.810000\n0.000000\n5.430000\n0.000000\n0.000000\n\n\n25%\n1.516522\n12.907500\n2.115000\n1.190000\n72.280000\n0.122500\n8.240000\n0.000000\n0.000000\n\n\n50%\n1.517680\n13.300000\n3.480000\n1.360000\n72.790000\n0.555000\n8.600000\n0.000000\n0.000000\n\n\n75%\n1.519157\n13.825000\n3.600000\n1.630000\n73.087500\n0.610000\n9.172500\n0.000000\n0.100000\n\n\nmax\n1.533930\n17.380000\n4.490000\n3.500000\n75.410000\n6.210000\n16.190000\n3.150000\n0.510000\n\n\n\n\n\n\n\nThere are no nulls in the dataset. The column types are all numeric. All 214 samples of the database have been included.\n\n\nCreate a dictionary of glass types.\nWe need a way to translate the numerical glass types found in the 1987 database to their English equivalents. This is accomplished by creating a Python dictionary.\nNote we could have omitted Vehicle Non-Float from this dictionary, since there are no examples in the data of this type; however, for the sake of clarity we will retain it.\nWe include also an exhibit that lists the counts for the various glass types.\n\n dict = {1: 'Window Float',\n  2: 'Window Non-Float', \n  3: 'Vehicle Float',\n  4: 'Vehicle Non-Float',\n  5: 'Container',\n  6: 'Tableware',\n  7: 'Headlamp'}\n\nvc=temp.replace({\"GlassType\": dict},inplace=True)\nvc=temp['GlassType'].value_counts().rename_axis('unique_values').reset_index(name='Counts')\nvc.set_index('unique_values',inplace=True)\nvc.rename_axis(['GlassType'],inplace=True)\ndisplay(vc)\n\n\n\n\n\n\n\n\nCounts\n\n\nGlassType\n\n\n\n\n\nWindow Non-Float\n76\n\n\nWindow Float\n70\n\n\nHeadlamp\n29\n\n\nVehicle Float\n17\n\n\nContainer\n13\n\n\nTableware\n9\n\n\n\n\n\n\n\n\n\nWhat do the types of glass represent in everyday life?\nThe terms “float” and “non-float” are glass industry terms. They describe the process by which window glass is made. Prior to 1959, all windows were made with non-float processes. These commonly involved grinding and polishing sheets of plate glass, which were cast on an iron surface. The result was not the perfectly smooth and clear glass we know today but rather was somewhat more translucent and sometimes ‘wavy’. Today, this kind of window glass is prized for its artistic appearance and historical significance, if not for its lack of clarity. Float glass is made by floating molten glass on a bed of molten tin under controlled environmental conditions in a bath chamber. An excellent video describing this process is here:\nIf a glass sample is Window Float, in all probability it was manufactured no earlier than 1959. Window Non-Float samples are probably older than 1959.\nVehicle Float glass describes the type of glass that has been used for vehicle windshields since 1959.\nHeadlamps are vehicle lamps, which illuminate the road ahead.\nContainer glass describes commercial product containers, whether for bottles or jars.\nTableware glass describes glassware for the table or any glass table item used in dining or the serving of food.\n\n\nCreate some utility functions for plotting graphs from the 1987 dataset.\nBefore using machine learning on the data, there is much we can learn about the two jars’ likeness to glass samples in the 1987 database, simply by observing counts based on specific criteria. By comparing each jar with its “nearest neighbors,” based on measurements for three elements in the periodic table, we may gain a sense of which type of glass in the database each jar most closely resembles.\nWe will therefore write functions to create three graphical reports, one for each of the three elements magnesium, calcium, and barium. Each of the three reports will display three things: 1) The complete range of counts for each glass type. (This graph will be repeated for each report.)  2) The range of counts for all glass types within 0.15 above and below the elemental measurement of the clear facsimile.  3) The range of counts for all glass types within 0.15 above and below the elemental measurement of the Nikumaroro jar.\nThe key function, numgroups, requires an element from the periodic table so that it can look up parameters in the included dictionary. This dictionary contains the measurements obtained from the lab for each element for the clear facsimile and for the Nikumaroro jar. Low and High parameter values for each element are also required, even when the complete range of counts is to be displayed. These will be passed to the function as hard-coded values that are either  &gt; a. exactly 0.15 above and below the measurement for each jar; or  &gt; b. the lowest and highest measured values in the database for each element.\nLast, the jar_spec parameter is simply the type of jar to be analyzed, clear facsimile or Nikumaroro jar. The jar_spec is used to create the title for each graph.\nIf the graph’s input parameters result in a graph with a single type of glass, no graph is created. (Bar graphs with but a single bar are, after all, neither very attractive nor useful!) Instead, a message is displayed that mirrors the text that accompanies the other graphs.\nIf the number of glass types returned by the criteria is equal to zero, we cause an error to be thrown with int(‘d’), which attempts to convert the letter ‘d’ to an integer, rather than waiting for the error that would have been thrown naturally by the plot statement. If we had waited for the plot statement to throw the error, useless information about the size of the graph (which cannot be drawn in this case) is displayed. In the case of this error, a simple message is printed to inform us that no relevant glass samples were found in the database.\nThe other function, ticks_restrict_to_integer, is a utility function that controls the number of tick marks on the y-axis of each graph. The y-axis represents the count. Because the scale of counts varies with each graph, it was necessary to make the y-axes more uniform with one another by having this function.\n\nfrom matplotlib.ticker import MultipleLocator\ndef ticks_restrict_to_integer(axis):\n    \"\"\"Restrict the ticks on the given axis to be at least integer;\n    that is, no half ticks at 1.5 for example.\n    \"\"\"\n    major_tick_locs = axis.get_majorticklocs()\n    if len(major_tick_locs) &lt; 2 or major_tick_locs[1] - major_tick_locs[0] &lt; 1:\n        axis.set_major_locator(MultipleLocator(1))    \n\ndef numgroups(element,low,high,jar_spec):\n    \"\"\"\n    non-fruitful function. Outputs a graph if the number of types of\n    glass within the DataFrame is &gt; 1.  Otherwise, the function prints\n    a message, for aesthetic reasons.\n\n    parameters:\n    element: an element in the periodic table\n    low: A minimum value for the measured element, which acts as the \n    minimum value to allow into the dataset sample\n    high: A maximum value for the measured element, which acts as the \n    maximum value to allow into the dataset sample\n    jar_spec: A value of either 'clear facsimile' or 'Nikumaroro jar', used\n    in the title of the graph. \n    \n    However, when the function is called to obtain the complete distri-\n    bution of counts, the value of jar_spec is set equal to None. \n    \n    Preconditions:\n    element is a string with value of: 'Mg' or 'Ca' or 'Ba'\n    \n    Low cannnot be greater than high.\n    \"\"\"\n    jar_spec='full range of ' + element + ' measurements in the database' if jar_spec==None else jar_spec\n    rangedict = {'Ba':['Barium',[.37,.74]],\n                 'Mg':['Magnesium',[2.4,4.3]],\n                 'Ca':['Calcium',[3.6,8.5]]}\n    facsimile_ref=rangedict[element][1][0]\n    artifact_ref=rangedict[element][1][1]\n    element_full_name=rangedict[element][0]\n    sampl = dataset[(dataset[element] &gt;= low) & (\n        dataset[element] &lt;= high)]\n    number_groups=sampl['GlassType'].value_counts().shape[0]\n    titletext1=\"Count of Glass Types in the 1987 database \\n with a range of measured values of \" + \\\n    element_full_name + \" \\n between \" + str(low) + \" and \" + str(high) + \" wt%. \\n \" + \\\n    \"This is the range of counts for the \" + jar_spec.upper()\n\n    if 'clear' in jar_spec or 'Niku' in jar_spec:\n        #Tolerance provides information in the report about the range of values considered in the graph.\n        tolerance=round((high-low)/2,2)\n        titletext2=\" that fall within a tolerance of +-\" + str(tolerance) + ' of its measurement.'\n    else:\n        tolerance=0\n        titletext2=\".\"\n        \n    if low&gt;high:\n        print('Low value must be smaller than the high value. Try again.')\n        return\n    \n    if number_groups==1:\n        print(titletext1 + titletext2)\n        print('There is only ONE sample Type.')\n        print(sampl['GlassType'].values[0],'=',sampl.shape[0])\n        print('reference: Clear facsimile jar =',facsimile_ref,'wt%'\n              '\\n Nikumaroro jar =',artifact_ref,'wt%')\n    else:\n        print('reference: Clear facsimile jar =',facsimile_ref,'wt%'\n              '\\n Nikumaroro jar =',artifact_ref,'wt%')\n        try:\n            vc=sampl['GlassType'].value_counts()\n            int('d') if len(vc)==0 else int('1')\n            \n            vc.plot(\n                kind='bar', figsize=(2.4*number_groups, 6), rot=0, cmap='Spectral');\n            plt.xlabel(\"Glass Type\", labelpad=14,fontsize=16,rotation=0)\n            plt.ylabel(\"Count of Type\", labelpad=70, \n                       fontsize=16,rotation=0)\n            plt.xticks(fontsize=14)\n            plt.yticks(fontsize=14)\n            plt.figtext(0.5, 1.0, titletext1+titletext2, wrap=True, horizontalalignment='center', fontsize=16)  \n            ax = plt.subplot()\n            ticks_restrict_to_integer(ax.yaxis)\n            plt.show()\n        except Exception as e: \n            #print(e)\n            print('For the ' + jar_spec + ', no glass samples in the U.K. data are in the range of ' + str(low) +\n                \" and \" + str(high) + ' wt% for ' + element_full_name + '.')\n            print()\n    return\n\n\n\nWhat types of glass in the 1987 database are most similar to the Nikumaroro jar and clear facsimile in terms of magnesium, barium or calcium content?\nTo review what was stated above, examining Mg, Ba, and Ca individually in the U.K. database, we can perform the following steps to produce a report: 1. Obtain the complete distribution of counts in the 1987 database by restricting the element’s values to the range between its minimum and maximum wt% values. 2. See the distribution of counts in the 1987 database that results from setting the range of measured wt% for the element to a tolerance 0.15 wt% above and below its measurement for the clear facsimile. 3. See the distribution of counts in the 1987 database that results from setting the range of measured wt% for the element to a tolerance 0.15 wt% above and below its measurement for the Nikumaroro jar. 4. Repeat steps 1 to 3 for the next element until all elements have been reported."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-magnesium",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-magnesium",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Report for Magnesium",
    "text": "Report for Magnesium\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nimport random\n\ndataset.replace({\"GlassType\": dict},inplace=True)\nelement='Mg'\nMgdict={'clear facsimile':[2.25, 2.55],'Nikumaroro jar':[4.15,4.45]}\nlow=0\nhigh=dataset[element].max()\n\n#Run report for all glass types\nnumgroups(element,low,high,None)\n\n#Run reports for the clear facsimile and Nikumaroro jar\nfor jar_type in Mgdict:\n    low=Mgdict[jar_type][0]\n    high=Mgdict[jar_type][1]\n    numgroups(element,low,high,jar_type)\n\nreference: Clear facsimile jar = 2.4 wt%\n Nikumaroro jar = 4.3 wt%\nreference: Clear facsimile jar = 2.4 wt%\n Nikumaroro jar = 4.3 wt%\nreference: Clear facsimile jar = 2.4 wt%\n Nikumaroro jar = 4.3 wt%\nFor the Nikumaroro jar, no glass samples in the U.K. data are in the range of 4.15 and 4.45 wt% for Magnesium.\n\n\n\n\n\n\n\n\n\n\nMagnesium Analysis\nMagnesium: The clear facsimile jar has 2.4 wt% Mg. There are only two glass types, tableware and non-float windows, with values between 2.25 and 2.55 wt% Mg. Magnesium, if it were the only feature, would seem to predict the clear facsimile jar to be in the window or the tableware family, a close cousin to the container family. The Nikumaroro jar has 4.3 wt% Mg. Checking back to the report we created with the describe() method, this value is well above the 75th percentile. We know from the literature on glassmaking that any Mg measurement from a modern glass sample that is above 3.5 wt% is likely to be a window, and not a container.[2]"
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-calcium",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-calcium",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Report for Calcium",
    "text": "Report for Calcium\n\ndataset.replace({\"GlassType\": dict},inplace=True)\nelement='Ca'\nCadict={'clear facsimile':[3.45, 3.75],'Nikumaroro jar':[8.35,8.65]}\nlow=0\nhigh=dataset[element].max()\n\n#Run report for all glass types\nnumgroups(element,low,high,None)\n\n#Run reports for the clear facsimile and Nikumaroro jar\nfor jar_type in Cadict:\n    low=Cadict[jar_type][0]\n    high=Cadict[jar_type][1]\n    numgroups(element,low,high,jar_type)\n\nreference: Clear facsimile jar = 3.6 wt%\n Nikumaroro jar = 8.5 wt%\nreference: Clear facsimile jar = 3.6 wt%\n Nikumaroro jar = 8.5 wt%\nFor the clear facsimile, no glass samples in the U.K. data are in the range of 3.45 and 3.75 wt% for Calcium.\n\nreference: Clear facsimile jar = 3.6 wt%\n Nikumaroro jar = 8.5 wt%\n\n\n\n\n\n\n\n\n\nCalcium Analysis\nCalcium: The clear facsimile jar has 3.6 wt% Ca. We can see from the report we created with the describe() method that this is off-scale low, a value less than all the samples in the database. No glass samples are in the range of 3.45% and 3.75% weight calcium. The Nikumaroro jar has 8.5 wt% Ca. This is close to the mean in the database (8.96) for all glass types, but no containers are displayed on the graph within +- .15 of the calcium value measured on the Nikumaroro jar."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-barium",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#report-for-barium",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Report for Barium",
    "text": "Report for Barium\n\ndataset.replace({\"GlassType\": dict},inplace=True)\nelement='Ba'\nBadict={'clear facsimile':[0.22, 0.52],'Nikumaroro jar':[0.59,0.89]}\nlow=0\nhigh=dataset[element].max()\n\n#Run report for all glass types\nnumgroups(element,low,high,None)\n\n#Run reports for the clear facsimile and Nikumaroro jar\nfor jar_type in Badict:\n    low=Badict[jar_type][0]\n    high=Badict[jar_type][1]\n    numgroups(element,low,high,jar_type)\n\nreference: Clear facsimile jar = 0.37 wt%\n Nikumaroro jar = 0.74 wt%\nreference: Clear facsimile jar = 0.37 wt%\n Nikumaroro jar = 0.74 wt%\nreference: Clear facsimile jar = 0.37 wt%\n Nikumaroro jar = 0.74 wt%\n\n\n\n\n\n\n\n\n\n\n\n\nBarium Analysis\nBarium: The ratio of 2:1 for the element Barium in the two glass samples (.74 for the Nikumaroro jar and .37 for the clear facsimile jar) suggests that the glass maker, Hazel-Atlas, used a recipe. This is not uncommon in the glass industry. For the clear facsimile, one window, one container and one headlamp have measurements of barium in the database that are within +- 0.15 of its measured value of barium. For the Nikumaroro jar, eight headlamps and one window have measurements of barium in the database that are within +-0.15 of its measured value of barium.\nConclusion: We have not yet applied machine learning to the identification of the Nikumaroro jar or the clear facsimile, but it would appear from the weight percent of some of the key ingredients from these jars that windows, of the float or non-float variety, and, to a lesser extent, headlamps, are strong candidates for how the 1987 database might predict their identity, if machine learning were employed as a predictive tool to do this. Nevertheless, some containers did appear in the bar graphs for the clear facsimile, indicating that there is at least a slight resemblance between that jar and containers from the U.K. database. The results are not exactly encouraging to the success of a machine learning model, but they are not discouraging enough to dampen curiosity at discovering what machine learning might be able to do with the data from the Nikumaroro jar and the clear facsimile.\n\n\nTurning the question around: If one were to subset the 1987 database only for containers, how would that database compare with the two jars? Both of the jars are, of course, containers.\n\nsampcont = dataset[dataset['GlassType'].isin(['Container'])]\nsampcont=sampcont.drop(['ID', 'GlassType'], axis=1)\nprint('Container summary report:')\ndisplay(sampcont.describe())\nrangedict = {'Ba':['Barium',[.37,.74]],\n                 'Mg':['Magnesium',[2.4,4.3]],\n                 'Ca':['Calcium',[3.6,8.5]]}\nELEMENTS=['Mg','Ca','Ba']\nfor element in ELEMENTS:\n    print()\n    print(\n        element,': min to max:',sampcont[element].min(),'to'\n        ,sampcont[element].max())\n\n    facsimile_ref=rangedict[element][1][0]\n    artifact_ref=rangedict[element][1][1]\n    print('reference: Clear facsimile jar =',facsimile_ref,'wt%'\n              '\\n Nikumaroro jar=',artifact_ref,'wt%')\n\nContainer summary report:\n\nMg : min to max: 0.0 to 2.68\nreference: Clear facsimile jar = 2.4 wt%\n Nikumaroro jar= 4.3 wt%\n\nCa : min to max: 5.87 to 12.5\nreference: Clear facsimile jar = 3.6 wt%\n Nikumaroro jar= 8.5 wt%\n\nBa : min to max: 0.0 to 2.2\nreference: Clear facsimile jar = 0.37 wt%\n Nikumaroro jar= 0.74 wt%\n\n\n\n\n\n\n\n\n\nRef_ix\nNa\nMg\nAl\nSi\nK\nCa\nBa\nFe\n\n\n\n\ncount\n13.000000\n13.000000\n13.000000\n13.000000\n13.000000\n13.000000\n13.000000\n13.000000\n13.000000\n\n\nmean\n1.518928\n12.827692\n0.773846\n2.033846\n72.366154\n1.470000\n10.123846\n0.187692\n0.060769\n\n\nstd\n0.003345\n0.777037\n0.999146\n0.693920\n1.282319\n2.138695\n2.183791\n0.608251\n0.155588\n\n\nmin\n1.513160\n11.030000\n0.000000\n1.400000\n69.890000\n0.130000\n5.870000\n0.000000\n0.000000\n\n\n25%\n1.516660\n12.730000\n0.000000\n1.560000\n72.180000\n0.380000\n9.700000\n0.000000\n0.000000\n\n\n50%\n1.519940\n12.970000\n0.000000\n1.760000\n72.690000\n0.580000\n11.270000\n0.000000\n0.000000\n\n\n75%\n1.521190\n13.270000\n1.710000\n2.170000\n73.390000\n0.970000\n11.530000\n0.000000\n0.000000\n\n\nmax\n1.523690\n14.010000\n2.680000\n3.500000\n73.880000\n6.210000\n12.500000\n2.200000\n0.510000\n\n\n\n\n\n\n\nWhen we restrict the U.K. database to only containers, the comparisons with the Nikumaroro jar and the clear facsimile enter into sharper relief. The clear facsimile has a high magnesium content for a container but this is still within range of the lowest and highest values within the U.K. database. The jar from Nikumaroro has a magnesium content much higher than that of the highest nearest neighbor in the U.K. database.\nThe clear facsimile has a calcium measurement lower than that of all the containers in the U.K. database. This indicates, perhaps, that calcium was not as heavily used in glassmaking in the early part of the twentieth century. The Nikumaroro jar has a calcium content, 8.5, that is lower than the 25th percentile. This is a low value for a container, especially when one considers that the mean for containers in the U.K. database, 10.12, is skewed toward the higher end of the range.\nThe clear facsimile jar and the Nikumaroro jar are within the range of minimum to maximum in barium weight percentage for containers; however, most of the measured values of barium in the database are zero. The measured values of barium for the clear facsimile and the Nikumaroro jar are still far higher than those usually found in the U.K. database.\n\n\nWhat do the correlations between elements for the different types of glass in the 1987 database reveal about late 20th century glassmaking, as compared with early 20th century glassmaking?\nTo answer this question, we can generate custom diverging colormaps from the U.K. database (excluding our samples of clear facsimile and Nikumaroro jar). The correlation colormaps, also known as heatmaps, show, for example, which elements of the glass most strongly correlate with refractive index, which may be considered synonymous with brilliance.\nTo keep the number of graphs to a manageable amount, we will restrict them to the glass types of Window Float, Window Non-Float and Containers. These three heatmaps provide a good summary of the correlations of ingredients in glassmaking of the late twentieth century.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ipywidgets import interact\n\n# There is a warning about setting a copy of a slice from a dataframe: ignore\nwarnings.filterwarnings('ignore')\n\nTYPES = ['Window Float', 'Window Non-Float', 'Container']\n\ndef get_dataset(src, glass_type):\n    #subset the df into a new df\n    df = src[src.GlassType == glass_type]\n    df.drop(['ID'], axis=1,inplace=True)\n    return df\n\ndef make_heatmap(source, title):\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    corr=source.corr()\n    # Generate a mask for the lower triangle\n    mask = np.tril(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, cmap=cmap, annot=True, fmt=\".2f\",\n    vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.title(title, fontsize=16)\n    plt.xticks(fontsize=14,rotation=0)\n    plt.yticks(fontsize=14,rotation=0)\n    plt.gcf().set_size_inches(15, 8)\n    return plt, title\n\ndef update_heatmap(glass_type):\n    src = get_dataset(dataset, glass_type)\n    title = make_heatmap(src, glass_type + ' glass correlation')\n    plt.title.text = title\n    plt.show()\n\nfor kind_of_glass in TYPES:\n    update_heatmap(kind_of_glass)\n\n# Instead of using a for-loop, it is also possible to have an interactive exhibit, using this Python syntax:\n#interact(update_heatmap, glass_type=TYPES);\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Correlation Heat Maps\nOne might expect barium, an ingredient used to increase brilliance, thus increasing refractive index, would be positively correlated with refractive index in these graphs. Barium, however, is frequently measured at 0.0 weight percent in the 1987 dataset or present in only trace amounts. It may be that late 20th century glassmaking does not incorporate barium to the same degree that it was incorporated into glass production of the early twentieth century. For an example of the importance of barium in the early 20th century, see a 1936 Hazel Atlas glass patent here.\nCalcium, in the form of calcium oxide, is highly correlated in the 1987 database with refractive index. This is due to the fact that calcium oxide increases refractive index. See https://www.academia.edu/12254939/Optical_and_mechanical_properties_of_calcium_phosphate_glasses for a study of this effect. There is good evidence that calcium was the element of choice to increase brilliance in 1980s glass production, rather than barium. The toxic effects of barium, which is a heavy metal, were becoming much better understood by the time the U.K. researchers assembled their database. The World Health Organization published a memorandum in 1991 in which it specifically warned of the dangers of barium in glass production. One prominent U.S. patent from 2013 specifically mentions CaO (calcium oxide) as an ideal substitute for metals such as barium and lead.\nBy contrast, in the early twentieth century barium was a favorite ingredient of glassmakers. As Francis Flint described in his patent (cited above), the use of barium sulfate increased brilliance, but the sulfates needed then to be reduced to prevent small seeds forming in the glass mixture, thus reducing the quality of the glass. Flint recommended zinc, magnesium, aluminum or tin as reducing agents. Sodium and calcium have been recommended in more modern literature of the art.[3] In window non-float glass, aluminum is positively correlated with barium.\nIt would seem good practice to analyze the correlations of each of these glass types separately, as we have done, since obviously the desired qualities of the glass will differ depending on the uses to which the glass will be put, and thus recipes will differ accordingly. The desired refractive index and brilliance of vehicle float glass will be far different than that of container or tableware glass, for example.\nThe elemental correlations in this 1987 database suggest changing priorities between production techniques of the early twentieth century and production techniques of the 1980s, toward more utilitarian styles and techniques. One does not require statistics to observe that container glass with high refractive index has become less common, if it ever was, giving way to containers in which seeing the contents clearly through the glass is the overriding concern. This change will probably affect what specific types of glass that a machine learning model can correctly predict for much earlier samples that it has not seen.\nNow we turn to our last question.\n\n\nUsing machine learning to train a model on the 1987 database, can that model be used to identify the type (container) of one or both of the jars unseen by the model?\nThis should be a straightforward machine learning classification problem. The first step is to gather up relevant modules from various sci-kit learn and imbalanced learning libraries, using multiple import statements.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error, cohen_kappa_score, classification_report\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold as SKF\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom numpy import set_printoptions, where\n\n\n\nSetting the stage for an attractive output\nNext, we will address the fact that the output we want to see needs to be prepared for attractive presentation. After we have generated predictions for the identity of the jars, we will generate, additionally, probability reports that outline the likelihood of the predictions. The standard probability scores supplied by the predict_proba function appear rather clumsily, as:\n[[0.42 0.4 0.07 0.04 0. 0.07]]\nThis output is a sequential list of probabilities that corresponds to the sequence of integer types supplied by the original 1987 database. The sequence is implied; that is, the 0.42 above is presumed to be the first type, Window Float, the 0.4 is presumed to be the second, and so on.\nIn their stead, it would be nice to have an equivalent and more readable report that formats the 2d array to appear like this:\nThe Nikumaroro jar has a probability of:\nA% to be a Window Float\nB% to be a Window Non-Float\nC% to be a Vehicle Float\nD% to be a Headlamp\nE% to be a Container\nF% to be a Tableware\nTo achieve this, we will want to re-sort the 2D list by descending order of probability, but when this is done, the implied sequence will no longer be of any use in identifying the glass types.\nTo solve for this, we will make an explicit counter variable. This new explicit counter, once created, will correspond with the original integer type values supplied by the 1987 database and will not be subject to the alteration that the implied index would experience when sorting.\n\nfrom colorama import Fore, Back, Style\n\ndef make_nicer_probability_output(array_to_convert,title):\n    \"\"\"\n    Author: Joe Cerniglia\n    Date: March 20, 2022\n    A function to convert the standard probability scores in Python machine learning libraries\n    to a report that supplies the categories from a dictionary and sorts the list of probabilities in\n    descending order.\n    \n    Parameters:\n    array_to_convert: an array to convert to a sorted 2d list \n    title: some text to be placed in the title or headings of the report\n    Preconditions:\n    The array must have the exact number of elements and format needed for the dictionary\n    and must be the output of a call to model.predict_proba\n    \n    returns None. The function itself prints the report.\n    \"\"\"\n    prob_list = array_to_convert.tolist()\n    #print(prob_list)\n    \n    # The counter can be used as a dictionary key for a \n    # dictionary we will create in the next step\n    counter=0\n    for probability in prob_list[0]:\n        prob_list[0][counter]=[counter,probability]\n        counter+=1\n    #print(prob_list)\n    # We have a 3d list; get back to the 2d list\n    prob_list=prob_list[0]\n    # Sort in descending order the second column of each row in the 2d list\n    # This allows for a descending order of probability and for the\n    # predicted type (highest probability) to rise to the top of the list\n    \n    # The lambda expression below takes each line (l) of the list as its \n    # argument. The expression after the colon, l[1], is the function, \n    # which takes the second variable in the line and sorts the line in \n    # descending order by that variable\n    prob_list=sorted(prob_list,key=lambda l:l[1], reverse=True)\n\n    counter=0\n    for prediction in prob_list:\n        if counter==0:\n            print(Fore.BLACK + 'The ' + title + ' has a probability of:')\n        if prediction[0]==3:\n            pred=Fore.RED + probdict[prediction[0]]\n        else:\n            pred=Fore.BLACK + probdict[prediction[0]]\n        print(Fore.BLACK + \"{:.0%}\".format(prediction[1]),'to be a', pred)\n        counter+=1\n    return None\n\nThere is one last piece of code that is needed to have this function work properly. We need to define a dictionary that will translate the counter variable above into English. This dictionary is called in the last for-loop of the function above. Notice that this dictionary is different than the one we created at the beginning, since it is now indexed sequentially starting at 0 and explicitly omits Vehicle Non-Float, a category for which there exist no examples in the database. Note also that we will only use this dictionary inside this function. When we need to translate the numerical glass types found in the 1987 database elsewhere in this code, we will still use the dictionary (dict) that we defined earlier.\n\n# This dictionary orders the possibilities of the 2d\nprobdict = {0: 'Window Float',\n  1: 'Window Non-Float',\n  2: 'Vehicle Float',\n  3: 'Container',\n  4: 'Tableware',\n  5: 'Headlamp'}\n\n\n\nSplit the data into feature set X and target set Y.\nNow with the correct modules imported, and our utility function defined, we can split the database into two arrays. First, we can use the pandas dataframe values method to convert the entire dataframe we set aside earlier to an array. Next, we can slice the array column-wise into X and Y arrays, with X as our features array and Y as our target array. Note that our columns are sliced from the number 1, which is actually the second variable in the array, not from the number 0. The reason is that we need to drop the ID variable. The ID variable is an index. It would improve the accuracy of our machine learning model (to 100% in fact!), but this accuracy would be a mirage. It would not accomplish our goal of training the model in how to classify unseen examples.\n\narray = dataset_ml.values\nprint(type(array))\nX = array[:,1:10]\nY = array[:,10]\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nShow the list of data features ranked by their power to influence prediction of the target variable.\nEarlier, we stated that we would make some educated guesses as to the values of the jars’ features that were either unavailable (refractive index) or not stated in the required units of weight percent (Fe, K, Si). We did not do this without some trepidation, since tampering with these features’ values would appear to reduce the rigor of our analysis. However, the benefits of having a complete training dataset appeared to outweigh these drawbacks.\nThe impact of this decision was unknown, but it was not unknowable. There is a method to assess the power of a given feature to influence the classification of a given sample of glass. This method is known as feature selection. Using feature selection, we may see a list of all the features in the dataset ranked in order of strongest to least strongest influence on the prediction of the class. If the features we modified were ranked highly in this list, we should be concerned about the integrity of the analysis. If the features we modified were not ranked highly in this list, we can proceed with our analysis with the confidence that the algorithm will be untroubled by our expedient modifications to the original data.*\nThe results were as favorable as one might have hoped. Using the SelectKBest class from the scikit-learn library, we can see that the variables for which we needed to supply values (highlighted in gold) were ranked toward the bottom of the ranked list.\nThe code that appears below takes the Numpy array created by SelectKBest and transforms it into a concise report that lists the relative importance, ranked descending, for all of the features in the glass database.\n\n(*Note that we also evaluated the effect of the features empirically. An additional experiment to run this program with a range of values for K, Si, Fe, and Ref_ix did not materially alter the predictions of the machine learning algorithm.)\n\n\nnames=['Ref_ix','Na','Mg','Al','Si','K','Ca','Ba','Fe']\n\n# feature extraction using univariate selection\ntest = SelectKBest(score_func=f_classif, k=4)\nfit = test.fit(X, Y)\n# summarize scores\n\n# Convert scores to dataframe\ndf_scores = pd.DataFrame(fit.scores_)\n# Convert names list to dataframe\ndf_names = pd.DataFrame(names) \n# Join two dataframes by indexes\nbest_features=pd.concat([df_scores, df_names], axis=1)\n# change the type of the columns from int to str\nbest_features.columns = best_features.columns.astype(str)\n# rename the columns to have more sensible names\nbest_features.columns.values[0] = \"Score\"\nbest_features.columns.values[1] = \"Feature_Name\"\n\n# sort the rows (features) by rank in descending order\nbest_features.sort_values(by='Score', ascending=False, inplace=True)\n# Add a column for rank to the dataframe\nbest_features['Rank'] = np.arange(1,len(best_features)+1)\n# Re-order the columns\nbest_features = best_features[['Feature_Name', 'Score','Rank']]\n# Format the dataframe to give the score two decimal places\nbest_features = best_features.style.format({'Score': \"{:.2f}\"})\n#print(type(best_features))\nbest_features=best_features.data\n\ndef color_relevant_gold(row):\n    \"\"\"\n    Takes a dataframe row and returns a string with\n    the background-color property `'background-color: gold'` for relevant\n    rows, black otherwise.\n    \"\"\"\n    if (row.values[0] == 'K'  or row.values[0] == 'Ref_ix' or row.values[0] == 'Si'\n        or row.values[0] == 'Fe'):\n        color = 'gold'\n    else:\n        color = ''\n    return ['background-color: %s' % color]*len(row.values)\n\n\nprint('      ----Best features----')\nbest_features.style.apply(color_relevant_gold, axis=1)\n\n      ----Best features----\n\n\n\n\n\n\n\n \nFeature_Name\nScore\nRank\n\n\n\n\n2\nMg\n65.544521\n1\n\n\n7\nBa\n38.974602\n2\n\n\n3\nAl\n35.726676\n3\n\n\n1\nNa\n28.548019\n4\n\n\n5\nK\n8.748128\n5\n\n\n6\nCa\n2.971426\n6\n\n\n4\nSi\n2.787330\n7\n\n\n8\nFe\n2.710882\n8\n\n\n0\nRef_ix\n1.608955\n9\n\n\n\n\n\n\n\n\nSplit the data into training and validation sets.\nTo create the conditions under which we will have the ability to test our future model’s effectiveness at classifying unseen data, we can now split both X and Y into validation and training arrays. The “train” pair of arrays can then be used for training the model, and the “validation” pair of arrays can be used to demonstrate how effective the model is after we have trained it. To do this, we will use the train_test_split function from sklearn’s model_selection library. We will create an 80-20 split of the data by entering a test_size parameter of 0.20. The training set will be 80% of the data, and the validation set will be 20% of the data. We will stratify the data so that the relative proportion of glass types in both pairs of training and validation sets is equivalent, despite the fact that the arrays are of different overall sizes.\n\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n    test_size=validation_size, random_state=seed, stratify=Y)\nprint(type(X_train))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nAdd the Nikumaroro jar and clear facsimile into the X and Y validation datasets.\nNext, we need to add our two jars (clear facsimile and Nikumaroro jar) to the validation arrays. The features of these two jars will be added to X_validation, and the target (glass type) of these two jars will be added to Y_validation. Having accomplished this, we will then have added two glass samples that the study authors could not possibly have anticipated when they built their database in 1987. This will be a great test of the skill of machine learning algorithms in general and of the completeness of their original dataset in particular.\nTo demonstrate that our jars have been added successfully, we will print out the shape of the validation arrays both before and after performing the append operations. Note that the shape returns a tuple, providing the number of rows, followed by the number of columns, separated by a comma.\nThe append operations for arrays are a little tricky. To do this, in addition to using numpy’s append method, we need to chain to that the reshape method to size the array appropriately prior to appending to it. The chained command executes from right to left, first reshaping the array to which we are appending, and then performing the append operation itself.\n\n#names=\n#  ['Ref_ix','Na',  'Mg',   'Al', 'Si',  'K',  'Ca',  'Ba',  'Fe']\n\nartifact_features=[1.52369,  13.1,  4.3,  .74,   72.37,  .24,  8.5,  .74,  .02]\nfacsimile_features=[1.51316, 11.7,  2.4,  .85,   72.37,  .12,  3.6,  .37,  .01]\nartifact_features_array=np.array(artifact_features)\nfacsimile_features_array=np.array(facsimile_features)\nprint('shape of X_validation before adding jars:',X_validation.shape)\n\nX_validation=np.append(\nX_validation,artifact_features_array).reshape(X_validation.shape[0]+1,9)\nX_validation=np.append(\nX_validation,facsimile_features_array).reshape(X_validation.shape[0]+1,9)\nprint('shape of X_validation after adding jars:',X_validation.shape)\n\n# 5 is equal to a container, the actual identity of the Nikumaroro jar and of the facsimile\nartifact_identity=[5.0]\nfacsimile_identity=[5.0]\nartifact_array=np.array(artifact_identity)\nfacsimile_array=np.array(facsimile_identity)\nprint('shape of Y_validation before adding jars:',Y_validation.shape)\n\nY_validation=np.append(\nY_validation,artifact_array).reshape(Y_validation.shape[0]+1)\nY_validation=np.append(\nY_validation,facsimile_array).reshape(Y_validation.shape[0]+1)\nprint('shape of Y_validation after adding jars:',Y_validation.shape)\n\nshape of X_validation before adding jars: (43, 9)\nshape of X_validation after adding jars: (45, 9)\nshape of Y_validation before adding jars: (43,)\nshape of Y_validation after adding jars: (45,)\n\n\n\n\nCreate a list of models to begin the testing process of choosing the best one.\nNow comes the heart of the machine learning program, the creation of models, also known as machine learning algorithms. There are many different kinds of machine learning algorithms. Each has its own strengths and weaknesses. The skill of machine learning algorithms on particular datasets will vary. Some will show very little skill in exposing the structure of the data, resulting in a model that cannot classify unseen glass examples very accurately. Others will show much greater skill. We want to test a variety of machine learning algorithms on the data so that we can select the model that is most likely to succeed in classifying the type of glass for this particular set of data.\nMachine learning algorithms come in two basic varieties: regression and classification. Regression algorithms treat our target variable (glass type) as a continuous variable. This means that they consider the targets as floating point numbers rather than as discrete integers. Classification algorithms treat the target variable as discrete categories. There is every advantage in testing both varieties, rather than trying to anticipate in advance which type of model might perform best.\n\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor(n_neighbors=5)))\nmodels.append(('CART Regressor', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('KNClass',KNeighborsClassifier(n_neighbors=5)))\nmodels.append(('RandomForest',RandomForestClassifier(\nn_estimators=100, max_features=9,class_weight='balanced')))\n\n\n\nScore the models.\nThere are many methods for scoring the effectiveness of different models so that they may be compared side by side. Some of these methods work optimally for regression models; others work optimally for classification models. The negative mean squared error, while not optimal for classification models, is acceptable for use in multi-class datasets such as this one, and it works very well for regression models. Thus, it is a good ‘all-purpose’ scoring method for our needs.\nWhat this code snippet does is to evaluate the success of each of the models defined above by repeatedly testing them on different randomized ‘folds’ of the data. The results of each test will not be the same. Each will vary to a lesser or greater extent, depending on which fold is selected. The score is actually a composite result of repeated tests on many folds. The negative mean square error score is expressed as a negative number, such that the highest negative number, that which is closest to zero, will be considered to have the best score. The standard deviation, given in parentheses, provides a sense of how much variation to expect within a given score.\n\n# Test options and evaluation metric\nscoring = 'neg_mean_squared_error'\n# evaluate each model in turn\nresults = []\nnames = []\nprint('Evaluation of Mean Square Error results of different models')\nfor name, model in models:\n  kfold = RepeatedStratifiedKFold(n_splits=7, n_repeats=3, random_state=7)\n  cv_results = cross_val_score(\n  model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)\n\nEvaluation of Mean Square Error results of different models\nLR: -1.206907 (0.388832)\nLASSO: -2.496606 (0.292277)\nEN: -2.112353 (0.293161)\nKNN: -0.980076 (0.386747)\nCART Regressor: -1.281429 (0.594722)\nSVR: -4.767184 (0.520934)\nKNClass: -1.683810 (0.841334)\nRandomForest: -1.137222 (0.696663)\n\n\n\n\nGraph the score results.\nThe test results above are complete, but they are a little hard to read. It would be far easier to evaluate a graphical representation of the data using a box plot. The box plot below takes each machine learning algorithm and places it alongside its neighbors for easy comparison. Note that there are tuning and scaling operations that might have incrementally increased the accuracy of our models, which have not been employed here.\nBased on the graph, it would appear that the Random Forest Classifier, a model with an excellent reputation among classification models, is an effective model, but KNN and Cart Regressor also seem competitive. In actual practice, however, KNN and CART produced a far less accurate result. When testing these models, the resulting confusion matrix (see below for an explanation of the confusion matrix) showed that these models are actually fairly weak.\nThis illustrates an important point: Multiple scoring methods are often required to see which model offers the best results. Still, a box graph such as the one shown below can narrow the number of judgments that must be made.\n\n # Compare Algorithms\nfig = plt.figure()\nfig.suptitle('UNTUNED Unscaled Algorithm Comparison',fontsize=20)\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.gcf().set_size_inches(15, 7)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\nOversampling, explained\nRandom Forest often pairs well with oversampling techniques, such as SMOTE (Synthetic Minority Over-sampling Technique). What SMOTE will do is to magnify certain classes that are sparsely represented in the training dataset. This magnification will allow the training process to resolve these under-represented classes more effectively, because it will have seen more examples than were initially presented. For example, tableware has only seven examples in the training dataset. By applying SMOTE to the training dataset, this number is magnified to 61. In fact, all of the glass types are ‘leveled’ to 61 examples after SMOTE is applied. This leveling will make it much easier for the Random Forest Classifier to classify unseen examples to a greater level of accuracy.\nNow we can observe a comparison of counts for each of the glass types before and after the application of SMOTE. This is only a test to see what the counts would be. Later on, we will apply SMOTE to the definition of our model. Our dictionary will come in handy here to translate the numerical glass types found in the 1987 database to their English equivalents.\n\ncounter=Counter(Y_train)\nprint('Counts Before SMOTE:')\nfor ele in counter:\n    print(dict[int(ele)],':',counter[ele])\noversample = SMOTE(random_state=42,k_neighbors=5)\nX_trainsm, Y_trainsm = oversample.fit_resample(X_train, Y_train)\ncountersm=Counter(Y_trainsm)\nprint()\nprint('Counts After SMOTE:')\nfor ele in countersm:\n    print(dict[int(ele)],':',countersm[ele])\n\nCounts Before SMOTE:\nWindow Float : 56\nHeadlamp : 23\nVehicle Float : 14\nWindow Non-Float : 61\nTableware : 7\nContainer : 10\n\nCounts After SMOTE:\nWindow Float : 61\nHeadlamp : 61\nVehicle Float : 61\nWindow Non-Float : 61\nTableware : 61\nContainer : 61\n\n\n\n\nVisualizing SMOTE\nSeeing the change in counts before and after the application of SMOTE oversampling is an excellent way of visualizing what SMOTE does. Another way to do this is to create a scatter plot on two elements of the periodic table and compare where the points are plotted before and after the application of SMOTE. The graphs following this function encode each dot with a color. The legend in the upper right corner of the graph shows which glass type each color represents.\n\nnames=['Ref_ix','Na','Mg','Al','Si','K','Ca','Ba','Fe']\ndef smotegraph(title_suffix, var_suffix=None):\n    \"\"\"\n    Author: Joe Cerniglia\n    Date: September 17, 2022\n    A function to plot two scatter graphs to show the distribution of glass types over two \n    elements of the periodic table.\n    \n    Parameters:\n    title_suffix: string that provides title text for which training dataset has been \n    graphed, original or SMOTE-oversampled.\n    var_suffix: string to indicate which training dataset to use in the plot\n    \n    returns None. The function produces graphs but does not return anything outside the function itself.\n    \"\"\"\n    # scatter plots of examples by class label: Aluminum (3) x Iron (8)\n    plt.figure(figsize=(7,5))\n    for label, _ in counter.items():\n        # row_ix returns an array containing all the row numbers for a given label (color)\n        if var_suffix==None:\n            row_ix = where(Y_train == label)[0] #slicing with zero allows access \n                #to the array embedded inside the tuple returned by the where\n            plt.scatter(\n                X_train[row_ix, 3], X_train[row_ix, 8], label=dict[label], alpha=1, s=30)\n        else:\n            row_ix = where(Y_trainsm == label)[0]\n            plt.scatter(\n                X_trainsm[row_ix, 3], X_trainsm[row_ix, 8], label=dict[label], alpha=1, s=30)\n    plt.legend(markerscale=1.5, fontsize=10)\n    plt.title(\"Glass of the UK database by Type\" + title_suffix, fontsize=20)\n    plt.xlabel(\"Al weight %\",fontsize=16)\n    plt.ylabel(\"Fe \\n weight %\",fontsize=16, rotation=0, labelpad=50)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.show()\n    return None\nsmotegraph(' - before SMOTE oversampling')\nsmotegraph(' - after SMOTE oversampling','sm')\n\n\n\n\n\n\n\nThe second graph clearly shows the number of dots on the graph increased. SMOTE creates observations that did not exist in the original U.K. database but which are nearest neighbors. Note especially how the upper right quadrant now has a number of containers (brown dots) where only one existed before! These observations have moderate amounts of iron in them. Iron increases refractive index, which generally means that glass with high iron content is less transparent. Since containers, especially for products, once equated translucency with luxury, these synthetic observations created by SMOTE seem plausible. Note also the row of purple dots at the bottom of the graph. These are tableware samples created by SMOTE that have low iron content, and thus are highly transparent, which seems quite correct for tableware.\n\n\nBuild data pipelines that can be used to input the training data.\nNow that we have seen what the counts would be for an oversampled training dataset, we can create a data pipeline that combines SMOTE with our previously selected model, Random Forest Classifier. To test the efficacy of SMOTE, we should create two models. The first model we create will have the ability to apply a Random Forest Classifier to a non-oversampled dataset. The second will have the ability to apply Random Forest Classifier to an oversampled dataset. We will also want to add in a standard scaler to both model pipelines. The standard scaler takes features of different scale and makes them more similar in scale. This technique is very useful in further enhancing the model’s ability to classify. Note that even after we have defined our model pipelines, we have not yet supplied any data to them.\n\nmodelnoSMOTE=make_pipeline(\nStandardScaler(),RandomForestClassifier(\nn_estimators=100,max_features=9,class_weight='balanced'))\n\nmodelwithSMOTE=make_pipeline(\nStandardScaler(),SMOTE(\nrandom_state=42, k_neighbors=5),RandomForestClassifier(\nn_estimators=100,max_features=9,class_weight='balanced'))\n\n\n\nFit the non-SMOTE model to the training data to train the model.\nFirst, we will use and evaluate the non-SMOTE model. We will apply the non-SMOTE model to actual data by supplying it with our training data. The fit method applies the model by using the training features and targets to learn how to generalize to unseen validation data.\n\nmodelnoSMOTE.fit(X_train, Y_train);\n\n\n\nApply the trained non-SMOTE model to unseen data.\nNow we can test the model’s performance by applying it to unseen data (X_validation) and generating a report on the accuracy of its performance.\nThe variable called predictions takes our validation features and attempts to classify what type of glass the features for each example represents. Recall that we added in the two jars to our validation dataset, and so these predictions will include them.\n\npredictions = modelnoSMOTE.predict(X_validation)\n\n\n\nHow to Read a Confusion Matrix\nThe confusion matrix need not be confusing. It provides all the predictions in a matrix. Think of the columns and rows of the matrix as having been labeled with the names of the integer glass types, ordered left to right and top to bottom, in sequence. Row labels represent the true values, and column labels represent the predicted values. The intersection of true and predicted represents the status of any given cell in the matrix. Using this matrix, we can tell how many predictions were correct, how many were over-predicted, and how many were under-predicted. The diagonal line that can be drawn from the top left of the matrix to the bottom right, slicing the matrix into two equal diagonal halves, represents correct predictions (true=predicted). Each number in the same row as a correct prediction represents an under-prediction. In other words, the model under-predicts when the number of correct predictions for a category is less than the actual number in that category. By summing all of the numerals in any given row, we obtain the true number of glass types for each type. Each number in the same column as a correct prediction represents an over-prediction. In other words, the model over-predicts when the number of total predictions for a category in a given column is more than the correct predictions in that category.\n\nprint('Random Forest without SMOTE oversampling')\nprint(confusion_matrix(Y_validation, predictions))\n\nRandom Forest without SMOTE oversampling\n[[13  1  0  0  0  0]\n [ 1 13  0  1  0  0]\n [ 1  1  1  0  0  0]\n [ 1  0  0  3  0  1]\n [ 0  1  0  0  1  0]\n [ 2  0  0  1  0  3]]\n\n\n\n\nCohen’s Kappa\nThe Cohen’s Kappa statistic is a measure of inter-rater reliability. The “raters,” in this machine learning example, are the actual glass types and the predictions of those types. In addition to considering the extent of agreement between these two, it also includes statistical consideration of the probability that this agreement could have occurred by chance.\nThe kappa score for the non-SMOTE Random Forest model is considered reasonably good.\n\nkappa=cohen_kappa_score(Y_validation, np.round(predictions,0))\nprint('Kappa (1=perfect;0=chance;&lt;0=worse than chance): %f' % kappa)\n\nKappa (1=perfect;0=chance;&lt;0=worse than chance): 0.665314\n\n\n\n\nMean absolute deviation\nThe mean absolute deviation, or MAD, is a measure of the average distance between each data value and the average value of the dataset. In terms of the confusion matrix above, it provides a sense of how far the values are distributed away from the central diagonal line.\n\nMAD = mean_absolute_error(Y_validation, predictions)\nprint('MAD (Mean Absolute Dev): %f' % MAD)\n\nMAD (Mean Absolute Dev): 0.711111\n\n\n\n\nThe classification report\nThe classification report calculates precision and recall. Precision is the number of correct guesses in a class divided by the total guesses in a class. It can be obtained by observing the columns in the confusion matrix. Recall is the number of correct guesses in a class divided by the total number of actual members of the class. It can be obtained by observing the rows in the confusion matrix. The f1-score is the harmonic mean of precision and recall and is a good overall indicator of model effectiveness. Support is the true count for each type.\n\nprint(classification_report(Y_validation, predictions, zero_division=1))\n\n              precision    recall  f1-score   support\n\n         1.0       0.72      0.93      0.81        14\n         2.0       0.81      0.87      0.84        15\n         3.0       1.00      0.33      0.50         3\n         5.0       0.60      0.60      0.60         5\n         6.0       1.00      0.50      0.67         2\n         7.0       0.75      0.50      0.60         6\n\n    accuracy                           0.76        45\n   macro avg       0.81      0.62      0.67        45\nweighted avg       0.77      0.76      0.74        45\n\n\n\n\n\nObserve the predictions for the Nikumaroro jar and the clear facsimile.\nSince we already added the clear facsimile and Nikumaroro jar to the validation dataset, the model has already made its predictions for these two samples. However, the evaluation thus far has not directly revealed what those predictions actually were. Now we will want to observe for ourselves how the model behaves toward these particular instances of the data. To do this, we can simply enter the array for each sample that we created above into the predict method of the noSMOTE model. We may obtain the underlying probabilities for these predictions by using the predict_proba method of the noSMOTE model.\n\nyhat = modelnoSMOTE.predict([artifact_features])\nprint('Nikumaroro jar prediction:',dict[int(yhat)])\nyhat2 = modelnoSMOTE.predict([facsimile_features])\nprint('clear facsimile prediction:',dict[int(yhat2)])\nyhat_probability = modelnoSMOTE.predict_proba([artifact_features])\nyhat2_probability = modelnoSMOTE.predict_proba([facsimile_features])\n\nNikumaroro jar prediction: Window Float\nclear facsimile prediction: Headlamp\n\n\n\n\nDisplay the details of the jar prediction probabilities for the non-SMOTE model.\nNow we have the opportunity to use the utility display function we created above. Nesting calls to this function inside another function will allow the report to be generated with a single call when we next run a report for the SMOTE version of our model. We can also here include extra niceties such as colored text to highlight probabilities for the Container class and check marks to indicate where the prediction was correct.\n\ndef the_report():\n    if dict[yhat[0]]=='Container':\n        print(Fore.BLACK + 'Nikumaroro jar prediction:',Fore.RED + dict[yhat[0]]+' ' + u'\\u2713')\n    else:\n        print(Fore.BLACK + 'Nikumaroro jar prediction:',Fore.BLACK + dict[yhat[0]])\n    make_nicer_probability_output(yhat_probability,'artifact')\n    print()\n    if dict[yhat2[0]]=='Container':\n        print(Fore.BLACK + 'clear facsimile prediction:',Fore.RED + dict[yhat2[0]]+' ' + u'\\u2713')\n    else:\n        print(Fore.BLACK + 'clear facsimile prediction:',Fore.BLACK + dict[yhat2[0]])\n    make_nicer_probability_output(yhat2_probability,'clear facsimile')\n    return\n\nthe_report()\n\nNikumaroro jar prediction: Window Float\nThe artifact has a probability of:\n46% to be a Window Float\n28% to be a Window Non-Float\n17% to be a Headlamp\n9% to be a Vehicle Float\n0% to be a Container\n0% to be a Tableware\n\nclear facsimile prediction: Headlamp\nThe clear facsimile has a probability of:\n33% to be a Headlamp\n26% to be a Container\n19% to be a Window Non-Float\n16% to be a Window Float\n5% to be a Tableware\n1% to be a Vehicle Float\n\n\n\n\nRun the report again for the SMOTE model.\nHaving proceeded through a step-by-step analysis of the machine learning classification report we have built, we can now run the same report again, this time using the second SMOTE model we created. This model will, again: 1. oversample the data with SMOTE 2. scale the data with StandardScaler 3. classify each glass example with RandomForestClassifier and 4. output the numerical prediction and probabilities of glass type for the Nikumaroro jar and for the clear facsimile\n\nmodelwithSMOTE.fit(X_train, Y_train)\n\nprint()\nprint('Random Forest with SMOTE oversampling')\npredictions = modelwithSMOTE.predict(X_validation)\nytrue=5\npredictions=np.round(predictions, 0)\n\nprint(confusion_matrix(Y_validation, predictions))\nkappa=cohen_kappa_score(Y_validation, np.round(predictions,0))\nMAD = mean_absolute_error(Y_validation, predictions)\nprint('Kappa (1=perfect;0=chance;&lt;0=worse than chance): %f' % kappa)\nprint('MAD (Mean Absolute Dev): %f' % MAD)\nprint(classification_report(Y_validation, predictions, zero_division=1))\nyhat = modelwithSMOTE.predict([artifact_features])\nprint('Nikumaroro jar prediction:',dict[int(yhat)])\nyhat2 = modelwithSMOTE.predict([facsimile_features])\nprint('clear facsimile prediction:',dict[int(yhat2)])\nyhat_probability = modelwithSMOTE.predict_proba([artifact_features])\nyhat2_probability = modelwithSMOTE.predict_proba([facsimile_features])   \n\n\nRandom Forest with SMOTE oversampling\n[[12  2  0  0  0  0]\n [ 1 13  0  1  0  0]\n [ 1  0  2  0  0  0]\n [ 0  1  0  4  0  0]\n [ 0  0  0  0  2  0]\n [ 2  0  0  1  0  3]]\nKappa (1=perfect;0=chance;&lt;0=worse than chance): 0.730539\nMAD (Mean Absolute Dev): 0.555556\n              precision    recall  f1-score   support\n\n         1.0       0.75      0.86      0.80        14\n         2.0       0.81      0.87      0.84        15\n         3.0       1.00      0.67      0.80         3\n         5.0       0.67      0.80      0.73         5\n         6.0       1.00      1.00      1.00         2\n         7.0       1.00      0.50      0.67         6\n\n    accuracy                           0.80        45\n   macro avg       0.87      0.78      0.81        45\nweighted avg       0.82      0.80      0.80        45\n\nNikumaroro jar prediction: Window Non-Float\nclear facsimile prediction: Container\n\n\n\n\nDisplay the detail of jar prediction probabilities for the SMOTE model.\nOur final coding step will be to re-run the report of probabilities using our SMOTE-enhanced Random Forest Classifier model.\n\nthe_report()\n\nNikumaroro jar prediction: Window Non-Float\nThe artifact has a probability of:\n52% to be a Window Non-Float\n32% to be a Window Float\n6% to be a Container\n6% to be a Headlamp\n4% to be a Vehicle Float\n0% to be a Tableware\n\nclear facsimile prediction: Container ✓\nThe clear facsimile has a probability of:\n49% to be a Container\n20% to be a Window Non-Float\n14% to be a Headlamp\n11% to be a Window Float\n5% to be a Tableware\n1% to be a Vehicle Float\n\n\n\n\nAnalysis Procedure\nLet us review the machine learning process that we have just created and executed. The following sequence of operations was performed: 1) Create two arrays. The first contains the database features, all of the predictive columns of the original dataset. The second contains the target column, GlassType. 2) Create a train-test, stratified split so that the model can be trained on 80% of the data and validated on a subsample of 20%. 3) Add the two external glass sample jars to the validation (unseen) data, both for features (X) and for target (Y). 4) Test a set of regression and classification models on the training dataset and report the MSE (mean squared error) results for each one, as well as the standard deviation of the MSE. 5) Plot the preliminary effectiveness of each model on a box plot. 6) Select the model most likely to succeed. In this case, Random Forest Classifier was selected. 7) Build two models of Random Forest Classifier, one without oversampling and the other with oversampling. Use the standard scaler in both models to make the scale of the features more uniform with one another. 8) Fit the models to the training dataset. 9) Print a report on the performance of the models. 10) For each model, print a report on the predicted result for both of the two unseen jars and the probability of the prediction for each glass type.\n\n\nFindings\nOne of the most interesting findings was that when the SMOTE oversampling algorithm was used with the Random Forest Classifier, the resolving power of the model increased to the point that it was consistently able to identify the clear facsimile jar correctly as a container. Without the SMOTE oversampling algorithm applied, the model predicted the clear facsimile jar to be a headlamp. The prediction of container scored a distant third or fourth place.\nThe non-SMOTE model predicted the Nikumaroro jar to be a window, usually of the float variety*. This is very close to the prediction made by the model when SMOTE was used. Following application of SMOTE oversampling, the model predicted the Nikumaroro jar to be a non-float window.\nThese machine learning model predictions would seem to indicate that the clear facsimile jar can be correctly identified through prudent enhancement of the model with oversampling techniques. These techniques, however, are ineffective in increasing the ability of the model to predict the Nikumaroro jar to be what it is, a container.\nUsing a well-tuned model, the fact that a non-float window was predicted for the Nikumaroro jar demonstrates that this database of late 20th century glassware lacks enough relevant examples that would allow it to predict the Nikumaroro jar correctly.\n* Because the model is stochastic in nature, results will not be exactly the same each time.\n\n\nCovariate Drift and Why it Matters\nThis lack of relevant examples in the training data is a common phenomenon in machine learning, so common in fact that it has a name: covariate drift. Covariate drift is defined as a mismatch between the relative sizes or attributes of categories between the training and the validation data. There are many possible reasons for why covariate drift happens. The passage of time may have changed the population, or perhaps those assembling the training data did not obtain enough available samples from each population, or for whatever reason overlooked certain segments of a population altogether.\nWhen extrapolating information about different populations, whether those populations be glass types, people, or some other collection, we want the categories we survey to be representative of the populations upon which we wish to generalize. If our validation data is not representative of the data we have used to train our model, that is a problem that needs to be addressed.\nBefore we can address it, however, we need to check whether it really exists, and, if so, how much it exists. We need to measure it.\n\n\nMeasuring Covariate Drift\nShikhar Gupta, a data scientist at Amazon, has provided an excellent program/algorithm for obtaining quantitative measurement of covariate drift. His paper is here: https://towardsdatascience.com/how-dis-similar-are-my-train-and-test-data-56af3923de9b. I have modified his work by placing it inside a function that can generate multiple reports for different testing situations, and I have adjusted it for the particular situation of testing the U.K. database. I have also added in an ROC curve graph and a short report detailing the various data transformations that are happening as the function progresses.\nThe basic idea behind this function is to combine the training features (X_train) and validation features (Y_train) dataset, stacking one on top of the other, into a new features dataset, x. We can then create a new target dataset, y, by creating, for every observation in x, a row in y with a new variable, is_train. The is_train variable is a boolean value indicating whether or not each observation came from the original training dataset (prior to combining training and validation). The values in y may be used to compare predicted results with the actual results from the new target dataset.\nHaving combined training and validation into a new features dataset, x, and created a corresponding target dataset, y, we need a way to separate x and y horizontally into new training and validation datasets. We can create our new training and validation datasets by using stratified Kfold. Stratified Kfold will ensure that each subsection of the data that is produced will have the same ratio of training values (original training data) to validation values (original validation data). By the time all the folds have been produced, we will have created an array of predictions for every single observation in x. Each of these observations in x will have generated a prediction of whether it came from the original training or from the original validation. In essence, we are creating a ‘blind test’ in which the random probability that any given observation from x came from training or from validation is roughly equal.\nThis procedure is somewhat different than the traditional one of creating a single validation “holdout” dataset we would then use to test the skill of a given model. Instead, what we are doing is fitting each component Kfold to our Random Forest model, assigning a prediction to each observation in the training set created, and adding those predictions to a master array containing a prediction for every observation in the original x.\nEach of the predictions that we generate will also have a probability associated with it. The probability calculates how likely is it that that the observation came from the training dataset. This probability can be used to create weighting factors that measure quantitatively a unit-less “distance” between each observation in the final Kfold and the original training data.\nTo do this weighting, we filter the predictions on the row indexes of the training features dataset created from that last Kfold. This dataset of predictions contains the same ratio of training to validation as existed in the original x dataset. We can then plot the weights from this data onto a distribution plot. If the distribution of the weights in this Kfold shows a clear separation from the original training data, and if the number contained in that separated distribution is roughly equivalent to the number of observations in the Kfold that came from the original validation dataset, we will know then that the validation data we originally selected to test our model has caused covariate drift. If, on the other hand, the distribution shows a central tendency around the value of 1, we will know then that the validation data does not have covariate drift, that the origin of the data itself is not a predictive variable (as we would hope it would not be), and that in fact our validation data is well-enough matched to our training data that it may be used to generate valid, if not always accurate, predictions.\nHere is the modification of Shikhar Gupta’s original program to measure covariate drift:\n\ndef covariate_drift(validation, title, splits):\n    \"\"\"\n    Authors: Shikhar Gupta and Joe Cerniglia\n    Date: September 17, 2022\n    A function to create metadata that can be used to measure covariate drift.\n    This function generates two graphs:\n        1. a ROC curve showing the skill of the model at determining whether the data was\n        from training or from validation\n        2. a distribution curve showing the degree to which training and validation differ.\n    A report is also generated, including a confusion matrix.\n    Parameters:\n    validation: an array containing all of the validation examples we wish to compare\n    against the training data.\n    title: string to provide a title for the distribution curve\n    splits: integer number of splits for the stratifiedKfold. The reciprocal of this number will \n    determine the ratio of validation samples to training samples.\n    \n    returns None. The function produces graphs but does not return anything outside the function itself.\n    \"\"\"\n    # change validation to a pandas dataframe\n    names=['Ref_ix','Na',  'Mg',   'Al', 'Si',  'K',  'Ca',  'Ba',  'Fe', 'GlassType']\n    validation=pd.DataFrame(validation, columns=names)\n    \n    print('# training rows:',len(train))\n    print('# validation rows:',len(validation))\n    \n    #adding a column to identify whether a row comes from the training set or not\n    validation['is_train'] = 0\n    train['is_train'] = 1\n\n    #combining test and train data\n    df_combine=pd.concat([train, validation], axis=0, ignore_index=True)\n\n    #dropping 'target' column. We are creating a new target from the origin of the data: test or train\n    df_combine = df_combine.drop('GlassType', axis=1)\n\n    x = df_combine.drop('is_train', axis=1).values #leaves only independent variables\n    y = df_combine['is_train'].values #labels: the new target variable is is_train\n\n    print('Combined validation and train - feature variables (rows, columns):',x.shape)\n    print('Combined validation and train - target variable (rows, columns):',y.shape)\n\n    m = RandomForestClassifier(n_jobs=-1, max_depth=5, min_samples_leaf=5)\n    predictions = np.zeros(y.shape) #creating an empty prediction array that is as large as the original\n    #dataframe of combined values\n\n    #SKF is stratified kfold\n    skf=SKF(n_splits=splits, shuffle=True, random_state=100)\n    #Stratified kfold cross validation is an extension of regular kfold cross validation but \n    #specifically for classification problems where rather than completely random splits, \n    #the ratio between the target classes (in this case, train/test) is the same in each fold as it \n    #is in the full dataset.\n    for fold, (train_idx, validation_idx) in enumerate(skf.split(x,y)):\n        X_train, X_validation = x[train_idx], x[validation_idx] \n        y_train, y_validation = y[train_idx], y[validation_idx]\n        m.fit(X_train, y_train)\n        probs = m.predict_proba(X_validation)[:, 1] #calculating the probability that is_train = 1\n        # Each time through the loop a different set of rows in the training arrays is selected, fit to the\n        # model, and a different (smaller) set of rows in the validation array is assigned a prediction,\n        # based on the fitted model.\n        # By the time the loop of 20 is finished, every observation in the combined training\n        # and validation data has a prediction value.\n        # The last fold will be the one selected for X_train and X_validation,\n        # y_train and y_validation, but these arrays will not be needed later, except for counts.\n        # Also, the training indexes of the final fold will be needed to create the distribution plot.\n        # This last fold's weights, derived from predictions from the last training fold, will be the ones \n        # used in the distribution plot.\n        predictions[validation_idx] = probs\n    \n    # Short report about the results\n    print('Confusion matrix for combined training + validation predictions:')\n    print(confusion_matrix(y.astype(float), np.round(predictions,0)))\n    print('# training rows after stratified kfold:',len(X_train))\n    print('# validation rows after stratified kfold:',len(X_validation))\n    print('Case mix for X_train after Kfold:')\n    ylist = [str(x) for x in y_train.tolist()]\n    ylist = map(lambda x: x.replace('0','from validation'),ylist)\n    ylist = map(lambda x: x.replace('1','from training'),ylist)\n    print(Counter(list(ylist)))\n   \n    # ROC curve of the entire set of predictions\n    fpr, tpr, thresholds = metrics.roc_curve(y, predictions, pos_label=1)\n    print('ROC-Area Under Curve score for train and validation distributions:',metrics.auc(fpr, tpr))\n    sns.set_style(\"white\")\n    plt.plot(fpr,tpr)\n    plt.ylabel('True Positive Rate',fontsize=15)\n    plt.xlabel('False Positive Rate',fontsize=15)\n    plt.show()\n    \n    sns.set_style(\"whitegrid\")\n    # Distribution plot of the training weights\n    plt.figure(figsize=(10,5))\n    predictions_train = predictions[train_idx] # Filter the predictions on the row indexes of the \n    # training features dataset created from that last Kfold.\n    print()\n    print('# of total training samples in the graph:',len(predictions_train))\n    weights = (1./predictions_train) - 1. \n    weights /= np.mean(weights) # Normalizing the weights\n    plt.xlabel('Computed sample weight \\n \\n The higher the weight, the more dissimilar '  + \n               'each kfold sample is to samples from the original training dataset',fontsize=18)\n    plt.ylabel('# Samples',fontsize=18)\n    plt.xlim([min(weights), max(weights)])\n    plt.figtext(0.5, 1, title, wrap=True, horizontalalignment='center', fontsize=18)\n    g=sns.histplot(weights, kde=False, color='mediumpurple')\n    g.set_xticks(g.get_xticks()[1:])\n    g.set_xticklabels(g.get_xticks(), size = 15)\n    g.set_xticks(range(0,int(round(max(weights)+1,0)),1));\n    plt.show()\n    \n    return   \n\n\n\nSelecting the validation dataset to compare with the training set for covariate drift\nAn important consideration in testing for coviariate drift will be how to build the validation dataset used to compare with the training dataset. The following function handles a variety of testing scenarios. One apparent problem in this testing is that we only have two jars with which to compare. They are the only validation examples that are of interest, since they are the ones that we know did not come from the original U.K. database.\nTo make this small validation set comparable in size to the original U.K. database, we will want to copy the data obtained from these jars over and over again, and append each copy to the validation, until it reaches the size of the training dataset. This function repeats the copy process a set number of times until that desired size is reached.\n\ndef set_validation(validation, repeat, run_repeat, type_repeat=None):\n    '''Modifies a validation dataframe by appending to it repeated artifacts'''\n    if not run_repeat:\n        return\n    else:\n        for n in range(repeat):\n            if type_repeat=='mix':\n                validation.append(artifact_features+artifact_identity)\n                validation.append(facsimile)\n            elif type_repeat=='facsimiles':\n                validation.append(facsimile_features+facsimile_identity)\n                validation.append(facsimile)\n            elif type_repeat=='artifacts':\n                validation.append(artifact_features+artifact_identity)\n                validation.append(artifact_features+artifact_identity)\n    return\n\n\n\nTesting how the jar from Nikumaroro and the jar from eBay stack up to the U.K. database\n\nElements of the Report\nNow we can run our first test for covariate drift. After using the set_validation function to create a validation dataset of 215 observations comprised of data from the Nikumaroro jar and the clear facsimile, we will then run that validation data through the main program.\nThe first two lines returned by the report display the count of original U.K. database glass samples, which is 214. The number of validation rows created by the set_validation function is 215.\nOur next two lines provide verification that x (training) and y (validation) have been combined, as well as the resulting count of that combination.\nThe next element in the report is the confusion matrix. This matrix can tell whether the model has been successful in separating training data from validation data. (See the section, How to Read a Confusion Matrix, for a better appreciation of how to interpret them.) If the validation and training row counts appear in the top left and lower right quadrants, respectively, then we know that covariate drift is present in the data.\nAfter this, we see the counts obtained from the last stratified Kfold. The Kfold subsets a training dataset (95%) from the x. The remaining observations (5%) from x belong to the validation dataset.\nWe then want to verify that the ratio of training to validation in the original x and y (50:50) has remained in the stratified Kfold training dataset. The line that begins with the word ‘Counter’ will show that it has.\nThe ROC-Area Under Curve score visually represents how skillful the model was in separating training (U.K. database) from validation (Nikumaroro jar and clear facsimile). If the score is 1.0 and the curve is a right angle that intersects in the top left quadrant, we know then that the model has been completely skillful in this task.\nThe last exhibit is the distribution plot, which takes the training set from the last Kfold and bins the weights derived from that training set, which tells us how the observations in the training set compare with the original validation set. Those bins that center on 1.0 represent training examples that stand an equal chance of having come from original validation or training sets. (Ideally, all rows would hover at or near this point.) Those bins that center on 0.0 have a 100% probability of having come from the training set. The greater the positive distance the bins are from 1.0, the more unlike the original training set these observations are.\n\ntrain = dataset_ml.drop(['ID'], axis=1)\n# The artifact data are given as lists\nvalidation = [artifact_features+artifact_identity]\nfacsimile = facsimile_features+facsimile_identity\nset_validation(validation, 107, True,'mix')\ncovariate_drift(validation,'Test of whether adding repeated Nikumaroro jars and clear facsimiles \\n ' + \n                'as validation samples causes a covariate drift',20)\n\n# training rows: 214\n# validation rows: 215\nCombined validation and train - feature variables (rows, columns): (429, 9)\nCombined validation and train - target variable (rows, columns): (429,)\nConfusion matrix for combined training + validation predictions:\n[[215   0]\n [  0 214]]\n# training rows after stratified kfold: 408\n# validation rows after stratified kfold: 21\nCase mix for X_train after Kfold:\nCounter({'from training': 204, 'from validation': 204})\nROC-Area Under Curve score for train and validation distributions: 1.0\n\n# of total training samples in the graph: 408\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of covariate drift: Effect of Nikumaroro jar and clear facsimile\nThe exhibits returned by the first test show unambiguously that the jar from Nikumaroro and the clear facsimile cause covariate drift in the analysis. The distribution plot shows two distinct groups, representing a clean division of samples between the U.K. database and the jars. The spread of the jar group is rather wide, indicating that some examples from the U.K. database are more similar to the jars than others.\n\n\nAnalysis of second test of covariate drift: Effect of the training data on itself\nOur last test is a control experiment. We want to know whether the training data itself contains some inherent covariate drift of its own. Does the training data from the U.K. database show internal consistency with itself? This is also a good way to check for how a dataset potentially without covariate drift should appear.\nThis last test shows that the U.K. database is not perfectly consistent. Rather than the jagged bell curve that would appear in a highly normal distribution, our plot shows a bimodal distribution. This is most likely the result of two large categories of data within the U.K. database. Window and Non-window are the most likely separations that this plot is showing, with the smaller peaks toward the right possibly representing the less populated categories of tableware, headlamps, and containers.\nThe overall picture drawn by this plot, however, is of a database without covariate drift.\n\ntrain = dataset_ml.drop(['ID'], axis=1)\n## Test of whether the training dataset itself has covariate drift\nvalidation2 = train.copy()\ncomparison=covariate_drift(validation2,'Test of whether training data itself has covariate drift',20)\n\n# training rows: 214\n# validation rows: 214\nCombined validation and train - feature variables (rows, columns): (428, 9)\nCombined validation and train - target variable (rows, columns): (428,)\nConfusion matrix for combined training + validation predictions:\n[[  4 210]\n [206   8]]\n# training rows after stratified kfold: 407\n# validation rows after stratified kfold: 21\nCase mix for X_train after Kfold:\nCounter({'from training': 204, 'from validation': 203})\nROC-Area Under Curve score for train and validation distributions: 0.003559262817713333\n\n# of total training samples in the graph: 407\n\n\n\n\n\n\n\n\n\n\nTreating Covariate Drift\nMany articles offer treatment options for addressing covariate drift. The only true measure of the success of treatment is the success of the machine learning models. Was a given model able to identify the jars correctly? We have already seen that, using SMOTE oversampling, we were able to identify the clear facsimile correctly, despite the covariate drift that was in the validation samples. Would additional techniques be able to identify the Nikumaroro jar as a container?\nAlbert Um, in an article for Medium, https://albertum.medium.com/covariate-shift-in-machine-learning-adf8d0077f79, has written a program that uses logistic regression to calculate weighting factors that may be used to weight the model to favor those training examples that are most like the validation set. The weights may then be used in the model to fit the original training examples and then predictions can be made on the jars that would presumably be more accurate than they would have been without the weights.\n\n\nSetting up the data\nHere is how it works: The setup of the training dataset is identical to Shikhar Gupta’s algorithm, albeit with slightly different syntax. We take the original dataframe of the U.K. database and drop the ‘ID’ and ‘GlassType’ variables, since these will not be needed in the analysis. Then we add our ‘is_train’ column to the data, populating it with ‘1’, indicating that these observations came from training. We convert the training dataset to an array and split it vertically into features (X_train) and the target (Y_train), which is comprised solely of the is_train variable.\nWe then build our validation dataset exactly as we did previously, appending multiple copies to itself until we have a dataframe that is the same size as the training dataset. Then we add the is_train column to the validation dataset, populating it with a value of ‘0’. We then convert the validation dataset to an array and split that array vertically into features (X_validation) and the target (Y_validation), which is comprised solely of the is_train variable.\n\n# Recover original training dataframe\ntrain = dataset_ml.drop(['ID','GlassType'], axis=1)\n# Assign new target\ntrain['is_train'] = 1\n\n# Convert training dataframe to an array\narray = train.values\n\n# Split training into X and Y\nX_train = array[:,0:9]\nY_train = array[:,9]\n\n# Re-create validation consisting of artifacts\nvalidation = [artifact_features + artifact_identity]\nfacsimile = facsimile_features + facsimile_identity\n\n# Enlarge the validation set by appending copies\nset_validation(validation, 106, True, 'mix')\n\n# Convert the list to a dataframe\nnames=['Ref_ix','Na',  'Mg',   'Al', 'Si',  'K',  'Ca',  'Ba',  'Fe', 'GlassType']\nvalidation=pd.DataFrame(validation, columns=names)\n    \nprint('# training rows:',len(train))\nprint('# validation rows:',len(validation))\n\n# Add the column to identify whether a row comes from train or not\nvalidation['is_train'] = 0\n\n# Convert validation dataframe to an array\nval_array=validation.values\n# Split validation into X and Y\nX_validation = val_array[:,0:9]\nY_validation = val_array[:,10]\n\n# training rows: 214\n# validation rows: 213\n\n\n\n\nCreating the weights\nNow Mr. Um takes the two X and two Y arrays and stacks them using np.vstack and np.hstack. He creates two new arrays, named X_cov and y_cov, which combine, as previously, the training features and validation features (X_cov), and then, in a separate array, the training target and the validation target (y_cov). These functions create arrays in the format that logistic regression will be able to use in fitting the model.\nNext, he fits the stacked arrays, X_cov and y_cov to a logistic regression model. This model uses both training (U.K. database) and validation (the jars) to create a new model that encompasses our “drifted” examples. With the new model in hand, he uses the decision_function of logistic regression to calculate confidence scores on the original training sample.\nTo reduce what Mr. Um calls “outrageous weight assignments,” he uses a cutoff value of 3 (or -3), which he applies conditionally to the array of confidence scores. All scores less than -3 are assigned a -3. All scores greater than 3 are assigned a 3. Mr. Um then exponentiates (raises Euler’s number, 2.71828, to the power of each weight), using the np.exp function.\nWe then have an array of weights that correspond with each row of the original training sample.\n\n# Albert Um\n# Fitting Logistic Regression to obtain weights similar to test\n\n# vertical stack feature matrix\nX_cov = np.vstack([X_train, X_validation])\n\n# horizontal stack label array\ny_cov = np.hstack([Y_train, Y_validation])\n\n# Instantiate a binary classifier for confidence scores\nweights_logreg = LogisticRegression(max_iter=2000000)\nweights_logreg.fit(X_cov, y_cov)\n\n# Calculate the weights for X_train, not to X_train + X_validation,\n# but based on the fit of the covariate stacks, which include training + validation\n# This brings the count to the original 214.\n# confidence scores\nconf_scores = weights_logreg.decision_function(X_train)\n#print(conf_scores)\n# Clipping (C = Cutoff, originally set at 3)\nC = 3\nconditions = [\n  conf_scores &lt; -C,\n  conf_scores &gt; C\n]\n\nchoices = [\n  -C,\n  C\n]\nconf_scores = np.select(conditions, choices, conf_scores)\n# exponentiate the confidence scores\nweights = np.exp(conf_scores)\nweights = weights.flatten()\nprint('# of weights:',len(weights))\n\n# of weights: 214\n\n\n\n\nFitting the model with the weights to the training sample\nWe can now bring back our original U.K. database as an array. Split the array into features (x) and target (y), using the original target variable, GlassType.\nWe then create stratified Kfolds of X_train (training features), X_validation (validation features), Y_train (training target), and Y_validation (validation target). We then fit our favorite model, Random Forests, to the training features and training target, using the sample weights we derived from the earlier fitting of the logistic regression model to the data that included both the U.K. dataset and the jars.\n\n# Bring back the original dataset with the original target column: GlassType\n# so that we can apply the weights above to test their predictive ability\narray = dataset_ml.values\nx = array[:,1:10]\n\ny = array[:,10]\n\n#print('length X_train:',len(X_train))\n#print('length weights:',len(weights))\n\n# We use a folded split to fit the model on the training data only, \n# thus avoiding data leakage in the fitting of the model.\n# The validation datasets must be naive to the training datasets to prevent data leakage.\n\n# The splits=4 allows the validation set to be about 25% of the size of the training set.\nskf=SKF(n_splits=4, shuffle=True, random_state=100)\nfor fold, (train_idx, validation_idx) in enumerate(skf.split(x,y)):\n        X_train, X_validation = x[train_idx], x[validation_idx] \n        Y_train, Y_validation = y[train_idx], y[validation_idx]\n\n#print(len(X_train),len(X_validation),len(Y_train),len(Y_validation))\n#print(Y_train)\n# The idea is to weight more heavily those training examples that are \n# more similar to the artifacts\n# Fit a new model with the logistic regression's custom weights\nmodel = RandomForestClassifier(n_jobs=-1,max_depth=5)\nmodel.fit(X_train, Y_train, sample_weight = weights[train_idx]);\n\n\n\nAssessing the skill of the weighted model\nNow it is time to see how this new model with weights performs against covariate drift. This next step is rather simple. We can use the model fitted in the prior step to make predictions on the validation features (jars), then print a confusion matrix and a classification report to assess the results.\n\nyhat = model.predict(X_validation)\nprint('Random Forest with sample_weight')\nprint(confusion_matrix(Y_validation, yhat))\nprint(classification_report(Y_validation, yhat, zero_division=1))\n\nRandom Forest with sample_weight\n[[13  4  0  0  0  0]\n [ 4 15  0  0  0  0]\n [ 3  1  0  0  0  0]\n [ 0  3  0  1  0  0]\n [ 1  0  0  0  1  0]\n [ 0  0  0  0  0  7]]\n              precision    recall  f1-score   support\n\n         1.0       0.62      0.76      0.68        17\n         2.0       0.65      0.79      0.71        19\n         3.0       1.00      0.00      0.00         4\n         5.0       1.00      0.25      0.40         4\n         6.0       1.00      0.50      0.67         2\n         7.0       1.00      1.00      1.00         7\n\n    accuracy                           0.70        53\n   macro avg       0.88      0.55      0.58        53\nweighted avg       0.75      0.70      0.66        53\n\n\n\nUnfortunately, our model performs a good deal worse than both our SMOTE and non-SMOTE models used earlier. On the Nikumaroro jar, it computes a minimal probability that the Nikumaroro jar is a container. On the clear facsimile, the results are very similar.\n\nyhat = model.predict([artifact_features])\nyhat2 = model.predict([facsimile_features])\nyhat_probability = model.predict_proba([artifact_features])\nyhat2_probability = model.predict_proba([facsimile_features])\nthe_report()\n\nNikumaroro jar prediction: Window Float\nThe artifact has a probability of:\n53% to be a Window Float\n18% to be a Window Non-Float\n17% to be a Headlamp\n12% to be a Vehicle Float\n0% to be a Container\n0% to be a Tableware\n\nclear facsimile prediction: Window Non-Float\nThe clear facsimile has a probability of:\n27% to be a Window Non-Float\n27% to be a Window Float\n17% to be a Headlamp\n11% to be a Container\n10% to be a Tableware\n9% to be a Vehicle Float\n\n\nUsing the same Random Forests model without weights as a control, we find that the skill of this model is about the same as that of the model with weights, and the clear facsimile and Nikumaroro jar are still assigned a very low probability as containers.\n\nmodelnoWeight = RandomForestClassifier(n_jobs=-1,max_depth=5)\nmodelnoWeight.fit(X_train, Y_train)\nyhat = modelnoWeight.predict(X_validation)\n\nprint('Random Forest without sample_weight')\nprint(confusion_matrix(Y_validation,yhat))\nprint(classification_report(Y_validation, yhat, zero_division=1))\n\nRandom Forest without sample_weight\n[[12  5  0  0  0  0]\n [ 5 13  0  0  0  1]\n [ 2  2  0  0  0  0]\n [ 0  3  0  1  0  0]\n [ 0  1  0  0  1  0]\n [ 0  0  0  0  0  7]]\n              precision    recall  f1-score   support\n\n         1.0       0.63      0.71      0.67        17\n         2.0       0.54      0.68      0.60        19\n         3.0       1.00      0.00      0.00         4\n         5.0       1.00      0.25      0.40         4\n         6.0       1.00      0.50      0.67         2\n         7.0       0.88      1.00      0.93         7\n\n    accuracy                           0.64        53\n   macro avg       0.84      0.52      0.55        53\nweighted avg       0.70      0.64      0.61        53\n\n\n\n\nyhat = modelnoWeight.predict([artifact_features])\nyhat2 = modelnoWeight.predict([facsimile_features])\nyhat_probability = modelnoWeight.predict_proba([artifact_features])\nyhat2_probability = modelnoWeight.predict_proba([facsimile_features])\nthe_report()\n\nNikumaroro jar prediction: Window Float\nThe artifact has a probability of:\n55% to be a Window Float\n25% to be a Window Non-Float\n10% to be a Headlamp\n10% to be a Vehicle Float\n0% to be a Tableware\n0% to be a Container\n\nclear facsimile prediction: Window Non-Float\nThe clear facsimile has a probability of:\n33% to be a Window Non-Float\n23% to be a Headlamp\n21% to be a Window Float\n10% to be a Vehicle Float\n8% to be a Tableware\n5% to be a Container\n\n\n\n\nWhy Did the Weighting Fail to Improve the Model?\nOn an intuitive level, the low skill of this weighted model can be understood by looking back at the scatter plot we used to demonstrate SMOTE. Let’s recreate this scatter plot now.\n\nsns.set_style(\"white\")\nsmotegraph(' - after SMOTE oversampling','sm')\n\n\n\n\nFor the weighting model to be successful, two things need to happen.\nFirst, samples in the original training set (U.K. database) that have somewhat similar features to the validation set (jars) must exist. Notice the upper left quadrant of the SMOTE graph above. There are only a few training examples in that region. If, for example, our jars have features that place them in the upper left corner of the graph, there will be no training examples there to weight in favor of the validation examples.\nSecond, at least a few of the samples in the original training set (U.K. database) that share similar features to the validation set (jars) must actually BE jars. If they are not jars, then weighting these examples will only cause the model to reinforce and amplify its misinterpretations.\nJudging by the number of proposed treatment options, it would seem that there is no single magic bullet to the problem of covariate drift. The best approach is to experiment, with the realization that some validation samples may have drifted beyond the reach of rescue by a skillful model. In essence, the Nikumaroro jar might be regarded not as an example of covariate drift, but rather as a paradigm shift, a combination of features too unique to be generalizable to the world of glass types available in 1987.\nThe most basic treatment option, and the preferred one, for covariate drift is to find new data examples to add to the training set that will diversify it and make it more adaptable to training it for unanticipated validation examples."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#conclusions",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#conclusions",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Conclusions",
    "text": "Conclusions\nOur task, then, would seem to be to supplement the 1987 U.K. database with chemical profiles of jars from the early twentieth century that are similar to the Nikumaroro jar.\nBut could these examples be found? Those of us who analyzed the Nikumaroro jar have searched for more than ten years and have not so far found any siblings that match that jar’s chemical profile. The search, however, continues.\nI derive two lessons from this machine learning project: 1. Machine learning, at least of the type developed here, is not without failure or flaw. It can sometimes fail spectacularly.\n\nFlawlessness, however, is neither very interesting nor very attainable, and in any case, it may not even be necessary. We can learn more sometimes by a model’s failure to learn than by its success.\n\nWe have learned that our machine learning model can correctly characterize the clear facsimile, a jar of the same size and shape as the Nikumaroro jar, and possibly from the same era. The model cannot correctly characterize the semi-opaque Nikumaroro jar. One might suppose that this jar has a rare and original recipe that, in the absence of any information other than the data nourishing the model, is not readily identifiable as to the glass type, even with the powerful machine learning tools, such as random forests, available to us in 2022. While this may not be confirmative as to whether or not the jar was owned and brought to Nikumaroro Island by Amelia Earhart, this fact is further confirmation of its originality and rarity, perhaps even in its own time."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#notes",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#notes",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Notes",
    "text": "Notes\n[1] Wallenberger, Frederick T. and Bingham, Paul A., ed. Fiberglass and Glass Technology: Energy-Friendly Compositions and Applications. New York: Springer Science and Business Media, 2010, p. 323. [2] Caddy, Brian, ed. Forensic Examination of Glass and Paint. London: Taylor & Francis, 2001, p. 61. [3] Kaur, Gurbinder. Bioactive Glasses: Potential Biomaterials for Future Therapy. Germany: Springer International Publishing, 2017, p. 107."
  },
  {
    "objectID": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#acknowledgements",
    "href": "stories/2022-03-27_Glass_ML/2022-10-14-Glass-ML-20th-Century.html#acknowledgements",
    "title": "Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI owe much of what I have learned to date in machine learning to Dr. Jason Brownlee: https://machinelearningmastery.com/machine-learning-in-python-step-by-step/\nI owe a debt of gratitude to reviewers from The Alan Turing Institute. Jennifer Ding, Research Application Manager and Eirini Zormpa, Community Manager of Open Collaboration, kindly offered numerous suggestions for clarifying both the code and the writing of this paper.\nThank you to all of the members of the Turing Data Stories Wednesday Meeting Group, who also offered many insights, camaraderie and support through many months of the process of creating new and interesting Turing Data Stories, and the work continues.\nThank you to Ric Gillespie of The International Group for Historic Aircraft Recovery for graciously allowing the sharing of this research with The Alan Turing Institute.\nThank you to Ian W. Evett and Ernest J. Spiehler, for making the data from their 1987 glass study available for students and experienced data scientists alike to use.\nThank you to my co-authors of the paper “A Freckle In Time,” which was written from 2010-2013: Greg George, Senior Staff Chemist at Persedo Spirits, Bill Lockhart, former assistant professor of sociology at New Mexico State University at Alamogordo, and Thomas Fulling King, former director of the United States Advisory Council on Historic Preservation.\nStory preview photograph credit: ID 98648556 © Mickem | Dreamstime.com"
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "",
    "text": "Helen Duncan, Alan Turing Institute\nChristina Last, Alan Turing Institute\nLuke Hare, Alan Turing Institute"
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#introduction",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#introduction",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Introduction",
    "text": "Introduction\nOn 28 June, 2021, newly installed Health Secretary Sajid Javid announced that the COVID-19 Vaccine Programme in the UK had severed the link between infection and severe disease, hospitalization, and death (see here for the speech made to parliament on this topic). Although cases were rising quickly at the time due to the spread of the Delta variant, this was not leading to as strong a rise in hospital admissions and deaths when compared to the earlier waves of the disease.\nHowever, one problem that had cropped up in the COVID-19 pandemic is that exponential rises in cases, combined with some strong inertia in the causal chains that lead from infection to hospital admission, can obscure a crisis situation that can quickly become much, much worse. The delay between infection and hospital admission can last for a few weeks, so it may be that we just had not waited long enough to see the effect of cases on the hospital admissions at the time of the speech. This story examines a method for analyzing the COVID-19 data in England for 2020-2021 and accounting for uncertainty by applying Bayesian inference to the problem."
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#data",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#data",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Data",
    "text": "Data\nThe UK has a COVID-19 data website with a public API that allows us to examine daily counts for COVID-19 tests, hospital admissions, deaths, and other statistics surrounding the pandemic. We are principally interested in determining how positive tests collected on a particular date lead to hospital admissions some time later. Because testing was not always consistent in the early days of the pandemic, we will focus our analysis on the fall “second wave” starting in July 2020, using test data starting from May 2020. While the May 2020 data likely includes a time period where widespread testing was not available, we do this to give our model a fixed window of 60 days to link hospital admissions to tests on any of the previous days in the window to ensure that we fully account for the possibility of a long delay between tests and hospital admission.\nSimilarly, since the first vaccine doses were given in late December 2020, we cut off our period of analysis for fitting the model at 31 December, 2020, to ensure that we fit the model to a time period where we can be confident that no widespread vaccines had been given. We will use this time period to fit the model parameters, and then make predictions on the spring/summer 2021 wave using the model parameters derived from the fall 2020 data.\nFirst, we can look at the COVID test data for England for this time period:\n\n%matplotlib inline\n\nimport pandas\nimport matplotlib.pyplot as plt\n\ndef get_data(query_type, start, end):\n    \"Queries UK Covid Data API for a particular type of data and returns data queried during a specific time frame\"\n    url = 'https://coronavirus.data.gov.uk/api/v1/data?filters=areaType=nation&structure=%7B%22areaType%22:%22areaType%22,%22areaName%22:%22areaName%22,%22areaCode%22:%22areaCode%22,%22date%22:%22date%22,%22query_type%22:%22query_type%22%7D&format=csv'\n    new_url = url.replace(\"query_type\", query_type)\n    df = pandas.read_csv(new_url)\n    df[\"date\"] = pandas.to_datetime(df[\"date\"])\n    query_string = 'areaName == \"England\" & date &gt;= \"{}\" & date &lt;= \"{}\"'.format(start, end)\n    return df.query(query_string)\n\ncases_str = \"newCasesBySpecimenDate\"\ncases = get_data(cases_str, \"2020-05-02\", \"2020-12-31\")\n\nplt.plot(cases[\"date\"], cases[cases_str])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Daily positive tests\")\nplt.xticks(rotation=45);\n\n\n\n\nAnd for the hospital admissions, we have:\n\nadmissions_str = \"newAdmissions\"\nadmissions = get_data(admissions_str, \"2020-07-01\", \"2020-12-31\")\nplt.plot(admissions[\"date\"], admissions[admissions_str])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Daily hospital admissions\")\nplt.xticks(rotation=45);"
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#model",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#model",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Model",
    "text": "Model\nTo examine hospital admissions, we need to develop a model that predicts two things: how many hospital admissions to expect given a number of positive COVID tests, and when those hospital admissions occur. One simple way we might do this is via the following:\n\nAll positive tests have a fixed probability \\(\\psi\\) of causing a hospital admission at some later date. This is all that is required to determine how many hospital admissions occur given a number of positive tests.\nFor those that will be admitted to hospital, we then need to determine when the person will go into hospital. We use the following model to determine the date:\n\nEvery day, a COVID-19 patient that will eventually require hospitalization has a fixed probability \\(p\\) of their condition worsening.\nOnce their condition has worsened \\(k\\) times, they are admitted to hospital.\nThis process follows a negative binomial distribution, shifted by \\(k\\) days. An equivalent way to view this is to say that the date of hospital admission follows a multinomial distribution, with the multinomial probabilities following the PMF of a negative binomial distribution, shifted by \\(k\\) days.\n\n\nA negative binomial distribution describes the number of successful Bernoulli trials expected before we obtain a fixed number of failures. Note that we need to shift this distribution because we care about the total number of trials before we reach the fixed number of failures, rather than the number of successes.\nOnce we can predict how many hospital admissions to expect and when they occur, we just need to provide the model with the number of positive tests on each day, and the model will output a prediction of the number of hospital admissions on each day. This can then be compared with the data to estimate the values of \\(\\psi\\), \\(p\\), and \\(k\\).\nThe model is implemented below. Note that since the UK COVID API returns data with the most recent dates first, while our simulation goes forward in time, we need to reverse the cases to feed it into the function that simulates hospitalizations. Note also that we do a few checks of the admission probabilities before they are fed into the multinomial distribution – this is because the outputs of the admission probabilities sum to greater than one on occasion due to rounding errors, and sometimes we might have accidentally put in a bad number for \\(k\\) that returns NaN values so we would like to prevent these situations from crashing our simulation.\n\nimport numpy as np\nfrom scipy.stats import binom, nbinom, multinomial\n\ndef reverse_df_data(df, column):\n    return np.array(df[column], dtype=np.int64)[::-1]\n\ndef simulate_hospital_admissions(cases, psi, k, p):\n    \"Run a random forward simulation of the hospitalizations given cases and the model parameters\"\n    \n    daily_hospital_admissions = binom.rvs(n=cases, p=np.broadcast_to(psi, (len(cases),)), size=(len(cases),))\n    \n    window = 60\n    dates = np.arange(0, window, 1, dtype=np.int64)\n    \n    admission_probs = nbinom.pmf(dates, loc=k, n=k, p=p)\n    \n    # Run a few checks on admission_probs to avoid issues with multinomial sampling\n    \n    if np.sum(admission_probs) &gt; 1.:\n        admission_probs = admission_probs/(1.+1.e-10)\n    if np.any(np.isnan(admission_probs)):\n        return np.zeros(len(cases)-window, dtype=np.int64)\n    \n    admissions = np.zeros(len(cases)+window)\n    for i in range(len(daily_hospital_admissions)):\n        admit_samples = multinomial.rvs(n=daily_hospital_admissions[i], p=admission_probs, size=1)[0]\n        admissions[i:i+window] += admit_samples\n    \n    return admissions[window:-window]\n\ncase_array = reverse_df_data(cases, cases_str)\nadmissions_array = simulate_hospital_admissions(case_array, 0.1, 4, 0.1)\n\nplt.plot(admissions[\"date\"][::-1], admissions_array)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Simulated daily hospital admissions\")\nplt.xticks(rotation=45);\n\n\n\n\nIf you try simulating different choices of \\(\\psi\\), \\(p\\) and \\(k\\), you can see that the parameters that control the timing of admissions does have an effect on the overall size and shape of the peak (or peaks). Thus, we need to find a way to use the actual hospitalization data to fit reasonable values for these simulation parameters."
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#bayesian-inference",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#bayesian-inference",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nTo fit the model and account for uncertainty, we will turn to Bayesian Inference. Bayesian Inference treats our model parameters \\(\\theta = (\\psi, p, k)\\) as uncertain and thus described by a probability distribution. We wish to compute the posterior distribution of our parameters conditioned on the observed hospitalization data from the fall 2020 \\(y\\), written \\(p(\\theta|y)\\). To compute this, we use Bayes’ rule\n\\[p(\\theta|y)=\\frac{p(y|\\theta)p(\\theta)}{p(y)}\\]\nwhich relates the posterior to the likelihood \\(p(y|\\theta)\\), or the probability that a given choice of parameters would have produced the data, our prior beliefs about reasonable parameter values \\(p(\\theta)\\), and the evidence \\(p(y)\\), which is the probability of getting the data given all possible models. In most cases, computing the evidence is not feasible, so we resort to drawing samples from the posterior distribution rather than computing the distribution analytically.\n\nPrior\nWe need to specify our initial beliefs about what we expect might be reasonable model parameters. Based on previous reporting on the pandemic, we expect a relatively small fraction of positive tests to result in hospital admissions. We can quantify this through a Beta distribution, a flexible distribution well suited for modeling parameters that are constrained between 0 and 1. Beta distributions have two shape parameters \\(a\\) and \\(b\\) (or \\(\\alpha\\) and \\(\\beta\\)) that control where the distribution has highest density. We want our priors to be skewed towards small numbers, so we will choose \\(a=0.5\\) and \\(b=2\\), which puts most of the distribution mass at smaller values.\nFor \\(k\\) and \\(p\\), we might want to consider information that hospital admissions usually occur a week or two after contracting the disease. However, there are two ways our model might express this: \\(k\\) and \\(p\\) could both be large, meaning that the patient has a high probability of their conditions worsening a little bit each day, eventually leading to a (predictable) hospital admission a large number of days later. Alternatively, \\(p\\) and \\(k\\) might both be small. This would describe the situation where the person testing positive has a low chance of their symptoms getting significantly worse every day, but when it does get worse they must be hospitalized quickly. This will lead to more variability in the exact hospitalization date. We have no reason to prefer either possibility, nor would we like to exclude either possibility, so we will use uniform priors for \\(k\\) and \\(p\\) (a Discrete Uniform distribution for \\(k\\) between 1 and 20 and a Beta distribution with \\(a=1\\) and \\(b=1\\) for \\(p\\)).\nFinally, we should note that the model for determining when a patient goes into hospital is probably not a very realistic description of how a COVID-19 infection proceeds. Rather, we should think more about the two end member cases described above as ways of capturing exactly when, on average, patients need to be admitted to hospital, and the variability in when that occurs. Looking at the values that best fit the data thus tells us more about the type of model that tends to fit the data, rather than providing actual clinical insight into the disease.\n\n\nLikelihood\nThe data for this model is a time series of hospital admissions that is a superposition of many Multinomial distributions. The likelihood for this model unfortunately does not exist in closed form. However, we can easily run a forward simulation of the approach by sampling from the appropriate probability distributions, so we will resort to a method known as Approximate Bayesian Computation (ABC) to approximate the likelihood.\nRather than requiring that the forward model produce exactly the same set of hospital admissions as is seen in the data (which is unlikely due to the fact that we are performing a single random sample when we run the forward simulation), we will instead try to find a model that is fairly close to the data. We use a Gaussian measure of dissimilarity, which penalizes models that show a large difference between the real data and the model predictions. The Gaussian measure depends on an additional parameter \\(\\epsilon\\), which can be thought of as a tolerance parameter that sets the scale for how much to penalize the difference between the real and simulated data.\nWe use \\(\\epsilon=500\\) here due to the number of daily hospitalizations being in the thousands at the peak, or in other words we want to penalize models that differ by more than around 500 between the real and simulated data. However, some experimenting with values from 10 to 2000 has shown that the exact results are not too sensitive to the choice of this parameter so we can be confident that the results do not depend too much on this choice.\n\n\nSequential Monte Carlo Sampling\nAs discussed above, rather than compute the posterior directly, we will draw samples from it. To do so, we will use a technique known as Sequential Monte Carlo (SMC) sampling, or, in some other contexts, Particle Markov Chain Monte Carlo. SMC is a method for sampling from an unknown distribution (the posterior) by instead sampling from a known distribution (the prior) and then resampling based on weights determined from the computed likelihood \\(\\times\\) prior for each sample. The higher the weight of a particular sample, the higher the probability it will be re-sampled and remain in the set of samples taken at the next step. Thus, over time, the samples where the posterior is highest will be repeatedly passed on to the next step, while those where the posterior is lower will be removed.\nThis resampling technique works best if the two distributions are similar. Because we expect the prior and posterior to be quite different, SMC gradually tunes between the prior and posterior by resampling from \\(q \\propto p(y|\\theta)^\\beta p(\\theta)\\) at each step, where \\(\\beta\\) varies from 0 to 1. This ensures that the two distributions are reasonably similar at each step and the resampling technique will be accurate.\nIn addition to resampling, which ensures that good samples are likely to show up in successive steps, SMC also perturbs the samples at each step using a Markov Chain Monte Carlo random walk to ensure that the method explores the regions surrounding good samples. Both the tempering and exploration can be tuned adaptively based on the results of the sampling to try and optimize the number of steps needed to move between the prior and the posterior. This helps to balance accuracy of the sampling with the computational cost.\n\n\nImplementation\nBelow, I implement this model and draw samples from it using the PyMC3 library. The Simulation class in PyMC3 allows us to do ABC inference and draw the samples using SMC without having the explicit likelihood implemented in the library.\nNote also that I need to bind the array of cases to the simulation function, which I do so using the partial function in the functools module – this is because the PyMC3 Simulation class requires that the only parameters that are passed to the simulation be the model parameters that we are trying to estimate. Since the actual cases are known, and are not estimated, we need to fix that value of the input.\nFinally, PyMC3 by default draws multiple sets of samples based on the number of cores on the computer, so I set the number of chains to be 1 to give consistent results across different machines. Be aware that to draw the full set of 2000 samples, the simulations were run for roughly 10 minutes on a MacBook Pro.\n\nfrom functools import partial\nimport pymc3\n\ndef fit_model(simulation, cases, hospitalization, epsilon):\n    \n    simulate_hospital = partial(simulation, cases=cases)\n\n    with pymc3.Model():\n        psi = pymc3.Beta(\"psi\", alpha=0.5, beta=2.)\n        p = pymc3.Beta(\"p\", alpha=1., beta=1.)\n        k = pymc3.DiscreteUniform(\"k\", lower=1, upper=20)\n    \n        sim_admissions = pymc3.Simulator(\"admissions\", simulate_hospital,\n                                         params=(psi, k, p), epsilon=epsilon,\n                                         observed=hospitalization)\n\n        trace = pymc3.sample_smc(kernel=\"ABC\", chains=1)\n\n    return trace\n\nobs_array = reverse_df_data(admissions, admissions_str)\nepsilon = 500.\n\ntrace = fit_model(simulate_hospital_admissions, case_array, obs_array, epsilon)\n\nInitializing SMC sampler...\nSampling 1 chain in 1 job\nStage:   0 Beta: 0.004\nStage:   1 Beta: 0.020\nStage:   2 Beta: 0.067\nStage:   3 Beta: 0.177\nStage:   4 Beta: 0.382\nStage:   5 Beta: 0.654\nStage:   6 Beta: 1.000"
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#results",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#results",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Results",
    "text": "Results\n\nPosterior Distribution\nFirst, we can look at the posterior samples to learn about the type of model that fits the data. We see that \\(\\psi\\) has a peak around 9%, meaning that proportion of positive tests lead to hospitalization. The distribution is fairly tight around this value, so there is not much uncertainty in this parameter.\nFor \\(p\\) and \\(k\\), we see that most of the samples tend to favor the low \\(p\\), low \\(k\\) model described above. As previously discussed, this is not necessarily a realistic model of how an infection evolves, but rather a way of understanding the uncertainty in when a patient goes into hospital. What is important is that there is variability in when we expect positive tests on a given day to lead to later hospitalizations.\n\nplt.subplot(131)\nplt.hist(trace[\"psi\"], bins=10)\nplt.xlabel(\"$\\psi$\")\nplt.subplot(132)\nplt.hist(trace[\"p\"], bins=10)\nplt.xlabel(\"p\")\nplt.subplot(133)\nplt.hist(trace[\"k\"], bins=list(range(0, 20)));\nplt.xlabel(\"k\")\nplt.tight_layout()\n\n\n\n\n\n\nPosterior Predictive Samples\nWe can look at the range of simulated outcomes and compare these (along with some summary statistics) to the actual evolution in cases.\n\ndef draw_posterior_samples(cases, trace):\n    \"Draw samples from the simulation using the SMC samples\"\n\n    samples = []\n\n    for s in trace:\n        samples.append(simulate_hospital_admissions(cases, s[\"psi\"], s[\"k\"], s[\"p\"]))\n\n    return np.array(samples)\n\ndef compute_posterior_statistics(samples):\n    \"Compute mean and credible intervals from posterior samples\"\n    \n    mean = np.mean(samples, axis=0)\n    quantile_005 = np.quantile(samples, 0.005, axis=0)\n    quantile_995 = np.quantile(samples, 0.995, axis=0)\n    \n    return mean, quantile_005, quantile_995\n\ndef plot_data_with_samples(cases, admissions, trace):\n    \n    samples = draw_posterior_samples(cases, trace)\n    \n    mean, quantile_005, quantile_995 = compute_posterior_statistics(samples)\n\n    plt.plot(admissions[\"date\"][::-1], samples.T[:,:100], color=\"C0\", alpha=0.2)\n    plt.plot(admissions[\"date\"], admissions[admissions_str], color=\"C1\", label=\"Data\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Daily Hospital Admissions\")\n    plt.plot([], [], color='C0', alpha=0.2, label='Posterior Samples')\n    plt.plot(admissions[\"date\"][::-1], mean, color=\"C2\", label=\"Mean\")\n    plt.plot(admissions[\"date\"][::-1], quantile_005, color=\"C3\", alpha=0.5)\n    plt.plot(admissions[\"date\"][::-1], quantile_995, color=\"C3\", alpha=0.5, label=\"99% Credible Interval\")\n    plt.legend()\n    plt.xticks(rotation=45)\n    \nplot_data_with_samples(case_array, admissions, trace)\n\n\n\n\nThe model is able to capture the overall dynamics of the fall 2020 wave, including the rise in admissions in October 2020, the decrease in late November 2020 due to the second national lockdown which began on 5 November, 2020, and the continued rise through Christmas 2020 when the third national lockdown came into effect on 6 January, 2021.\nInterestingly, the model has difficulty capturing the timing of the decrease at the end of the second lockdown, where the actual data strays outside of the 99% interval on the posterior samples. One reason this might occur is that during the second lockdown, schools remained in session, while most other non-essential sectors of the economy were forced to close. This means that younger children probably saw a smaller decrease in their exposure to COVID than most adults (many of whom were just staying home and not going to work). This likely changed the underlying demographics of the population testing positive to skew younger. We expect fewer hospitalizations in a younger population, which might explain the more rapid drop in admissions in late November 2020 in the real data when compared to the simulations. This appears to be a consistent feature of all models that were fit to the data, suggesting this is a change in the data rather than a shortcoming of the model.\n\n\nPredicting the Spring 2021 Wave\nWe can now query the data for the spring/summer 2021 increase in cases, and forecast what the model fit on the fall 2020 data would predict. We simulate the spring/summer 2021 wave for all sets of samples that were drawn from the posterior to determine the posterior predictive distribution, and then for each point in time we can compute credible intervals to put uncertainty bounds on the predicted hospitalizations if the spring/summer 2021 wave could be modelled with the same parameters as the fall 2020 wave:\n\nspring_cases = get_data(cases_str, \"2021-03-02\", \"2021-09-30\")\nspring_case_array = reverse_df_data(spring_cases, cases_str)\nspring_admissions = get_data(admissions_str, \"2021-05-01\", \"2021-09-30\")\n\nplot_data_with_samples(spring_case_array, spring_admissions, trace)\n\n\n\n\nAs we can see, the model predicts substantially more hospitalization (by a factor of about 4-5) than is observed in the data, lending credence to the claim that the link between infections and hospital admissions had been weakened. Additionally, the uncertainty in the prediction is much smaller than the observed differences, suggesting that this would need to be due to some extremely rare fluke if there had not been any changes in the underlying population being exposed to COVID. Note also that the fluctuations in the number of hospital admissions predicted in summer 2021 are quite large (roughly a difference of 1000 between the lowest and highest estimates). This suggests that despite months of data on infections and hospitalizations there is still considerable uncertainty in understanding how much hospital care a population exposed to COVID will require. This should highlight the need for careful consideration of the modelling assumptions that are made in any forecast of the effect of an emerging, uncertain infectious disease on the vulnerable members of a population."
  },
  {
    "objectID": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#discussion",
    "href": "stories/2021-11-03-Vaccine_Efficacy/2021-11-03-vaccine-Efficacy.html#discussion",
    "title": "Modelling COVID-19 Hospitalizations in England",
    "section": "Discussion",
    "text": "Discussion\nWhy might the link between infections and hospitalizations be weaker? One obvious reason is vaccination, which is what was highlighted by the Health Secretary in his announcement. While this almost certainly played a big role, we cannot necessarily attribute the entire effect to this. Why?\nAs mentioned above, we saw that the model had difficulty handling the data during the second national lockdown, and one possible explanation was a change in the population testing positive. Indeed, during the time period under consideration, schools were open while other parts of the economy remained under restriction, meaning that it is possible that more testing in schools could be partially responsible for this difference. Since young people received vaccines later than the elderly population during spring/summer 2021, we would expect that as time progresses, the population that tests positive should gradually skew younger, which could explain some of the reduction in hospital admissions over spring/summer 2021. However, we can potentially estimate the magnitude of this effect by looking at the November 2020 data, which shows a reduction of a few hundred hospital admissions. This is substantially smaller than the observed differences, so it is unlikely to explain the full discrepancy between the model and the data.\nGiven that we are confident that the data differs from the predictions, how might we account for vaccinations? One possibility is to make \\(\\psi\\) no longer a constant, but rather a function of the single and double dose vaccination rates, possibly with a time lag to allow for immunity to build up. We might also fit three different \\(\\psi\\) values for 0, 1, or 2 vaccine doses, though since vaccinated people are less likely than unvaccinated people to become infected, we would also need to estimate the chances of infection which is likely to be difficult. This is particularly true due to the fact that infection rates can be very locally heterogeneous, as pockets of the population with more or less vaccination coverage will see very different infection rates.\nRegardless of the exact cause of this reduced hospitalization risk, the change in the data clearly shows that this wave of the pandemic differs from the previous ones. As the UK Government shifts its policies to one of learning to live with the virus, rather than one of actively suppressing it, data on infections and hospitalizations will continue to provide valuable insight into the evolution of the pandemic and should be guiding our decisions and behavior. Hopefully, this model provides an example of how this can be done with an eye for building simple, interpretable models that robustly account for uncertainty."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "",
    "text": "Camila Rangel Smith, Alan Turing Institute, GitHub: @crangelsmith. Twitter: @CamilaRangelS.\nBill Finnegan, University of Oxford, GitHub: @billfinnegan. Twitter: @wmfinnegan."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#introduction",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#introduction",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Introduction",
    "text": "Introduction\nThe Covid-19 pandemic has caused catastrophic losses and changes to life worldwide since the first reported cases in December 2019. A year later, in December 2020, Covid-19 vaccines began to be approved for general public use in countries worldwide, after an unprecedented speed and scale of development and clinical trials (for a process that typically take closer to a decade to complete). Vaccines have therefore emerged as the shining light at the end of the tunnel that we all hope will lead to the end of the days of lockdown measures and hospitals filled to capacity.\nSo the question on the tip of our tongues in 2021 has been “when will I, my family, my country and the world be vaccinated against Covid-19?” In this data story we’ll be focusing on the country-level question, with the objective of estimating when the adult population in the United Kingdom (UK) will be fully vaccinated against Covid-19. By “fully vaccinated” we mean vaccinated with all the doses required to get maximum protection from a specific vaccine type (typically two doses). We’ll collect data on the progress of the UK’s vaccination programme so far, and then explore several approaches for forecasting how this might continue in the future.\nA brief introduction to vaccinations in the UK: The Pfizer/BioNTech vaccine, which uses new mRNA vaccine technology, was given emergency approval on 2nd December 2020, with the first dose being administered to 91-year-old Margaret Keenan on 8th December. Since that date vaccines have continued to be rolled out in an increasing number of locations nationwide, including GP surgeries, hospitals, pharmacies and dedicated vaccination centres. The Oxford/AstraZeneca vaccine, based on more traditional vaccine technology, began to be administered around a month later on 4th January, and the Moderna mRNA vaccine more recently on 7th April 2021. All three vaccines require two doses, typically spaced between one and three months apart. Priority for vaccination is given to those at highest risk from Covid-19, namely healthcare workers, the elderly, and clinically vulnerable people. We expand on the details of the vaccination programme throughout this data story.\nNote that the story is written based on the status of the vaccination programme on 4th May 2021.\n🔧 In this story you can find the technical descriptions enclosed by spanners. If the technical side is not your thing feel free to skip it and hide the code cells 😊 🔧"
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#setup",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#setup",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Setup",
    "text": "Setup\n🔧 For this analysis we’ll be using the requests and json libraries for interacting with data from the internet, numpy, pandas and datetime for manipulating and analysing data and dates, and matplotlib and seaborn for visualising data. In the cell below we also configure some of the defaults for the style of matplotlib figures, in particular increasing the font size. 🔧\n\n#collapse_show\n# Get data from the internet\nimport requests\nimport json\n\n# Analyse data and dates\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Visualisations\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\nplt.style.use(\"seaborn-notebook\")\nfontsize = 14\nplt.rc(\"font\", size=fontsize)\nplt.rc(\"axes\", titlesize=fontsize)\nplt.rc(\"axes\", labelsize=fontsize)\nplt.rc(\"xtick\", labelsize=fontsize)\nplt.rc(\"ytick\", labelsize=fontsize)\nplt.rc(\"legend\", fontsize=fontsize)\nwidth = 12\nheight = 5\nfigsize = (width, height)\n%matplotlib inline"
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#getting-the-data",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#getting-the-data",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Getting the Data",
    "text": "Getting the Data\nThe government has developed a Coronavirus dashboard which is updated daily with key statistics including the number of Covid-19 tests, cases, hospitalisations, deaths and vaccinations. You can view the dashboard here: https://coronavirus.data.gov.uk/\nAll of the source data can be downloaded, for example as a spreadsheet, by clicking the “download” button under each table or graph. Even better for our purposes is that the dashboard has an “Application Programming Interface” (API), which we can use to extract precisely the data we’re interested in programmatically for this notebook!\n🔧 The documentation for the API is found under the “Developer’s Guide” section of the dashboard, which describes the various parameters that are available and how they should be configured. In particular, there are Python examples which show us how to structure a data query. There are three main steps, as follows:\nStep 1: Defining the geographical region we are interested in. Some of the data is available regionally as well as nationally. The number of positive cases, for example, is available both for the UK as a whole but also down to “local authority” level (local council areas, of which there are over 300 in England alone). It’s possible to get vaccine data for each of the four nations in the UK separately, but for this story we’ll only consider the UK as a whole.\nIn the API, to define that we wish to retrieve summary data for the whole UK we must set the areaType parameter to be the value “overview”, as follows: 🔧\n\nfilters = \"areaType=overview\"\n\n🔧 Step 2: Defining which data we wish to retrieve. To explore the status of the vaccination programme we would like to know the number of new first and second vaccine doses administered on each day, and the overall (cumulative) total of first and second doses administered up to and including each day.\nIn the API these fields are given long names like newPeopleVaccinatedFirstDoseByPublishDate. Finding the correct names for the data you’re interested can be a bit tricky. Most are listed in the documentation but not all - the vaccination data has been added fairly recently and isn’t documented, for example. Usually the field names for the API match the column names in files downloaded from the website, so you can do some super sleuthing and find them that way if you’d like to try an analysis with different data!\nAs the names are long, the API helpfully let’s us rename them to something more convenient using the dictionary format seen below. We’ll call the cumulative totals cumFirst and cumSecond, and the new doses each day newFirst and newSecond. Finally, to pass this dictionary to the API it must be converted into a string without any spaces, which we achieve using the json.dumps function. 🔧\n\n#collapse_show\n# define which data columns we wish to retrieve, and what to call them\nstructure = {\n    \"date\": \"date\",\n    \"newFirst\": \"newPeopleVaccinatedFirstDoseByPublishDate\",\n    \"cumFirst\": \"cumPeopleVaccinatedFirstDoseByPublishDate\",\n    \"newSecond\": \"newPeopleVaccinatedSecondDoseByPublishDate\",\n    \"cumSecond\": \"cumPeopleVaccinatedSecondDoseByPublishDate\",\n}\n\n# convert the dictionary into a string without spaces (using the separators argument)\nstructure = json.dumps(structure, separators=(\",\", \":\"))\n\n🔧 Step 3: Submitting the API query. We can now package our parameters up into the structure required by the API, and send our query using the requests.get function. If the query is successful we should see a status code of 200 (as per the convention for request status codes). If not it should also come back with a useful error message to diagnose the problem with our query. 🔧\n\n#collapse_show\n# query the API for the data\nENDPOINT = \"https://api.coronavirus.data.gov.uk/v1/data\"\n\napi_params = {\"filters\": filters, \"structure\": structure}\nresponse = requests.get(ENDPOINT, params=api_params, timeout=10)\n\nif response.status_code != 200:\n    request_failed = True\n    print(f\"Request failed: { response.text }\")\nelse:\n    request_failed = False\n\n🔧 Our query was successful, but where is the data? We can convert the data the request contains into a Python function using the .json() method on the response, and then look at what fields are contained in the data (the keys of the dictionary): 🔧\n\n#collapse_show\nif request_failed:\n    # load previously saved data if API request failed\n    with open(\"20210517_data.json\") as f:\n        j = json.load(f)\nelse:\n    j = response.json()\n\nprint(j.keys())\n\ndict_keys(['length', 'maxPageLimit', 'totalRecords', 'data', 'requestPayload', 'pagination'])\n\n\n🔧 This doesn’t look like the vaccination data we’re expecting yet. The response also includes metadata about our query, in particular whether we queried too much data to return in one go. If the “length” of our query was larger than the “maxPageLimit” we’d have to split our query into multiple queries. 🔧\n\nprint(j[\"length\"], j[\"maxPageLimit\"])\n\n127 2500\n\n\n🔧 In this case we are a long way under the limit so we should have all the data. To find it we can have a look at one of the elements in the “data” list: 🔧\n\nprint(j[\"data\"][0])\n\n{'date': '2021-05-16', 'newFirst': 131318, 'cumFirst': 36704672, 'newSecond': 183745, 'cumSecond': 20287403}\n\n\n🔧 Each element of the “data” list contains the vaccination data for one day. To make it easier to analyse the data we can convert it to a pandas data frame, using the date as the unique index for the rows (as we have one row for each day). We also take care to properly convert the date string representations into actual Python datetimes, so we can benefit from pandas’ features for processing time series data. We also convert the numbers into units of 1 million to make them easier to read in figures and tables later. 🔧\n\n#collapse_show\ndf = pd.DataFrame(j[\"data\"])\n\n# use the \"date\" column to index our data\ndf.set_index(\"date\", inplace=True)\n# convert the date text strings into Python datetimes\ndf.index = pd.to_datetime(df.index)\n# sort the data from oldest to newest\ndf.sort_index(inplace=True)\n\n# convert all totals to millions\ndf = df / 1e6\n\ndf.tail()\n\n\n\n\n\n\n\n\nnewFirst\ncumFirst\nnewSecond\ncumSecond\n\n\ndate\n\n\n\n\n\n\n\n\n2021-05-12\n0.184210\n35.906671\n0.452437\n18.890969\n\n\n2021-05-13\n0.209284\n36.115955\n0.428041\n19.319010\n\n\n2021-05-14\n0.220068\n36.336023\n0.393402\n19.712412\n\n\n2021-05-15\n0.237331\n36.573354\n0.391246\n20.103658\n\n\n2021-05-16\n0.131318\n36.704672\n0.183745\n20.287403\n\n\n\n\n\n\n\n🔧 Later on, it will also be helpful to have data on the total number of doses given, i.e. the number of first doses plus the number of second doses. Pandas let’s us quickly create new columns for these values, as follows: 🔧\n\n#collapse_show\ndf[\"newTot\"] = df[\"newFirst\"] + df[\"newSecond\"]\ndf[\"cumTot\"] = df[\"cumFirst\"] + df[\"cumSecond\"]\n\n🔧 Our query above gets all the data up to the current date. To ensure re-running the notebook reproduces the same results from the time of writing only data up to 4th May should be included, which is done using the run_as_date variable below. If you’d like to see the latest results instead you can change the value to today’s date. 🔧\n\n#collapse_show\n# To reproduce the results iin the story set the date below to 4th May 2021\n# (run_as_date = datetime(2021, 5, 4)), or use today's date to update\n# the results with the latest available data (run_as_date = datetime.now()).\nrun_as_date = datetime(2021, 5, 4)\n\n# Latest data at time of publishing (vs. initial time of writing as above)\n# Used to compare forecasts with actual data at the end of the story\npublish_date = datetime(2021, 5, 16)\npublish_date_data = df.loc[publish_date]\n\n# filter the data to only include dates up to the run_as_date\ndf = df[df.index &lt; run_as_date]"
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#vaccines-so-far",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#vaccines-so-far",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Vaccines So Far",
    "text": "Vaccines So Far\nNow we have the data we need to start forecasting the future of the vaccination programme, but before we jump in it’s always a good idea to explore the historical data first. Let’s have a look at the first row of our data (the earliest date we have data for):\n\ndf.iloc[0]\n\nnewFirst          NaN\ncumFirst     2.286572\nnewSecond         NaN\ncumSecond    0.391399\nnewTot            NaN\ncumTot       2.677971\nName: 2021-01-10 00:00:00, dtype: float64\n\n\nThe last row of our data (the last date we have data for):\n\ndf.iloc[-1]\n\nnewFirst      0.072600\ncumFirst     34.667904\nnewSecond     0.127499\ncumSecond    15.630007\nnewTot        0.200099\ncumTot       50.297911\nName: 2021-05-03 00:00:00, dtype: float64\n\n\nAnd finally the mean (average) number of doses administered per day:\n\ndf[\"newFirst\"].mean(), df[\"newSecond\"].mean()\n\n(0.2865604601769912, 0.13485493805309734)\n\n\nAlthough the first Covid-19 vaccine in the UK (outside of clinical trials) was administered on 8th December 2020, the daily data that we have starts on 10th January 2021, by which time 2.29 million first doses and 0.39 million second doses had been given. Since that date a mean (average) of 0.29 million (290,000) first doses and 0.13 million (130,000) second doses have been administered per day. As of 3rd May 2021, a total of 34.7 million people have been vaccinated with a first dose and 15.6 million with a second dose, which we can see by looking at the last row of the data.\nTo make sense of these numbers we are missing one more piece of data - how many people are there to vaccinate? We can find this in the government’s vaccine delivery plan. The plan specifies that the UK adult population (children are not currently being vaccinated) is approximately 53 million people, as well giving the sub-totals for the nine priority groups who will receive vaccines first. Government targets typically focus on three goals - vaccinating the 15 million people in priority groups 1-4 (the over 70s, care home residents, and health & social care workers), the 32 million people in the groups 1-9 (additionally including over 50s and the clinically vulnerable), and the whole adult population.\nComparing to the values above, we can see that the number of people vaccinated so far exceeds the population of groups 1-9 for first doses, and of groups 1-4 for second doses. However, this doesn’t mean everyone in those groups has been vaccinated. Although take up rates of the vaccine in the UK are high, there is regional variation in the speed of the rollout and take up rates are lower in certain sub-groups, such as for ethnic minorities, poorer areas, and care home staff. The population in each priority group is also estimated as the groups can be overlapping. For example, a clinically vulnerable, 50 year-old social care worker meets the criteria for priority groups 2, 6 and 9. Further statistics on take up rates can be found in NHS England data.\nBearing all the above in mind, the priority group population totals are still helpful to give a rough estimate of progress through the vaccination programme so let’s save them for future use:\n\n# no. people to vaccinate in each priority group (in millions)\npriority_totals = {\"Groups 1-4\": 15, \"Groups 1-9\": 32, \"All Adults\": 53}\n\n🔧 It’s helpful to visualise the data, so before continuing we will do a bit of work here to set up plotting functions we can re-use throughout our analysis. First let’s define colours and labels to use for the data columns in all the figures we create. These are in the col_format dictionary below, which all our plotting functions can access.\nThen we define a function plot_column, which uses the pandas plot function to plot a column from the dataset in our chosen style. It also includes options to display a weekly rolling average of the data, rather than the original raw data, calculated using the statement df[column].rolling(window=7).mean() below (where window=7 means we calculate the average across 7 days). Finally, it will be helpful to distinguish between actual historical vaccine data, and estimates from the forecasts we create. If the forecast_date argument is defined the data after that date will be plotted with a dashed line instead of a solid line. The data after that date is selected using the statement data[data.index &gt;= forecast_date]. 🔧\n\n#collapse_show\ncol_format = {\n    \"cumFirst\": {\"label\": \"1st\", \"color\": \"orange\"},\n    \"cumSecond\": {\"label\": \"2nd\", \"color\": \"deepskyblue\"},\n    \"cumTot\": {\"label\": \"Total\", \"color\": \"k\"},\n    \"newFirst\": {\"label\": \"1st\", \"color\": \"orange\"},\n    \"newSecond\": {\"label\": \"2nd\", \"color\": \"deepskyblue\"},\n    \"newTot\": {\"label\": \"Total\", \"color\": \"k\"},\n}\n\n\ndef plot_column(\n    df,\n    column,\n    ax,\n    forecast_date=None,\n    rolling=False,\n    forecast_label=\"Forecast\",\n    **kwargs\n):\n    \"\"\"\n    Plot a column in a data frame, optionally calculating its rolling weekly\n    average and distinguishing between actual and forecasted data.\n    df - vaccination data frame\n    column - column of df to plot\n    ax - matplotlib axis to use for the plot\n    forecast_date - plot data from this date with a dashed line\n    rolling - plot rolling weekly average instead of raw data\n    forecast_label - label given to forecast data in plot legend\n    **kwargs - additional arguments passed to pandas plotting function\n    \"\"\"\n    color = col_format[column][\"color\"]\n    label = col_format[column][\"label\"]\n\n    if rolling:\n        data = df[column].rolling(window=7).mean()\n        label = label + \" (7d avg)\"\n    else:\n        data = df[column]\n\n    if forecast_date is None:\n        data.plot(color=color, label=label, ax=ax, **kwargs)\n    else:\n        # plot actual data with solid line\n        data[data.index &lt;= forecast_date].plot(\n            color=color, label=label, ax=ax, linestyle=\"-\", linewidth=3, **kwargs\n        )\n        # plot forecast data with dashed line\n        data[data.index &gt;= forecast_date].plot(\n            color=color,\n            label=forecast_label + \" \" + label,\n            ax=ax,\n            linestyle=\"--\",\n            **kwargs\n        )\n\n🔧 As well as showing the number of doses, we’d like to show when the number of people vaccinated (with 1st or 2nd doses) exceeds the total of each priority sub-group. For each group, we can find the date this happened using the statement df[df[column] &gt;= (pop - tol)].index[0], where pop is the population of the sub-group, column is either cumFirst or cumSecond (depending on whether we want to calculate it for 1st or 2nd doses), and tol is a small value used to account for any rounding errors in the forecasts we do later.\nThe annotate_group_completions function below can then be used to add horizontal and vertical lines and text labels to a plot for those dates. As discussed earlier, we must remember that it’s not true that everyone in the priority group will have been vaccinated by the dates we calculate here, rather they are general indicators of overall progress. 🔧\n\n#collapse_show\ndef annotate_group_completions(df, column, ax, text_offset=1.02, tol=1e-7):\n    \"\"\"\n    Add text labels and lines indicating the dates the number of people\n    vaccinated exceded the population of priority sub-groups.\n    df - vaccination data frame\n    column - column to add labels for, either cumFirst or cumSecond\n    ax - matplotlib axix to add labels to\n    text_offset - space between data point and text label\n    tol - define group to be vaccinated if total vaccines is within tol of its population\n    \"\"\"\n    label = col_format[column][\"label\"]\n    max_col = df[column].max()\n\n    for name, pop in priority_totals.items():\n        if max_col &gt;= (pop - tol):  # vaccines completed for this group\n            complete_date = df[df[column] &gt;= (pop - tol)].index[0]\n\n            ax.hlines(pop, 0, complete_date, color=\"k\", linewidth=0.5)\n            ax.vlines(complete_date, 0, pop, color=\"k\", linewidth=0.5)\n            ax.text(\n                complete_date,\n                pop * text_offset,\n                f\"{name}\\n{label} Doses\\n{complete_date.date()}\",\n                ha=\"center\",\n                size=12,\n                fontweight=\"bold\",\n            )\n            # extend x-axis if text label is near the end of the axis\n            if df.index.max() - complete_date &lt; timedelta(days=7):\n                ax.set_xlim(df.index.min(), complete_date + timedelta(days=7))\n\n🔧 Now we are almost there! The plot_cumulative_doses function below uses the two previous functions to construct a figure for the cumulative total of first and second doses: 🔧\n\n#collapse_show\ndef plot_cumulative_doses(\n    df, forecast_date=None, figsize=figsize, title=None, forecast_label=\"Forecast\"\n):\n    \"\"\"\n    Plot cumulative first and second doses, and the dates when\n    the vaccination of prioriy groups completed.\n    Optionally distinguish actual data and forecasted data. Data\n    after forecast_date will be displayed with a dashed line.\n    \"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    for col in [\"cumFirst\", \"cumSecond\"]:\n        plot_column(\n            df,\n            col,\n            ax,\n            forecast_date=forecast_date,\n            forecast_label=forecast_label,\n        )\n        annotate_group_completions(df, col, ax)\n\n    # set axis limits, titles and legend\n    ymin = 0\n    ymax = df[\"cumFirst\"].max() * 1.15\n    ax.spines['top'].set_visible(False)\n    ax.set_ylim(ymin, ymax)\n    ax.set_ylabel(\"Number of Doses [millions]\")\n    ax.set_xlabel(\"\")\n    ax.legend(loc=\"upper left\")\n    if title is not None:\n        ax.set_title(title, fontsize=18)\n\n    # add a secondary y-axis showing the % of the population  vaccinated\n    right_ax = ax.twinx()\n    right_ax.spines['top'].set_visible(False)\n    uk_pop = priority_totals[\"All Adults\"]\n    y_perc_min = 100 * ymin / uk_pop\n    y_perc_max = 100 * ymax / uk_pop\n    right_ax.set_ylim(y_perc_min, y_perc_max)\n    right_ax.set_yticks(range(int(y_perc_min), min(int(y_perc_max), 110), 10))\n    right_ax.set_ylabel(\"% Adults Vaccinated\")\n\n    return ax\n\nThe following figure shows the history of the total number of people vaccinated with first and second doses, and the dates when priority groups were vaccinated:\n\nplot_cumulative_doses(df, title=f\"UK Vaccinations up to {df.index.date.max()}\")\n\n&lt;AxesSubplot:title={'center':'UK Vaccinations up to 2021-05-03'}, ylabel='Number of Doses [millions]'&gt;\n\n\n\n\n\nAs stated in Boris Johnson’s address to the nation on 4th January 2021, the first target the government set was to offer the first dose of a vaccine to everyone in the top four priority groups by mid-February. As we can see above this target was met on 13th February. The next target set by the government was then to offer a first dose to the first nine priority groups (the over 50s) with a first dose by the end of April. This was somewhat pessimistic given the vaccination rates at the time, and was ultimately completed a couple of weeks earlier on 9th April. Overall, the start of the UK vaccination programme has been very successful and amongst the fastest in the world.\nIt’s also interesting to look at the daily number of new doses each day, and this is particularly relevant for forecasting future doses. As there’s quite a large variation in the day to day totals (more on this later) it’s helpful to look at weekly rolling averages as well. We’ll also display the combined total of 1st and 2nd doses administered on each day (newTot), as this gives the best representation of the overall vaccine supply and capacity to administer vaccines.\n🔧 We can create a similar function to do this: 🔧\n\n#collapse_show\ndef plot_daily_doses(\n    df,\n    forecast_date=None,\n    show_daily=True,\n    show_rolling=True,\n    figsize=figsize,\n    title=None,\n):\n    \"\"\"\n    Plot daily first doses, second doses, the sum of 1st\n    and 2nd doses, and their weekly rolling averages.\n    \"\"\"\n    # figure properties\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n\n    for col in [\"newFirst\", \"newSecond\", \"newTot\"]:\n        if show_daily and col != \"newTot\":  # display daily data\n            plot_column(df, col, ax, marker=\".\", linestyle=\"None\")\n\n        if show_rolling:  # display 7 day averages\n            plot_column(df, col, ax, forecast_date=forecast_date, rolling=True)\n\n    ax.set_ylim(0, df[\"newTot\"].max() * 1.1)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Daily New Doses [millions]\")\n    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n    if title is not None:\n        ax.set_title(title, fontsize=18)\n\n\nplot_daily_doses(df, title=f\"Daily Vaccinations up to {df.index.date.max()}\")\n\n\n\n\nBetween January and until early February the vaccine doses administered per day steadily increased from around 0.25 million to 0.45 million on average. After that date rates plateaued and temporarily fell, mostly due to challenges and fluctuations in securing vaccine supplies. Late March saw the highest rates, peaking at 0.6 million doses per day in the week starting 14th March, though this then fell again, particularly over the Easter weekend (2nd-5th April), before recovering more recently. Supply difficulties are currently expected to continue in coming months. As well as fluctuations in the total, we can see that the number of second doses administered has been steadily increasing, and overtook the rate of new first doses in early April, with relatively few new first doses being given since then. More on this later.\nIf you look closely at the actual daily numbers of 1st and 2nd doses (the markers rather than the rolling average lines), it seems like they vary up and down with a period of 7 days. We can make this clearer by plotting the number of doses delivered on each weekday.\n🔧 The day of the week for all the dates in our data can be determined using the pandas function day_name. Then we make use of the boxplot function in the seaborn plotting library to show the distribution in doses delivered on each weekday: 🔧\n\n#collapse_show\ndf[\"weekday\"] = df.index.day_name()\n\nplt.figure(figsize=figsize)\nsns.boxplot(\n    x=\"weekday\",\n    y=\"newTot\",\n    data=df,\n    order=[\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\",\n    ],\n)\nplt.xlabel(\"\")\nplt.ylim([0, plt.ylim()[1]])\nplt.ylabel(\"Daily Doses (1st + 2nd) [millions]\")\n\nText(0, 0.5, 'Daily Doses (1st + 2nd) [millions]')\n\n\n\n\n\nMore doses are administered Wednesday to Saturday than Sunday to Tuesday, with on average half as many (0.26 million) delivered on Sunday than on Saturday (0.53 million). Although I could not find an official explanation for this, one factor is likely related to the capacity and opening hours of vaccination centres and GP surgeries, which are more likely to be closed on Sundays for example. Other statistics have similar trends, such as fewer Covid-19 deaths being recorded at weekends due to reporting delays."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#when-are-second-doses-being-given",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#when-are-second-doses-being-given",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "When are Second Doses Being Given?",
    "text": "When are Second Doses Being Given?\nA high-profile decision announced by the UK Joint Committee on Vaccination and Immunisation (JCVI) on the 30th December 2020, was to delay second doses by as long as possible, up to 12 weeks instead of the originally recommended 3-4 weeks. The decision was taken to be able to protect as many people as possible with a single dose of the vaccine within the first few months of the year, but was fairly controversial at the time due to clinical trials using shorter gaps between doses.\nThe delay between first and second doses will be an important component in our forecasts, so let’s calculate when second doses are actually being given in practice (more precisely than “up to 12 weeks”). We can estimate this by determining the number of days until the total number of second doses (cumSecond) matches the total number of first doses (cumFirst) on a given date.\n🔧 The for loop below calculates the delay for all dates in the past, and then prints the most recent delay: 🔧\n\n#collapse_show\nfor date_2nd, row in df.sort_index(ascending=False).iterrows():\n    if row[\"cumSecond\"] &gt;= df[\"cumFirst\"].min():\n        # find the last date where number of 1st doses is less than\n        #  or equal to number of 2nd doses on current row\n        date_1st = df[df[\"cumFirst\"] &lt;= row[\"cumSecond\"]].index.max()\n        # calculate how many days it was until 2nd doses were given\n        delay = (date_2nd - date_1st).days\n        df.loc[date_2nd, \"delaySecond\"] = delay\n    else:\n        break\n\navg_second_delay = df[\"delaySecond\"][-1]\nprint(\n    \"1st doses from\",\n    (df.index[-1] - timedelta(days=avg_second_delay)).date(),\n    \"were completed\",\n    avg_second_delay,\n    \"days later, on\",\n    df.index[-1].date(),\n)\n\n1st doses from 2021-02-15 were completed 77.0 days later, on 2021-05-03\n\n\nCurrently, second doses are lagging first doses by 77 days (eleven weeks). In the history of the data so far, the delay has varied between 71 and 77 days:\n\ndf[\"delaySecond\"].min(), df[\"delaySecond\"].max()\n\n(71.0, 77.0)\n\n\nWith the gap increasing from 72 to 75 days over the Easter weekend (2nd-5th April) where fewer doses (of any type) were administered than normal, and slowly drifting towards longer delays since then:\n\n#collapse_show\nfig, ax = plt.subplots(1, 1, figsize=figsize)\ndf.loc[df.index &gt;= datetime(2021, 3, 21), \"delaySecond\"].plot(ax=ax)\nax.set_xlabel(\"\")\nax.set_ylabel(\"2nd Dose Delay [days]\")\n\nText(0, 0.5, '2nd Dose Delay [days]')\n\n\n\n\n\nThe second dose delay values we calculate here are averages for the whole country. In reality, the actual gap between doses for each individual will vary depending on the availability of appointments and vaccination strategy in their area."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#forecasting-future-vaccinations",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#forecasting-future-vaccinations",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Forecasting Future Vaccinations",
    "text": "Forecasting Future Vaccinations\nNow we have a good idea about what’s happened so far, but knowing that how can we forecast when the UK might fully vaccinate the adult population? At a high level, there will be two components to our forecasts: estimates for the total number of doses that will be administered on each future date, and of how those doses will be divided between first doses and second doses. We won’t specify the exact form of our total daily doses estimation for now, except saying that it will be a function depending on two arguments - the date we want an estimate for, and the previous vaccination data. This general form will allow us to quickly try a few different forecasting approaches later.\nFor the second part (deciding which of the available vaccines will be given as first or second doses) we will make a number of assumptions that apply to all the forecasts we generate, as follows: - Everyone is given two doses of a vaccine. - Second doses are given avg_second_delay days after first doses (as calculated above). This means we expect the cumulative total of second doses today to match the cumulative total of first doses avg_second_delay days ago. Or equivalently, the number of new second doses required today is the difference between the current cumulative total of second doses and the cumulative total of first doses avg_second_delay days ago. - Second doses are given priority over new first doses. New first doses will only be given if the total number of doses available on a given day is more than the number of people requiring a second dose. - Once there is capacity to do so, second doses can be given sooner than avg_second_delay days later. Current UK guidance is that second doses of the Pfizer vaccine can be administered after 3 weeks, or after 4 weeks for the AstraZeneca vaccine, and up to 12 weeks later for both. - 100% of the UK adult population will be vaccinated.\nThese assumptions generally seem reasonable, but there are limitations with all of them. It’s likely that booster doses of Covid-19 vaccines will be needed, so ultimately people may receive more than two doses (so we’re predicting only the completion of the first phase of the UK vaccination programme). Alternatively, single-dose vaccines such as the Johnson & Johnson vaccine may start to be administered. New vaccine types coming online may also invalidate our third assumption above, as doses of a new vaccine can only be given as first doses initially - they can’t be prioritised for second doses straight away like other vaccines (unless people begin to be given two doses of different vaccine types). Although it seems reasonable to give second doses earlier once there is spare capacity to do so, we don’t know whether this will be the case and it may be that spare capacity is re-directed elsewhere. We’ve also seen that the delay between doses has varied between 71 and 77 days so far, but we initially keep it fixed at the most recent value of 77 days.\nFinally, vaccine uptake in the over 50s has been close to 95% overall, but it’s not 100% and has been much lower in certain sub-groups as discussed earlier and may be lower in younger age groups going forward. Also, people with certain health conditions, such as allergies, are unable to have a vaccine. Our forecasts will predict a theoretical completion date for the vaccination programme if 100% of adults were vaccinated, but in reality some people will choose not or cannot be vaccinated. We should keep all the limitations above in mind when interpreting our results.\n🔧 The forecast_vaccines function below encodes everything described above. We give the vaccination data so far (as the input df), add new rows for future dates up to a specified end_date, determine how many doses will be administered on each date using a function doses_fn (that we haven’t defined yet), and distributes those doses according to our assumptions. In many places the timedelta function from the python datetime library is used to get data from a number of days before the forecast date. 🔧\n\n#collapse_show\ndef forecast_vaccines(\n    df,\n    avg_second_delay,\n    doses_fn,\n    uk_pop=priority_totals[\"All Adults\"],\n    end_date=datetime(2021, 10, 1),\n    min_second_delay=28,\n):\n    \"\"\"\n    Forecast future vaccine doses.\n\n    df: DataFrame of actual vaccine data.\n    avg_second_delay: Days after 1st dose that 2nd doses will be given.\n    doses_fn: Function to calculate number of doses administered each\n    day. Takes 2 arguments - df and a date.\n    uk_pop: Total population to be vaccinated.\n    end_date: Forecast until this date.\n    min_second_delay: If there is spare capacity, allow second doses to\n    be given earlier than avg_second_delay, down to this number of days.\n    \"\"\"\n    # extend our time series index to the future\n    first_data = df.index.min()\n    last_data = df.index.max()\n    if end_date &lt; last_data:\n        raise ValueError(\n            f\"end_date ({end_date}) should be after the last date in df {last_data}\"\n        )\n    future_dates = pd.date_range(last_data, end_date, closed=\"right\")\n    df = df.append(pd.DataFrame(index=future_dates))\n    df.sort_index(inplace=True)\n\n    for d in future_dates:\n        dose2_sofar = df.loc[d - timedelta(days=1), \"cumSecond\"]\n\n        if d - timedelta(days=avg_second_delay) &lt; first_data:\n            # no 1st dose data avg_second_delay ago, assume no 2nd doses required\n            pending_2nd_doses = 0\n        else:\n            # 2nd doses needed is difference between 2nd doses so far and 1st\n            # doses avg_second_delay days ago (who now require 2nd dose)\n            dose2_req = df.loc[d - timedelta(days=avg_second_delay), \"cumFirst\"]\n\n            pending_2nd_doses = max([0, dose2_req - dose2_sofar])\n\n        # use forecasting function to determine number of doses available today\n        total_doses_today = doses_fn(df, d)\n\n        # don't vaccinate more than the total population (with 2 doses)\n        if total_doses_today + df.loc[d - timedelta(days=1), \"cumTot\"] &gt; 2 * uk_pop:\n            total_doses_today = 2 * uk_pop - df.loc[d - timedelta(days=1), \"cumTot\"]\n\n        # give all 2nd doses required (up to limit of total doses available)\n        dose2_today = min(pending_2nd_doses, total_doses_today)\n\n        # remaining vaccines given as new 1st doses\n        dose1_remaining = uk_pop - df.loc[d - timedelta(days=1), \"cumFirst\"]\n        dose1_today = min(total_doses_today - dose2_today, dose1_remaining)\n\n        # if there are spare doses, try giving 2nd doses earlier than usual\n        if dose1_today + dose2_today &lt; total_doses_today:\n\n\n            dose2_early_req = df.loc[d - timedelta(days=min_second_delay), \"cumFirst\"]\n            dose2_early_pending = max([0, dose2_early_req - dose2_sofar])\n            dose2_today = min(total_doses_today - dose1_today, dose2_early_pending)\n\n        # save today's values\n        df.loc[d, \"newSecond\"] = dose2_today\n        df.loc[d, \"cumSecond\"] = (\n            df.loc[d - timedelta(days=1), \"cumSecond\"] + dose2_today\n        )\n        df.loc[d, \"newFirst\"] = dose1_today\n        df.loc[d, \"cumFirst\"] = df.loc[d - timedelta(days=1), \"cumFirst\"] + dose1_today\n        df.loc[d, \"newTot\"] = dose1_today + dose2_today\n        df.loc[d, \"cumTot\"] = df.loc[d, \"cumFirst\"] + df.loc[d, \"cumSecond\"]\n\n    return df\n\nThe final piece of estimating the number of doses that will be available in the future is also where we have the least certainty. The scale and speed of the Covid-19 vaccine roll-out worldwide is unprecedented and there have been challenges, such as with lower production than expected in supply chains, and other delays such as the roll-out of the AstraZeneca vaccine being paused in some countries to investigate side-effects.\nGiven these circumstances it’s possible future vaccine supply in the UK could be quite different to past supply. But as a starting point let’s consider a simpler question - when will the UK population be vaccinated if doses continue to be given at the same rate as the last two weeks? In the last two weeks, vaccines have been given at a rate of:\n\n#collapse_show\nn_days = 14\nmean_doses_this_week = df[\"newTot\"].tail(n_days).mean()\nprint(\n    \"Between\",\n    df.index[-n_days].date(),\n    \"and\",\n    df.index[-1].date(),\n    f\"there was a mean of {mean_doses_this_week:.2f}\",\n    \"million doses given per day.\",\n)\n\nBetween 2021-04-20 and 2021-05-03 there was a mean of 0.49 million doses given per day.\n\n\n🔧 Now all we need to run our forecast is to create the function to calculate how many doses are available each day. In this case it can be a simple function (const_doses below) that always returns the 0.49 million average daily doses calculated above. Even though we don’t use them here, the function must have two inputs (df and date) to be compatible with our general forecast_vaccines function created above. Let’s wrap all this into a function forecast_const we can easily use to run forecasts with other constant daily dose totals, then run it on our last week average: 🔧\n\n#collapse_show\ndef forecast_const(\n    df,\n    avg_second_delay,\n    daily_doses,\n    uk_pop=priority_totals[\"All Adults\"],\n    end_date=datetime(2021, 10, 1),\n    min_second_delay=28,\n):\n    \"\"\"\n    Forecast vaccines assumming 'daily_doses' doses are given per day.\n    \"\"\"\n\n    def const_doses(df, date):\n        return daily_doses\n\n    df_forecast = forecast_vaccines(\n        df,\n        avg_second_delay,\n        doses_fn=const_doses,\n        uk_pop=uk_pop,\n        end_date=end_date,\n        min_second_delay=min_second_delay,\n    )\n\n    return df_forecast\n\n\ndf_forecast = forecast_const(df, avg_second_delay, mean_doses_this_week)\n\nWe can re-use our plotting functions to examine our forecast. Let’s start with the daily number of 1st and 2nd doses given per day:\n\n#collapse_show\nlast_data = df.index.max()\n\nplot_daily_doses(\n    df_forecast,\n    forecast_date=last_data,\n    show_daily=False,\n    title=f\"Daily Vaccinations Forecast (as of {df.index.date.max()})\",\n)\n\n\n\n\nAs expected, the total number of (1st + 2nd) doses given per day remains constant throughout our simple forecast. In the 1st and 2nd doses we see an interesting pattern where every couple of months the doses being administered swap between being mostly first doses or mostly second doses. In fact, the period over which this happens is approximately the 77 day gap between first and second doses we have assumed. At the start of the year (almost) the whole vaccine capacity was dedicated to giving new first doses, and then starting from late March all those people must be given second doses, meaning there is little spare capacity for new first doses. Our forecast predicts we will need to go through this cycle two times to vaccinate the whole population with two doses.\nNow let’s look at when we predict the population will be vaccinated:\n\n#collapse_show\nplot_cumulative_doses(\n    df_forecast,\n    forecast_date=last_data,\n    figsize=(15, 8),\n    title=f\"UK Vaccination Forecast (as of {df.index.date.max()})\",\n)\n\n&lt;AxesSubplot:title={'center':'UK Vaccination Forecast (as of 2021-05-03)'}, ylabel='Number of Doses [millions]'&gt;\n\n\n\n\n\nThe current phase of mostly second dose vaccines being given leads to 32 million people (the population of groups 1-9) being fully vaccinated by late June in our forecast. The rest of the adult population is given a first dose of the vaccine by 19th July, and then is fully vaccinated on 26th August. Note that this gap is only 38 days (less than the 77 days between doses we use initially), as we allow second doses to be given earlier once there is spare capacity to do so."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#alternative-forecasts",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#alternative-forecasts",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Alternative Forecasts",
    "text": "Alternative Forecasts\nOur forecast assumed doses will continue to be given at the rate from the the last two weeks, but looking back at the data we can see that this is higher than at most other times so far this year. Although it’s possible the number of daily doses increases further in the future, we could consider that to be fairly optimistic. It may seem unlikely that vaccination rates fall to the levels seen at the start of the year, but there are reasons this could be the case. In particular, near the end of the vaccination programme demand may be lower in the remaining population, which was the case in Israel earlier this year for example. So what if we ask a more pessimistic question - when will the population be vaccinated with two doses if vaccines are administered at the same rate as the whole year so far? Or, even better, what is the range of likely completion dates?\nTo answer this, we’ll create a new doses_fn (rnd_doses below) where the total number of doses given on any date in the future is the same as the number given on a day in the past, with that day selected randomly. We also limit the date in the past to have the same weekday as the forecast date, to take into account the differences during the week seen earlier (with more doses given Thursday-Saturday). Because the number of doses is chosen randomly, each time the forecast is run we’ll get a different result. By running the forecast many times we can get an estimate of the uncertainty in the completion dates.\n🔧 This is done using forecast_rnd function below, which returns a list of 200 forecasts: 🔧\n\n#collapse_show\ndef forecast_rnd(\n    df,\n    avg_second_delay,\n    uk_pop=priority_totals[\"All Adults\"],\n    end_date=datetime(2021, 10, 1),\n    min_second_delay=28,\n    rng=np.random.default_rng(seed=123),\n    n_forecasts=100,\n):\n    \"\"\"\n    Run n_forecasts random forecasts, using the random number\n    generator rng to randomly choose the number of doses today\n    to be the same as a date in the past on the same weekday.\n    \"\"\"\n\n    def rnd_doses(df, date):\n        doses = rng.choice(\n            df.loc[df.index.dayofweek == date.dayofweek, \"newTot\"].dropna()\n        )\n        return doses\n\n    forecasts = [\n        forecast_vaccines(\n            df,\n            avg_second_delay,\n            doses_fn=rnd_doses,\n            uk_pop=uk_pop,\n            end_date=end_date,\n            min_second_delay=min_second_delay,\n        )\n        for _ in range(n_forecasts)\n    ]\n\n    return forecasts\n\n\nn_forecasts = 200\nrnd_forecasts = forecast_rnd(df, avg_second_delay, n_forecasts=n_forecasts)\n\nIf we plot the number of daily doses in each of the 200 forecasts we get a spread of values centred around the average rate for the year, as expected:\n\n#collapse_show\nplt.figure(figsize=(15, 5))\nfor fcast in rnd_forecasts:\n    plt.plot(\n        fcast[\"newTot\"].rolling(window=7).mean(), alpha=10 / n_forecasts, color=\"k\"\n    )\n\nplt.ylabel(\"Doses [millions]\")\nplt.title(\"Daily New Doses (7 day Average)\")\n\nText(0.5, 1.0, 'Daily New Doses (7 day Average)')\n\n\n\n\n\nRather than plotting each of the 200 individual forecasts, let’s calculate 50% (median), 2.5% and 97.5% quantiles of the forecasts, specifically of the cumulative total of first and second doses. 95% of the forecast values lie between the 2.5% and 97.5% quantiles, so we can be confident the future data will be in that range (but only under our assumption that vaccines are given at the same rate as the year so far).\n🔧 To do that we can combine all the data for one column into a single data frame with the pd.concat function, then calculate the quantiles with the pandas quantiles function: 🔧\n\n#collapse_show\nquantiles = [0.025, 0.5, 0.975]\nq_forecasts = {}\n\nfor i, col in enumerate([\"cumFirst\", \"cumSecond\"]):\n    rnd_col = pd.concat(\n        [fcast[col].rename(f\"forecast{i}\") for fcast in rnd_forecasts],\n        axis=1,\n        names=[0, 1],\n    )\n    q_forecasts[col] = rnd_col.quantile(quantiles, axis=1)\n\nNow let’s plot the median of our forecasts, create a shaded range between the 2.5% and 97.5% quantiles to show the uncertainty (with the matplotlib function fill_between), and also display our original “last week” forecast to compare:\n\n#collapse_show\nax = plot_cumulative_doses(\n    pd.DataFrame(\n        {\n            \"cumFirst\": q_forecasts[\"cumFirst\"].loc[0.5],\n            \"cumSecond\": q_forecasts[\"cumSecond\"].loc[0.5],\n        }\n    ),\n    forecast_date=last_data,\n    figsize=(15, 8),\n)\n\nq_start = 0.025\nq_end = 0.975\nalpha = 0.25\nfor col in [\"cumFirst\", \"cumSecond\"]:\n    # add shaded region between q_start and q_end quantiles\n    ax.fill_between(\n        q_forecasts[col].loc[q_start].index,\n        q_forecasts[col].loc[q_start],\n        q_forecasts[col].loc[q_end],\n        color=col_format[col][\"color\"],\n        alpha=alpha,\n        label=f\"95% interval {col_format[col]['label']}\",\n    )\n    # also show previous (last week average) forecast, to compare\n    df_forecast[col].plot(\n        linestyle=\"None\",\n        marker=\".\",\n        markersize=4,\n        color=col_format[col][\"color\"],\n        label=f\"Last 2 weeks, {col_format[col]['label']}\",\n        ax=ax,\n    )\n\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe930c85c50&gt;\n\n\n\n\n\nUnder these assumptions, the population is fully vaccinated with two doses a couple of weeks later in mid-September rather than late-August. Note that our original forecast using the rate from the last 2 weeks lies outside of the uncertainty band for our new forecast. In other words, the population will only be vaccinated by the end of August if vaccines continue to be given at their current high rate without reverting back to the lower rates seen earlier in the year.\nIf we implemented a more sophisticated forecasting algorithm, such as an autoregressive or Bayesian structural time series model, we may be able to get improved estimates that incorporate the likelihood of both our “optimistic” and “pessimistic” cases."
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#government-targets-and-sage-vaccine-roll-out-estimates",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#government-targets-and-sage-vaccine-roll-out-estimates",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Government Targets and SAGE Vaccine Roll-out Estimates",
    "text": "Government Targets and SAGE Vaccine Roll-out Estimates\nThe government has the target of all UK adults being offered a first dose of the vaccine by the end of July. This is roughly in line with both of our forecasting approaches, with our optimistic forecast being a bit earlier in mid-July and the pessimistic forecast early-August. At the end of March, the Scientific Advisory Group for Emergencies (SAGE) also produced a report including the following two scenarios for the vaccine roll-out in England: - Fast scenario: 0.39m doses per day in England until week commencing 26th July and 0.29m per day thereafter - Slow scenario: 0.36m doses per day in England until week commencing 26th July and 0.29m per day thereafter\nApproximately scaling these up for the whole UK (56 million population England, 67 million population UK) gives: - Fast scenario: 0.46m doses per day until week commencing 26th July and 0.34m per day thereafter - Slow scenario: 0.43m doses per day until week commencing 26th July and 0.34m per day thereafter\nThe SAGE forecasts use an 11-week (77 day) period between doses, which is also in line with what we derived ourselves from the data.\nWe can implement the assumptions from SAGE ourselves in our own forecasts just by creating a new doses_fn (sage_doses below) that returns a fixed number of doses depending on whether we’re using the fast or slow strategy, and whether the date is after the 26th July or not:\n\n#collapse_show\ndef forecast_sage(\n    df,\n    avg_second_delay=77,\n    scenario=\"fast\",\n    uk_pop=priority_totals[\"All Adults\"],\n    end_date=datetime(2021, 10, 1),\n    min_second_delay=28,\n):\n    \"\"\"\n    Forecast vaccines for the SAGE 'fast' and 'slow' scenarios.\n    \"\"\"\n    if not (scenario == \"fast\" or scenario == \"slow\"):\n        raise ValueError(\"scenario must be 'fast' or 'slow'\")\n\n    def sage_doses(df, date, scenario=scenario):\n        if date &gt; datetime(2021, 7, 25):\n            return 0.34\n        else:\n            if scenario == \"fast\":\n                return 0.46\n            else:\n                return 0.43\n\n    df_forecast = forecast_vaccines(\n        df,\n        avg_second_delay,\n        doses_fn=sage_doses,\n        uk_pop=uk_pop,\n        end_date=end_date,\n        min_second_delay=min_second_delay,\n    )\n\n    return df_forecast\n\nThen we can generate and plot the forecasts in the same way as we’ve done before (displaying both the fast and slow strategies in the same figure):\n\n#collapse_show\ndf_sage_fast = forecast_sage(\n    df,\n    scenario=\"fast\",\n)\ndf_sage_slow = forecast_sage(\n    df,\n    scenario=\"slow\",\n)\n\nax = plot_cumulative_doses(\n    df_sage_fast,\n    forecast_date=last_data,\n    figsize=(15, 8),\n    title=f\"SAGE Scenarios\",\n    forecast_label=\"SAGE Fast\",\n)\n\nfor col in [\"cumFirst\", \"cumSecond\"]:\n    ax.plot(\n        df_sage_slow.loc[df_sage_slow.index &gt; last_data, col],\n        color=col_format[col][\"color\"],\n        linestyle=\":\",\n        linewidth=2,\n        label=f\"SAGE Slow {col_format[col]['label']}\",\n    )\n    ax.plot(\n        publish_date,\n        publish_date_data[col],\n        \"o\",\n        color=col_format[col][\"color\"],\n        markersize=10,\n        label=f\"{publish_date.strftime('%d %b')}, {col_format[col]['label']}\",\n    )\n\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe9712404d0&gt;\n\n\n\n\n\nThe SAGE forecasts are similar to our more pessimistic forecasts, with the population fully vaccinated in mid-September, though the difference between their “slow” and “fast” strategies is smaller than the uncertainty we calculated. This gives us confidence that our forecasting approaches are reasonable, though it’s interesting that SAGE does not expect the current rate of vaccinations to be maintained. It may be that they are aware of upcoming difficulties in vaccine supplies, or simply that taking a cautious approach is more appropriate for their purposes.\nWe should also note that we’ve allowed vaccines to be given sooner than 11 weeks in the latter stages of the programme in our “SAGE forecasts” above, but the original SAGE report does not account for this. If second doses are not given earlier it would push the completion dates of all our forecasts back a few weeks to early to mid-October, potentially missing the government target.\nFinally, we’ve included markers in the figure above that show the current status on 16th May, the time of publication. The actual values look similar to our forecast values, but there have actually been more total doses (first and second combined) overall than we predicted, with 6.7 million doses since the 4th May. In comparison, our optimistic forecast using the average doses in the 2 weeks leading up to 4th May predicted 6.4 million doses, or 5.7 million in our pessimistic forecast using the average rate from the whole year so far (the SAGE scenarios are 5.6 million and 6.0 million). However, we have correctly predicted how many second doses would be given by this stage - 4.66 million compared to 4.65 million in our forecasts assuming an eleven week gap between doses. We look forward to seeing how this develops, and plan to do a story in the future to evaluate how well our forecasts performed!"
  },
  {
    "objectID": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#conclusion",
    "href": "stories/2021-05-28-COVID-Vaccination-Forecasting/2021-05-19-COVID-19-Vaccine-Forecasting.html#conclusion",
    "title": "When will the United Kingdom be Fully Vaccinated Against COVID-19?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this Turing Data Story, we have used data collected from the UK government’s Covid-19 data dashboard (using their API) to estimate when the UK may finish vaccinating its adult population.\nWith the demands and challenges of worldwide vaccine supply, it’s difficult to accurately estimate how many vaccine doses will be available in the future, so we generated two sets of forecasts - a more optimistic forecast assuming that vaccines will continue to be given at an average of 0.49 million doses per day, which was the rate in the two weeks up to the 4th of May, and a more pessimistic forecast assuming vaccines are given at a rate consistent with the whole year so far. In our optimistic forecast the population is fully vaccinated on 26th August, or in our pessimistic forecasts a few weeks later in mid-September. Our pessimistic forecast is broadly in agreement with the assumptions in modelling by SAGE and with government targets.\nAs well as the uncertainty in future vaccine supply and demand, our forecasts made a number of simple assumptions about the vaccine roll-out which may not prove to be correct. We assumed 100% of the adult population will be vaccinated, for example, but we know take up rates are much lower in certain sub-groups and some people are unable to be vaccinated due to other health conditions. The number of people vaccinated is therefore likely to plateau earlier than the dates seen in our forecasts (and with fewer people vaccinated). In addition, we haven’t considered differences between current and new vaccine types (which may have different dosing regimens) or regional differences. Our forecasts also assume constant vaccination rates, but a model incorporating the possibility of upward or downward trends may be more accurate. Finally, once the whole adult population has been vaccinated with a first dose we’ve assumed second doses will start to be given earlier than 11-weeks, but we don’t know whether this will be the case. All these combined mean the range of feasible completion dates is wider than we’ve calculated.\nNevertheless, we can be hopeful that the majority of the population will be vaccinated by early autumn, and that life in the UK will be much less impacted by Covid-19 on a daily basis by that time, though it’s difficult to say whether it will return back to what was considered to be “normal” before 2020! Finishing by the autumn would also be perfect timing for the anticipated start of giving further booster doses to the most vulnerable in time for winter.\nWorldwide, Covid-19 case rates are sadly still high and vaccination rates low in many countries at the time of writing. This excellent dashboard from Our World in Data shows vaccination progress worldwide, including links to data sources that (with some pre-processing) could also be fed into the forecasting functions developed here. For European countries, the European Centre for Disease Prevention and Control has a combined dataset available. Do feel free to experiment with the code from this story; ideas could include looking at the forecast results for different dates in the past (by changing run_as_date), using data from other countries, or trying different forecasting algorithms, for example. Thanks for reading!"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "",
    "text": "All the authors above contributed to this data story story with ideas, code development and story telling."
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#covid-19-and-deprivation",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#covid-19-and-deprivation",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "COVID-19 and Deprivation",
    "text": "COVID-19 and Deprivation\nEveryone has been impacted by the COVID-19 pandemic. On the 23rd of March 2020, the UK Government announced various lockdown measures with the intention of limiting the spread of the virus and reducing the number of COVID-19 related deaths. These lockdown measures meant the temporary closure of many commercial shops and businesses, as well as the limiting of work based travel to only those jobs that could not be done at home.\nWe are concerned that the impact of COVID-19 has disproportionately affected certain groups of people. In particular, that the lockdown may have had a worse impact on those in the most deprived areas, whose livelihoods may have required them to leave the house more frequently.\nThere have been a number of concerns with Government COVID-19 reporting, in particular with testing and mortality statistics. This motivates independent, open analysis to validate and expand on our understanding of our current state of the pandemic.\nEarlier in June, the Office of National Statistics (ONS) published a report exploring this exact question: to assess whether those living in the most deprived areas of the UK were disproportionately affected by COVID-19. The report seems to confirm our fear - between the months of March to May 2020 those in the most deprived areas of the UK were more than twice as likely to die as a result of COVID-19 than those in the least deprived areas.\nThere are two caveats that we have with the ONS analysis. The first is reproducibility. We want to confirm the ONS results by making the analysis procedure open. The second caveat is that the ONS report aggregates data over time, and therefore that it might miss interesting differences in outcomes between the different stages of lockdown. The lockdown was most severe between March and May, with measures relaxing from June onwards. We wonder whether the ONS analysis will continue to be relevant as lockdown eases. For this purpose, we wish to extend the ONS analysis to cover all available data, and at the same time, make a comparison between the different stages of lockdown.\nThus for our first story we ask:\nHave the COVID-19 lockdown measures protected people equally across all socio-economic groups in society?\nWe have two main objectives\n\nWe want to replicate the ONS analysis using their provided data to ensure that we have all the inputs necessary to understand the problem.\nWe want to extend the ONS analysis to consider different time periods - representing the severity of the different stages of lockdown - to see how this affects people from different socio-economic groups.\n\n\nKey Metrics\nOur analysis will involve exploring the relationship between the following key metrics:\n\nCOVID-19 mortality rates over time and across geographical regions.\nIndex of multiple deprivation (IMD) by geographical region - a measure of the geographic spread of social deprivation (see definition and explanation).\n\n\n\nData Sources\nWe will use the following datasets:\n\nMortality count time series\nIMD Rankings (England only)\nPopulations provided by the ONS\nLocal Authority District Code Region Lookup Table provided by the ONS\nONS Mortality and Depravation Data\n\nIn case any of the data sources become unavailable in the future, a download mirror is available here.\nFor simplicity this study is only focusing on England. We understand the importance of investigating all of the regions of the UK. However due to the difference of lockdown measures across the nations of the UK, and also due to the way the IMD ranking is defined, an independent analysis is required for each nation. We encourage readers to replicate our analysis with the other nations.\n\n\nAnalysis Outline\nHere’s a list of the different steps of the analysis:\n\nDownload and process data from multiple deprivation and COVID-19 deaths.\nCombine the different datasets into a single table by joining on geographical region.\nCalculate age standardised mortality rates from mortality counts.\nReplicate the ONS analysis, looking at mortality rate by region.\nVisualise the distribution of COVID-19 deaths across the UK.\nSegment the data into time periods, corresponding to the different stages of lockdown.\nExplore at the relationship between our two key metrics (deprivation and mortality rates) in the different time periods."
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#setup",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#setup",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "🔧 Setup",
    "text": "🔧 Setup\nWe begin by setting up our environment and importing various python libraries that we will be using for the analysis. In particular, pandas and numpy are key data science libraries used for data processing. matplotlib and seaborn will help us visualise our data.\n\nimport os\nimport requests\nfrom datetime import datetime\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import  pearsonr\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='white')\n\n🔧 Let’s make some directories in which we can store the data we are going to download.\n\n# downloaded data goes here\ndownloaded_data_dir = 'data/downloaded'\n\n# processed data goes here\nderived_data_dirname = 'data/derived'\n\n# create the directory if it does not already exist\nos.makedirs(downloaded_data_dir, exist_ok=True)\nos.makedirs(derived_data_dirname, exist_ok=True)\n\n🔧 Here is a small helper function which will download files from a URL.\n\n# This function can download data from a URL and then save it in a directory of our choice.\ndef download_file(url, filename):\n        \n    # create the directory if it does not already exist\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n\n    # make the HTTP request\n    r = requests.get(url, allow_redirects=True)\n\n    # save file\n    _ = open(filename, 'wb').write(r.content)"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#index-of-multiple-deprivation-imd",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#index-of-multiple-deprivation-imd",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Index of Multiple Deprivation (IMD)",
    "text": "Index of Multiple Deprivation (IMD)\n🔧 Now let’s download and process our deprivation data. This data provides a deprivation rank (lower rank meaning more deprived) for each geographical region in England (the geographical regions are here are called Lower Super Output Areas, or LSOAs). As a rough idea of scale, LSOAs contain on average around 1,500 people.\n\nDownload\n\n# specify URL\nurl = 'https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833970/File_1_-_IMD2019_Index_of_Multiple_Deprivation.xlsx'\n\n# specify filename\nfilename = 'ONS_2019_Index_of_Multiple_Deprivation.xlsx'\n\n# construct file path\nfilepath = os.path.join(downloaded_data_dir, filename)\n\n# download and save file at the specified URL\ndownload_file(url, filepath)\n\n# read the relevant sheet\nimd_df = pd.read_excel(filepath, sheet_name='IMD2019')\n\nIf we sort by deprivation rank, we can get an idea of the most / least deprived LSOAs.\n\nimd_df.sort_values(by='Index of Multiple Deprivation (IMD) Rank').head()\n\n\n\n\n\n\n\n\nLSOA code (2011)\nLSOA name (2011)\nLocal Authority District code (2019)\nLocal Authority District name (2019)\nIndex of Multiple Deprivation (IMD) Rank\nIndex of Multiple Deprivation (IMD) Decile\n\n\n\n\n21400\nE01021988\nTendring 018A\nE07000076\nTendring\n1\n1\n\n\n12280\nE01012673\nBlackpool 010A\nE06000009\nBlackpool\n2\n1\n\n\n12288\nE01012681\nBlackpool 006A\nE06000009\nBlackpool\n3\n1\n\n\n12279\nE01012672\nBlackpool 013B\nE06000009\nBlackpool\n4\n1\n\n\n12278\nE01012671\nBlackpool 013A\nE06000009\nBlackpool\n5\n1\n\n\n\n\n\n\n\n\nimd_df.sort_values(by='Index of Multiple Deprivation (IMD) Rank').tail()\n\n\n\n\n\n\n\n\nLSOA code (2011)\nLSOA name (2011)\nLocal Authority District code (2019)\nLocal Authority District name (2019)\nIndex of Multiple Deprivation (IMD) Rank\nIndex of Multiple Deprivation (IMD) Decile\n\n\n\n\n17759\nE01018293\nSouth Cambridgeshire 012B\nE07000012\nSouth Cambridgeshire\n32840\n10\n\n\n15715\nE01016187\nBracknell Forest 002D\nE06000036\nBracknell Forest\n32841\n10\n\n\n30976\nE01031773\nMid Sussex 008D\nE07000228\nMid Sussex\n32842\n10\n\n\n26986\nE01027699\nHarrogate 021A\nE07000165\nHarrogate\n32843\n10\n\n\n17268\nE01017787\nChiltern 005E\nE07000005\nChiltern\n32844\n10\n\n\n\n\n\n\n\n\n\nDerive Mean IMD Decile\nAt this point we want to join the two datasets together in order to explore the relationship between our two key metrics.\nA problem is that the index of multiple deprivation comes with a geographical granularity at the LSOA level, whilst the COVID-19 mortality counts come with a geographical granularity at the Local Authority District (LAD) level. To complicate things, for each LAD there are generally multiple LSOAs, each with different indexes of multiple deprivation. For more information about the different geographical regions in the UK, read this.\nWe need to aggregate the LSOAs into LADs by averaging out the indexes of multiple deprivation. First let’s write some functions to help us.\n\ndef get_mean_IMD_decile(LAD_code):\n    # select relevant LSOAs\n    LSOAs = imd_df[imd_df['Local Authority District code (2019)'] == LAD_code]\n    \n    # calculate mean IMD rank\n    mean_IMD_decile = round(LSOAs['Index of Multiple Deprivation (IMD) Decile'].mean(), 2)\n    std_IMD_decile = round(LSOAs['Index of Multiple Deprivation (IMD) Decile'].std(), 2)\n    \n    return mean_IMD_decile, std_IMD_decile\n\nNow we can use these functions to calculate the mean IMD decile in each Local Authority District.\n\nLAD_codes = imd_df['Local Authority District code (2019)'].unique()\nmean_IMD_decile, std_IMD_decile  = np.vectorize(get_mean_IMD_decile)(LAD_codes)\n\nLAD_df = pd.DataFrame({'LAD Code': LAD_codes, \n                       'LAD Name': imd_df['Local Authority District name (2019)'].unique(),\n                       'Mean IMD decile': mean_IMD_decile,\n                       'Std IMD decile': std_IMD_decile})\n\nLAD_df = LAD_df.set_index('LAD Code')\n\nLet’s make a quick histogram of the mean IMD decile.\n\nLAD_df['Mean IMD decile'].hist(range=(1,11), bins=10)\nplt.xlabel('Mean IMD Decile')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\nIt should be noted that we lose some information when averaging the IMD ranks in this way. The central region of the distribution is relatively flat, and so we cannot differentiate well between LADs in this region.\nNotice there are no Local Authority Districts that have a mean IMD decile of 1 or 10. This is due to the presence of variance inside each Local Authority District. For example, there is no single LAD whose constituent LSOAs all have a IMD deciles of 1 (or 10). See the table below for the maximum and minimum mean IMD deciles. Note that Blackpool, the most deprived (on average) LAD in England, has a mean IMD decile of 2.41. This demonstrates that this LAD has some LSOAs that are not in the most deprived deciles. The opposite is true for the least deprived areas. The “Std IMD decile” column in the below table shows the level of variation of the IMD (measured by the standard deviation) within each LAD.\n\nLAD_df.sort_values(by='Mean IMD decile')\n\n\n\n\n\n\n\n\nLAD Name\nMean IMD decile\nStd IMD decile\n\n\nLAD Code\n\n\n\n\n\n\n\nE06000009\nBlackpool\n2.41\n1.58\n\n\nE08000003\nManchester\n2.54\n1.84\n\n\nE08000011\nKnowsley\n2.56\n1.91\n\n\nE09000002\nBarking and Dagenham\n2.68\n1.01\n\n\nE09000012\nHackney\n2.74\n1.11\n\n\n...\n...\n...\n...\n\n\nE07000155\nSouth Northamptonshire\n8.78\n1.32\n\n\nE07000176\nRushcliffe\n8.82\n1.58\n\n\nE07000005\nChiltern\n8.86\n1.51\n\n\nE06000041\nWokingham\n9.27\n1.43\n\n\nE07000089\nHart\n9.39\n1.08\n\n\n\n\n317 rows × 3 columns"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#derive-age-standardisation-weight",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#derive-age-standardisation-weight",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Derive Age Standardisation Weight",
    "text": "Derive Age Standardisation Weight\nTo account for the different population sizes in the different Local Area Districts, we want to use a mortality rate rather than an overall count. When we do this we convert a count into a rate per 100,000 people. Furthermore, we want to account for differences in the age distributions of the different LADs in order to make a valid comparison between the different geographic areas. An age standardised rate allows for this comparison. Ideally we would calculate this rate directly from the data, but as our mortality over time dataset does not contain information about age, we instead will need to extract a standardisation factor from a different dataset.\nThe dataset we will use to do this comes from the ONS study on COVID-19 and deprivation. We will use it to derive a standardisation factor which will allow us to convert our mortality counts into an age and population standardise mortality rate. This mortality rate is a European standard (2013 ESP). As we mentioned, we cannot calculate the factor directly as our mortality over time dataset does not include age information, so this reverse engineering is the best we can do.\nFor more information on how the mortality rate is calculated, see here. Simply put, this is the formula that we are assuming approximates the relationship between the age standardised rate and the mortality count:\nage standardised mortality rate = [standardisation factor] * [mortality count]\n⚠️ The above procedure is not ideal because it assumes that the distribution of ages of those who died inside each Local Area District is constant in time, and therefore the standardisation factor we derive in one dataset (which doesn’t have information about time) can be applied to the other (which has information about time).\nFirst, let’s download the data.\n\n# download the ONS data from the deprivation study\nurl = 'https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fdeathsinvolvingcovid19bylocalareaanddeprivation%2f1march2020to17april2020/referencetablesdraft.xlsx'\n\n# specify filename\nfilename = 'ONS_age_standarisation_April2020.xlsx'\n\n# construct file path\nfilepath = os.path.join(downloaded_data_dir, filename)\n\n# download and save file at the specified URL\ndownload_file(url, filepath)\n\n# read the relevant sheet\nage_rate_df = pd.read_excel(filepath, sheet_name='Table 2', header=3)\n\nNext, we can do some minor selection and reformatting of the DataFrame.\n\n# data if given for many categories and regios, lets choose the inclusive gender, and the unitary authority levels\nage_rate_persons_df = age_rate_df[age_rate_df['Sex'] == 'Persons'] \n\n# rename columns\nage_rate_persons_df.columns = ['Sex', 'Geography type', 'LAD Code', 'Area name', 'All causes Deaths',\n       'All causes Rate','' ,'All causes Lower CI', 'All causes Upper CI','' ,'COVID-19 Deaths',\n       'COVID-19 Rate', '','COVID-19 Lower CI', 'COVID-19 Upper CI' ]\n\n# remove anomalous row (Isles of Scilly) without numerical data\nage_rate_persons_df = age_rate_persons_df[age_rate_persons_df['All causes Rate'] != ':']\nage_rate_persons_df = age_rate_persons_df[age_rate_persons_df['COVID-19 Rate'] != ':']\n\n# remove columns where all entries are NaN due to the spreadsheet formating.\nage_rate_persons_df.dropna(axis=1)\nage_rate_persons_df.head()\n\n\n\n\n\n\n\n\nSex\nGeography type\nLAD Code\nArea name\nAll causes Deaths\nAll causes Rate\n\nAll causes Lower CI\nAll causes Upper CI\n\nCOVID-19 Deaths\nCOVID-19 Rate\n\nCOVID-19 Lower CI\nCOVID-19 Upper CI\n\n\n\n\n1\nPersons\nUnitary Authority\nE06000001\nHartlepool\n154\n170.7\nNaN\n143.5\n197.8\nNaN\n29\n31\nNaN\n20.7\n44.5\n\n\n2\nPersons\nUnitary Authority\nE06000002\nMiddlesbrough\n289\n256\nNaN\n226.1\n286\nNaN\n89\n79\nNaN\n63.2\n97.6\n\n\n3\nPersons\nUnitary Authority\nE06000003\nRedcar and Cleveland\n215\n142.6\nNaN\n123.5\n161.8\nNaN\n40\n26.5\nNaN\n18.9\n36.2\n\n\n4\nPersons\nUnitary Authority\nE06000004\nStockton-on-Tees\n297\n167\nNaN\n147.8\n186.1\nNaN\n38\n21\nNaN\n14.8\n28.9\n\n\n5\nPersons\nUnitary Authority\nE06000005\nDarlington\n169\n151.5\nNaN\n128.6\n174.4\nNaN\n26\n22.9\nNaN\n15\n33.7\n\n\n\n\n\n\n\nLet us now calculate the factor by which we need to multiply the count of deaths to derive the age-standardised mortality rate per 100,000 habitants.\n\n# derive standardisation factors\nage_rate_persons_df['All causes rate factor'] = (\n    age_rate_persons_df['All causes Rate'] / age_rate_persons_df['All causes Deaths'] )\n\nage_rate_persons_df['COVID-19 rate factor'] = (\n    age_rate_persons_df['COVID-19 Rate'] / age_rate_persons_df['COVID-19 Deaths'] )\n\n# drop columns\nage_rate_persons_df = age_rate_persons_df[['LAD Code', 'All causes rate factor', 'COVID-19 rate factor']]\n\nWe can merge this into the previous DataFrame so all the information is accessible in one place.\n\nLAD_df = LAD_df.reset_index()\nLAD_df = LAD_df.merge(age_rate_persons_df, on='LAD Code', how='inner')\nLAD_df = LAD_df.set_index('LAD Code')\n\nFinally, let’s save the standardisation factors for each LAD, stored in the DataFrame LAD_df, so that we can easily use them later.\n\n# create filename\nLAD_df_filename = 'Local_Authority_District_Lookup.csv'\nLAD_df_filepath = os.path.join(derived_data_dirname, LAD_df_filename)\n\n# write to csv\nLAD_df.to_csv(LAD_df_filepath, index=False)"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#mortality-counts",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#mortality-counts",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Mortality Counts",
    "text": "Mortality Counts\nNow we are ready to download the main dataset that we will be analysing: the number of COVID-19 and non COVID-19 deaths across time and place.\n\nDownload and Format\nLet’s download the ONS dataset containing mortality counts by week and Local Authority District.\n\n# specify URL\nurl = 'https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fhealthandsocialcare%2fcausesofdeath%2fdatasets%2fdeathregistrationsandoccurrencesbylocalauthorityandhealthboard%2f2020/lahbtablesweek35.xlsx'\n\n# specify filename\nfilename = 'ONS_COVID_Mortality_Counts.xlsx'\n\n# construct file path\nfilepath = os.path.join(downloaded_data_dir, filename)\n\n# download and save file at the specified URL\ndownload_file(url, filepath)\n\n\n# specify the sheet of the excel file we want to read\nsheet_name = 'Occurrences - All data'\n\n# read the sheet into a pandas DataFrame\nmortality_df = pd.read_excel(filepath, sheet_name=sheet_name, header=3)\n\nLet’s quickly check if all the LADs are represented in both datasets so that we can join the IMD rank with the mortality information for each LAD.\n\nnot_in_imd = set(mortality_df['Area code']) - set(imd_df['Local Authority District code (2019)'])\nnot_in_mortality = set(imd_df['Local Authority District code (2019)']) - set(mortality_df['Area code'])\n\nprint('There are', len(not_in_mortality), 'codes in the IMD dataset but not in the mortality dataset.')\nprint('There are', len(not_in_imd), 'codes in the mortality dataset but not in the IMD dataset.')\n\nThere are 4 codes in the IMD dataset but not in the mortality dataset.\nThere are 30 codes in the mortality dataset but not in the IMD dataset.\n\n\nWe have 346 LAD codes in the mortality data set, and only 317 in the IMD dataset. Upon closer inspection, it turned out that IMD dataset does not contain any Welsh entries (as the IMD ranking is defined for England only). Additionally, the mortality dataset contains a single entry for Buckinghamshire, a new unitary authority in 2020 (E06000060). The IMD dataset, meanwhile, contains 4 LAD codes for Buckinghamshire. We will drop these anomalous locations from the analysis for now.\n\n# extract those LAD codes which are present in the mortality dataset but not the IMD dataset (Wales)\nmissing_LAD_codes_df = mortality_df[~mortality_df['Area code'].isin(imd_df['Local Authority District code (2019)'])]\nmissing_LAD_codes = missing_LAD_codes_df['Area code'].unique()\n\n# filter by common LAD codes\nmortality_df = mortality_df[~mortality_df['Area code'].isin(missing_LAD_codes)]\n\nFurthermore, the age standardisation factor derived previously was not able to be derived for one LAD (the Isles of Scilly). Let’s drop that now too to avoid any problems later down the line.\n\n# remove LADs from the mortality DataFrame if we do not have an entry for them in the LAD_df\nmortality_df = mortality_df[mortality_df['Area code'].isin(LAD_df.index)]\n\nFinally, since we are interested in looking at the effect of COVID-19 and the lockdown policies on the working population, we can remove deaths that took place in care homes or hospices.\n\n# select only deaths outside of care homes and hospices\nmortality_df = mortality_df[(mortality_df['Place of death'] != 'Care home') & \n                            (mortality_df['Place of death'] != 'Hospice')]\n\n# to instead select only deaths in care homes or hospices, use this line:\n#mortality_df = mortality_df[(mortality_df['Place of death']=='Care home') | \n#                            (mortality_df['Place of death']=='Hospice')]\n\nThe mortality data starts from Wednesday 1st Jan 2020. Let’s use that to convert the supplied week numbers into a date.\n\n# first day of 2020 is a Wednesday\nmortality_df['Date'] = [datetime.strptime(f'2020 {n-1} 3', '%Y %W %w').strftime('%Y-%m-%d') \n                        for n in mortality_df['Week number']]\n\n# drop week number column\nmortality_df = mortality_df.drop(columns='Week number')\n\nFinally, we can take a random sample of 5 rows from the DataFrame to check everything looks okay, and to get an idea of its structure.\n\nmortality_df.sample(n=5)\n\n\n\n\n\n\n\n\nArea code\nGeography type\nArea name\nCause of death\nPlace of death\nNumber of deaths\nDate\n\n\n\n\n43031\nE07000136\nLocal Authority\nBoston\nCOVID 19\nOther communal establishment\n0\n2020-03-11\n\n\n27659\nE08000002\nLocal Authority\nBury\nCOVID 19\nOther communal establishment\n0\n2020-02-12\n\n\n41644\nE06000043\nLocal Authority\nBrighton and Hove\nAll causes\nHospital\n13\n2020-03-11\n\n\n28045\nE08000036\nLocal Authority\nWakefield\nAll causes\nElsewhere\n2\n2020-02-12\n\n\n77533\nE09000007\nLocal Authority\nCamden\nAll causes\nElsewhere\n0\n2020-05-06\n\n\n\n\n\n\n\nIf you want to reproduce the results from the initial ONS report, you can restrict the date ranges of the data by uncommenting these lines.\n\n#mortality_df = mortality_df[mortality_df['Date'] &gt; '2020-03-01']\n#mortality_df = mortality_df[mortality_df['Date'] &lt; '2020-04-18']"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#download-local-area-district-to-region-lookup-table",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#download-local-area-district-to-region-lookup-table",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Download Local Area District to Region Lookup Table",
    "text": "Download Local Area District to Region Lookup Table\nAs shown in the ONS report, a nice plot to make is the total number of mortalities in each region of England (a region is composed of many LADs). To do this, we need to know which region each LAD belongs. Let’s download this data now from the following website: https://geoportal.statistics.gov.uk/datasets/local-authority-district-to-region-april-2019-lookup-in-england.\n\n# specify URL\nurl = 'https://opendata.arcgis.com/datasets/3ba3daf9278f47daba0f561889c3521a_0.csv'\n\n# specify filename\nfilename = 'LAD_Code_Region_Lookup.csv'\n\n# construct file path\nfilepath = os.path.join (downloaded_data_dir, filename)\n\n# download and save file at the specified URL\ndownload_file(url, filepath)\n\n# read the relevant sheet\nLAD_code_region_lookup_df = pd.read_csv(filepath, index_col='FID').set_index('LAD19CD')\n\nTaking a look at the data, we can see that the index “LAD19CD” contains our familiar LAD code, and the column “RGN12NM” gives us the name of the region in which that LAD is located. Perfect!\n\nLAD_code_region_lookup_df.head()\n\n\n\n\n\n\n\n\nLAD19NM\nRGN19CD\nRGN19NM\n\n\nLAD19CD\n\n\n\n\n\n\n\nE09000001\nCity of London\nE12000007\nLondon\n\n\nE06000054\nWiltshire\nE12000009\nSouth West\n\n\nE09000002\nBarking and Dagenham\nE12000007\nLondon\n\n\nE09000003\nBarnet\nE12000007\nLondon\n\n\nE09000004\nBexley\nE12000007\nLondon"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#split-data-into-different-time-periods",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#split-data-into-different-time-periods",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Split Data Into Different Time Periods",
    "text": "Split Data Into Different Time Periods\nNow, the final step before we can really start our analysis is to split our dataset into different time periods. As mentioned in the introduction, we want to compare COVID-19 mortality before, during and after lockdown was in place. Within each time period, we sum over all deaths, and add the IMD decile for each LAD. Finally, we’ll also include Region information.\nLet’s write a function to sum the mortality data over time to get the total number of deaths. Then, we’ll reformat the DataFrame to separate COVID-19 and non COVID-19 deaths. Finally, we’ll use the table downloaded in the previous section to get the Region name for each LAD. The function combines information from all the previous DataFrames and produces a DataFrame with everything we need to do our analysis.\n\ndef filter_date_and_aggregate(df, date_range=None):\n    \"\"\"\n    The function:\n    - Selects dates that are inside the supplied date range.\n    - Sums over time in this date range.\n    - Separates COVID-19 vs non COVID-19 deaths.\n    - Decorates rows with area and region name columns.\n    - Calculates the standardised mortality rate using previously calculated factors.\n    - Pulls in the mean IMD decile as previously calculated.\n    \"\"\"\n    \n    # filter dates\n    if date_range:\n        df = df[(df['Date'] &gt;= date_range[0]) & (df['Date'] &lt; date_range[1])]\n        if len(df) == 0:\n            print('Error: Please make sure there is some data availbile for the supplied date range!')\n            return None\n    \n    # sum over time\n    df = df.groupby(by=['Area code', 'Cause of death']).sum()\n    df = df.reset_index(level=[-1])\n\n    # seperate out all deaths and COVID deaths as their own columns\n    df = df.pivot(columns='Cause of death', values='Number of deaths')\n    df.columns.name = ''\n\n    # rename columns\n    df = df.rename(columns={'All causes': 'Total deaths', 'COVID 19': 'COVID deaths'})\n\n    # add non-COVID deaths as column\n    df['Non COVID deaths'] = df['Total deaths'] - df['COVID deaths']\n\n    # add area names\n    df['Area name'] = LAD_df.loc[df.index]['LAD Name']\n\n    # add region names\n    df['Region name'] = LAD_code_region_lookup_df.loc[df.index]['RGN19NM']\n    \n    # Calculate the rate per 100k using the age-standardisation factor estimated previously\n    df['COVID-19 rate'] = (LAD_df.loc[df.index]['COVID-19 rate factor'] * df['COVID deaths']).astype(float)\n    df['All causes rate'] = (LAD_df.loc[df.index]['All causes rate factor'] * df['Total deaths']).astype(float)\n    \n    # import mean IMD rank\n    df['Mean IMD decile'] = LAD_df['Mean IMD decile']\n    \n    return df\n\n\n# sum over \"Place of death\" column\nmortality_sum_df = mortality_df.groupby(by=['Area code', 'Date', 'Cause of death']).sum().reset_index()\n\nFirst, let’s agreggate the data without splitting into different time periods.\n\n# this line performs the agreggation step (summing mortality over time).\n# and also includes information about deprivation, no date filtering yet.\ntotal_deaths_df = filter_date_and_aggregate(mortality_sum_df)\n\nNow we can split up the data into three periods. The first period is from January 1st to April 7th - 16 days from the beginning of lockdown. The second period runs from April 7th to June 1st - 16 days after the stay at home order was lifted. The final period runs from June 1st to August 28th.\nWe use a time delay of 16 days after key policy decisions to account for the time lag between onset of the disase and death. The number was taken from this study: “Clinical characteristics of 113 deceased patients with coronavirus disease 2019: retrospective study” BMJ 2020;368:m1091.\n\n# first date range\nfirst_date_range = ('2020-01-01', '2020-04-07')\nfirst_df = filter_date_and_aggregate(mortality_sum_df, first_date_range)\nfirst_df['period'] = 1\n\n# second date range\nsecond_date_range = ('2020-04-07', '2020-06-01')\nsecond_df = filter_date_and_aggregate(mortality_sum_df, second_date_range)\nsecond_df['period'] = 2\n\n# second date range\nthird_date_range = ('2020-06-01', '2020-08-28')\nthird_df = filter_date_and_aggregate(mortality_sum_df, third_date_range)\nthird_df['period'] = 3\n\n\nprint('Total deaths from COVID-19 in before lockdown period:\\t', first_df['COVID deaths'].sum())\nprint('Total deaths from COVID-19 in during lockdown period:\\t', second_df['COVID deaths'].sum())\nprint('Total deaths from COVID-19 in after lockdown period:\\t', third_df['COVID deaths'].sum())\n\nTotal deaths from COVID-19 in before lockdown period:    6521\nTotal deaths from COVID-19 in during lockdown period:    24179\nTotal deaths from COVID-19 in after lockdown period:     3217\n\n\nLet’s also divide the rate by the number of days in each time period, which will give us the the age standardised mortality rate per day.\n\ndef get_num_days(date_range):\n    d0 = datetime.strptime(date_range[1], '%Y-%m-%d')\n    d1 = datetime.strptime(date_range[0], '%Y-%m-%d')\n    return (d0 - d1).days\n\nfull_date_range = ('2020-01-01', '2020-08-28')\ntotal_deaths_df['All causes rate'] = total_deaths_df['All causes rate'] / get_num_days(full_date_range)\ntotal_deaths_df['COVID-19 rate'] = total_deaths_df['COVID-19 rate'] / get_num_days(full_date_range)\n\nfirst_df['All causes rate'] = first_df['All causes rate'] / get_num_days(first_date_range)\nfirst_df['COVID-19 rate']   = first_df['COVID-19 rate']   / get_num_days(first_date_range)\n\nsecond_df['All causes rate'] = second_df['All causes rate'] / get_num_days(second_date_range)\nsecond_df['COVID-19 rate']   = second_df['COVID-19 rate']   / get_num_days(second_date_range)\n\nthird_df['All causes rate'] = third_df['All causes rate'] / get_num_days(third_date_range)\nthird_df['COVID-19 rate']   = third_df['COVID-19 rate']   / get_num_days(third_date_range)\n\n\n# recombining with the additional column\nall_df = pd.concat([first_df,second_df, third_df])\n\nNow we are finished with all of the processing steps and we are ready for the fun part: analysing the data!"
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#total-mortalities-by-region",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#total-mortalities-by-region",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Total Mortalities by Region",
    "text": "Total Mortalities by Region\nFor Figure 1, we will produce a stacked bar chart showing the mortality count (split up into COVID-19 and non COVID-19 deaths) for each Region in England.\nSumming over region and sorting the values, we are almost ready to make the plot.\n\n# sum over LADs in each region\ntotal_deaths_by_region_df = total_deaths_df.groupby(by='Region name').sum()\n\n# sort ascending\ntotal_deaths_by_region_df = total_deaths_by_region_df.sort_values(by='Total deaths', ascending=True)\n\nFinally, let’s write the code to actually make the plot.\n\n# region names\nxs = total_deaths_by_region_df.index\n\n# mortality counts\nnon_covid_deaths = total_deaths_by_region_df['Non COVID deaths']\ncovid_deaths = total_deaths_by_region_df['COVID deaths']\n\n# set bar width\nwidth = 0.75\n\n# colors similar to ONS\ncovid_color = (251/255, 213/255, 59/255, 0.9)\nnoncovid_color = (25/255, 142/255, 188/255, 0.9)\n\n# create a figure and plot data\nplt.figure(figsize=(7,10))\np1 = plt.barh(xs, covid_deaths, width, color=covid_color, label='COVID-19 Deaths')\np2 = plt.barh(xs, non_covid_deaths, width, left=covid_deaths, color=noncovid_color, label='Non COVID-19 Deaths')\n\n# label axes\nplt.xlabel('Deaths Since 01/01/2020', fontsize=16)\nplt.ylabel('Region', fontsize=16)\nplt.yticks(rotation=30)\n\n# add vertical grid lines\nplt.gca().xaxis.grid(True, linestyle='-', which='major', color='grey', alpha=.25)\n\n# show legend and plot\nplt.legend(fontsize=14)\nplt.show()\n\n\n\n\nThere it is! A lot of work but now we can already begin to try and understand what this data is telling us. Here are some conclusions: - For all regions, the number of COVID-19 deaths is smaller than the number of non COVID-19 deaths. - The number of deaths varies a lot between different regions. This can be due to the fact that there are different numbers of people living in each region (for example, there are more people living in the South East than there are in the North East). On top of that, we know that older people have a higher risk of dying after contracting COVID-19, and as the age distributions are different for different regions, this can also affect the overall number of deaths."
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#standardised-mortality-rate-by-region",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#standardised-mortality-rate-by-region",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Standardised Mortality Rate by Region",
    "text": "Standardised Mortality Rate by Region\nTo account for the varying population sizes and age distributions, let’s look at the age-standardised mortality rates per 100,000 people, standardised to the 2013 European Standard Population. Age-standardised mortality rates allow for differences in the age structure of populations and therefore allow valid comparisons to be made between geographical areas, the sexes and over time.\n\n# calculate the mean rate per region and sort\ntotal_rates_df_by_region = total_deaths_df.groupby(by='Region name', as_index=False).agg('mean')\ntotal_rates_df_by_region = total_rates_df_by_region.sort_values(by='All causes rate', ascending=True)\n\n\n# region names\nx_labels = total_rates_df_by_region['Region name']\nxs = np.array(range(len(x_labels)))\n\n# mortality counts\nnon_covid_rate = total_rates_df_by_region['All causes rate']\ncovid_rate = total_rates_df_by_region['COVID-19 rate']\n\n# set bar width\nwidth = 0.4\n\n# create a figure and plot data\nplt.figure(figsize=(7,10))\np2 = plt.barh(xs+0.2, non_covid_rate, width, color=noncovid_color, label='All Causes Mortality Rate', tick_label=x_labels)\np1 = plt.barh(xs-0.2, covid_rate, width, color=covid_color, label='COVID-19 Mortality Rate')\n\n# label axes\nplt.xlabel('Age standardised mortality rate per 100,000 people per day since 01/01/2020', fontsize=16)\nplt.ylabel('Region', fontsize=16)\nplt.yticks(rotation=30)\n\n# add vertical grid lines\nplt.gca().xaxis.grid(True, linestyle='-', which='major', color='grey', alpha=.25)\n\n# show legend and plot\nplt.legend(fontsize=14, loc='lower right')\nplt.show()\n\n\n\n\nNote that as we plot the rates, we switch from a stacked bar chart (showing counts of COVID-19 and non COVID-19 mortalities), to two bar charts side by side (showing the COVID-19 mortality rate, and the all causes mortality rate). Even with this caveat in mind, when looking at the chart we see it tells a very different story to the previous plot. For example, in the previous plot, the South East had the highest number of total deaths, but looking at the standardised rates in this plot we see that it is ranked second from the bottom. This shows that the raw mortality counts do not tell the whole story, and so we cannot rely solely on them to make meaningful comparisons between different Regions."
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#comparing-all-causes-and-covid-19-mortality-rates",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#comparing-all-causes-and-covid-19-mortality-rates",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Comparing All Causes and COVID-19 Mortality Rates",
    "text": "Comparing All Causes and COVID-19 Mortality Rates\nTo get started, let’s overlay the mortality rates for all causes of death and COVID-19 on the same plot.\n\n# plot formatting\npo = {'s': 10, 'alpha':0.5}\n\n# select data\nIMD_decile = total_deaths_df['Mean IMD decile']\nall_causes_rate = total_deaths_df['All causes rate']\ncovid_rate = total_deaths_df['COVID-19 rate']\n\n# calcualte correlations and slopes\nac_stats = get_corr_and_slope(IMD_decile, all_causes_rate)\nc19_stats = get_corr_and_slope(IMD_decile, covid_rate)\n\n# make plots\nplt.figure(figsize=(10,6))\nsns.regplot(x=IMD_decile, y=all_causes_rate, label=f'All causes $r={ac_stats[0]}$', scatter_kws=po)\nsns.regplot(x=IMD_decile, y=covid_rate, color='red', label=f'COVID-19 $r={c19_stats[0]}$', scatter_kws=po)\n\n# format plot\nplt.ylabel('Standardised Mortality Rate', fontsize=14)\nplt.xlabel('Mean IMD Decile', fontsize=14)\nplt.legend(fontsize=14); plt.ylim((0, 3.5))\nplt.show()\n\n\n\n\nIn the plot legends, \\(r\\) is the correlation coefficient. If you are unfamiliar with the concept of correlation, or want a refresher, take a look here.\nRecall that the lower the mean IMD rank in each LAD, the more deprived the area is. There is a negative correlation between the standardised rate and the IMD decile, for both the all causes rate and the COVID-19 rate. The negative correlation tells us that in more deprived areas (those with a lower mean IMD decile), the standardised mortality rate is higher.\nNote however that the strength of the correlations are quite different. COVID-19 appears to discriminate less based on social deprivation than all causes of death combined. This link between social deprivation and mortality has been previously observed - see discussions here and here."
  },
  {
    "objectID": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#time-periods",
    "href": "stories/2020-11-20-COVID-19/2020-11-20-Who_was_protected_by_the_first_COVID-19_lockdown.html#time-periods",
    "title": "Who was protected by the first COVID-19 lockdown in England?",
    "section": "Time Periods",
    "text": "Time Periods\nWe are now finally ready to investigate the relationship between mortality and deprivation in the different periods of lockdown. The three periods we defined are: - Before lockdown: January 1st to April 7th - Duing lockdown: April 7th to June 1st - After lockdown: June 1st to August 28th\nWe will look at this relationship for all causes of death, and for COVID-19 deaths separately.\n\nAll Causes\n\nymax = 4.5\nplt.figure(figsize=(16,5))\n\n# calcualte correlations and slopes\npre_stats = get_corr_and_slope(first_df['Mean IMD decile'], first_df['All causes rate'])\ndur_stats = get_corr_and_slope(second_df['Mean IMD decile'], second_df['All causes rate'])\npos_stats = get_corr_and_slope(third_df['Mean IMD decile'], third_df['All causes rate'])\n\nplt.subplot(131)\nplt.title('Before Lockdown')\nsns.regplot(x='Mean IMD decile', y='All causes rate', data=first_df, label=f\"$r={pre_stats[0]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('All Causes Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, ymax))\n\nplt.subplot(132)\nplt.title('During Lockdown')\nsns.regplot(x='Mean IMD decile', y='All causes rate', data=second_df, label=f\"$r={dur_stats[0]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('All Causes Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, ymax))\n\nplt.subplot(133)\nplt.title('After Lockdown')\nsns.regplot(x='Mean IMD decile', y='All causes rate', data=third_df, label=f\"$r={pos_stats[0]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('All Causes Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, ymax))\n\nplt.show()\n\n\n\n\nAfter splitting the data into the three time periods, we that the the negative correlation persists for each period, but has different strengths. The potential reasons for the differences in strength are numerous. The middle plot refers to the peak period in the number of COVID-19 deaths. This could be one explanation for the interesting effect that we observe in the middle plot: the correlation is lower (the variance is larger), but the slope seems to be steeper.\nTo get a better sense of the number of deaths as a function of time take a look at this ONS study.\n\n\nCOVID-19\nNext we’ll make the same set of three plots, but look specifically at deaths involving COVID-19.\n\nymax = 2.5\nplt.figure(figsize=(16,5))\n\n# calcualte correlations and slopes\npre_stats = get_corr_and_slope(first_df['Mean IMD decile'], first_df['COVID-19 rate'])\ndur_stats = get_corr_and_slope(second_df['Mean IMD decile'], second_df['COVID-19 rate'])\npos_stats = get_corr_and_slope(third_df['Mean IMD decile'], third_df['COVID-19 rate'])\n\nplt.subplot(131)\nplt.title('Before Lockdown')\nsns.regplot(x='Mean IMD decile', y='COVID-19 rate', data=first_df, label=f\"\\t$r={pre_stats[0]}$  $m={pre_stats[1]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('COVID-19 Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, 1))\n\nplt.subplot(132)\nplt.title('During Lockdown')\nsns.regplot(x='Mean IMD decile', y='COVID-19 rate', data=second_df, label=f\"\\t$r={dur_stats[0]}$  $m={dur_stats[1]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('COVID-19 Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, ymax))\n\nplt.subplot(133)\nplt.title('After Lockdown')\nsns.regplot(x='Mean IMD decile', y='COVID-19 rate', data=third_df, label=f\"\\t$r={pos_stats[0]}$  $m={pos_stats[1]}$\", scatter_kws=po)\nplt.xlabel('Mean IMD Decile', fontsize=12)\nplt.ylabel('COVID-19 Standardised Mortality Rate', fontsize=12)\nplt.legend(); plt.ylim((0, 1))\n\nplt.show()\n\n\n\n\n\n⚠️ The “During Lockdown” middle plot contains the peak of COVID-19 mortalities in the UK. The standardised rate is therefore much higher in this plot, and the range of the \\(y\\)-axis has been increased to reflect this.\n\nLooking only at COVID-19 mortalities, we observe that during lockdown and the peak of the pandemic, the strength of the correlation increases. Again, there are many things that could be the cause of this. Our hypothesis is that, during lockdown, those in more socially deprived areas were more likely to be in circumstances that increased their exposure to COVID-19 (for example key workers who are unable to work remotely - see here and here)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "An open community creating “Data Stories”: A mix of open data, code, narrative 💬, visuals 📊📈 and knowledge 🧠 to help understand the world around us. Want to contribute? Take a look here.\n\nPosts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nEsport predictions: Overwatch League\n\n\n\n\n\nWe study a single season of games in the professional esports league for Overwatch and try to predict game outcomes based on past performance.\n\n\n\n\n\n\nOct 11, 2023\n\n\nMarkus Hauru, Tim Powell, Kevin Xu\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling London’s mobility patterns with Boris Bikes\n\n\n\n\n\nUsing Transport for London’s data on the Santander Bike Scheme to understand how Londoners move around the city\n\n\n\n\n\n\nJul 7, 2023\n\n\nJames Bishop, Markus Hauru, Federico Nanni, Camila Rangel Smith\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning for the 20th century - Artifact Classification From an Aviation Mystery\n\n\n\n\n\nCan a United Kingdom database from 1987 classify a glass cosmetics jar that might have belonged to Amelia Earhart? (Photograph credit ID 98648556. Copyright Mickem, Dreamstime.com.)\n\n\n\n\n\n\nOct 14, 2022\n\n\nJoe Cerniglia\n\n\n\n\n\n\n  \n\n\n\n\nDesert Island Discs - Famous people and their musical tastes\n\n\n\n\n\nAs the iconic BBC radio programme turns 80, we explore notable people and the music that tells the stories of their lives.\n\n\n\n\n\n\nMar 9, 2022\n\n\nHelen Duncan, Bill Finnegan, Luke Hare, Camila Rangel Smith, Sam Van Stroud\n\n\n\n\n\n\n  \n\n\n\n\nModelling COVID-19 Hospitalizations in England\n\n\n\n\n\nInvestigating the link between infections and hospitalizations using COVID-19 data in England for 2020-2021 and Bayesian inference.\n\n\n\n\n\n\nNov 3, 2021\n\n\nEric Daub\n\n\n\n\n\n\n  \n\n\n\n\nFrench Pastries, Baseball Players, and Marginal Utility\n\n\n\n\n\nUnderstanding and estimating the replacement value of US baseball players with Bayesian models.\n\n\n\n\n\n\nJul 21, 2021\n\n\nEric Daub\n\n\n\n\n\n\n  \n\n\n\n\nWhen will the United Kingdom be Fully Vaccinated Against COVID-19?\n\n\n\n\n\n\n\nCOVID-19\n\n\nvaccinations\n\n\nforecasting\n\n\n\n\nBy May 2021 the United Kingdom had fully vaccinated one third of the adult population. Based on the progress so far, we estimate when the whole population will be vaccinated.\n\n\n\n\n\n\nMay 19, 2021\n\n\nJack Roberts\n\n\n\n\n\n\n  \n\n\n\n\nModelling Mail-In Votes In the 2020 US Election\n\n\n\n\n\nUsing hierarchical Bayesian modeling to predict outcomes and associated uncertainties in the 2020 US election.\n\n\n\n\n\n\nFeb 26, 2021\n\n\nEric Daub\n\n\n\n\n\n\n  \n\n\n\n\nWho was protected by the first COVID-19 lockdown in England?\n\n\n\n\n\n\n\nCOVID-19\n\n\ndata wrangling\n\n\ndata exploration\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2020\n\n\nDavid Beavan, Camila Rangel Smith, Sam Van Stroud, Kevin Xu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "",
    "text": "Camila Rangel Smith, Alan Turing Institute, GitHub: @crangelsmith. Twitter: @CamilaRangelS.\nMartin O’Reilly, Alan Turing Institute, Github: @martintoreilly."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#other-contributors",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#other-contributors",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "",
    "text": "Camila Rangel Smith, Alan Turing Institute, GitHub: @crangelsmith. Twitter: @CamilaRangelS.\nMartin O’Reilly, Alan Turing Institute, Github: @martintoreilly."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#reviewers",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#reviewers",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Reviewers",
    "text": "Reviewers\n\nSam Van Stroud, University College London, GitHub: @samvanstroud\nKevin Xu, Civil Service Fast Stream, GitHub: @kevinxufs"
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#introduction",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#introduction",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Introduction",
    "text": "Introduction\nThe Covid-19 Pandemic led to record numbers of mail-in votes in the 2020 United States Presidential Election. Because of the high volume of mail ballots, plus rules that prevented some states from counting these ballots before election day, the result of the election remained uncertain for a week, with periodic updates coming as ballots were tabulated and reported.\nIn particular, several states had very close races that had the potential to tip the election in favor of either candidate. The US elects the president using the Electoral College system, where every state has a fixed number of electoral votes depending on its population. These electoral votes determine the outcome, not the national popular vote. The states almost universally employ a “winner-take-all” model for allocating their electoral votes. Because of this, each year a few “swing states” have a large effect on the outcome of the election. For example, in 2000 the election came down to around a 500 vote margin in Florida (out of over 6 million ballots cast), despite the fact that Al Gore easily won the national popular vote. In 2020, a few states with very close races dominated the headlines for the week after the election, of which we will look at Pennsylvania, Arizona, and Georgia in this post. The final outcome of the election hung on the results from these states, and the slow drip feed of additional ballots being released left the public constantly checking the news for updates.\nIn this Turing Data Story, I examine a few ways to analyze the data updates coming from each state to predict the final outcome. This originated from some Slack discussions with Camila Rangel Smith and Martin O’Reilly, whom I list above as contributors for this reason. In particular, our initial interest in this question centered around uncertainties in the analysis done by Camila and Martin, which I have carried out using Bayesian inference to quantify uncertainty and determine when we might have reasonably called each state for the eventual winner based on the data as it underwent regular updates."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#data",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#data",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Data",
    "text": "Data\nTo create the models in the post, I use the NYT Election Data Scraper, which is an open source repository that collected data from the New York Times website every few minutes over the months following the election. We can use this data to explore how the results evolved over time following election day, and compare our results with how the news media reported these changes over time.\nIn particular, the data includes an estimate of the number of votes remaining, which is a crucial figure that we need in order to mathematically forecast the outcome. The New York Times bases their model for the votes remaining using turnout estimates from Edison Research, which is mentioned here. Based on this turnout estimate, combined with the updates of votes as they are counted, we can use this data to forecast the outcome.\nTo load this data into a Python session for analysis, I can use Pandas to simply load from the CSV version of the data directly from the URL, and extract the state that I wish to examine:\n\n%matplotlib inline\n\n\nimport pandas\nimport datetime\n\ndef load_data(state, timestamp=None):\n    \"\"\"\n    Loads election data updates from CSV file as a pandas data frame\n\n    Retrieves data from the live file on Github, which is loaded into a\n    data frame before extracting the relevant state data.\n\n    State must be a string, which will be searched in the \"state\" field\n    of the data frame.\n\n    Timestamp must be a datetime string. Optional, default is current\n    date and time.\n\n    Returns a data frame holding all updates from a particular state,\n    prior to the given timestamp. The \"vote_differential\" field is turned\n    into a signed margin that is positive for a Biden lead. Also adds\n    columns for the number of votes for Biden and Trump.\n    \"\"\"\n\n    if timestamp is None:\n       timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n    try:\n        data = pandas.read_csv(\"https://alex.github.io/nyt-2020-election-scraper/battleground-state-changes.csv\")\n    except:\n        data = pandas.read_csv(\"battleground-state-changes.csv\")\n\n    data.loc[data[\"leading_candidate_name\"] == \"Trump\", \"vote_differential\"] \\\n        = -data.loc[data[\"leading_candidate_name\"] == \"Trump\", \"vote_differential\"]\n\n    data[\"biden_votes\"] = None\n    data.loc[data[\"leading_candidate_name\"] == \"Biden\", \"biden_votes\"] \\\n        = data.loc[data[\"leading_candidate_name\"] == \"Biden\", \"leading_candidate_votes\"]\n    data.loc[data[\"trailing_candidate_name\"] == \"Biden\", \"biden_votes\"] \\\n        = data.loc[data[\"trailing_candidate_name\"] == \"Biden\", \"trailing_candidate_votes\"]\n\n    data[\"trump_votes\"] = None\n    data.loc[data[\"leading_candidate_name\"] == \"Trump\", \"trump_votes\"] \\\n        = data.loc[data[\"leading_candidate_name\"] == \"Trump\", \"leading_candidate_votes\"]\n    data.loc[data[\"trailing_candidate_name\"] == \"Trump\", \"trump_votes\"] \\\n        = data.loc[data[\"trailing_candidate_name\"] == \"Trump\", \"trailing_candidate_votes\"]\n\n    data[\"timestamp\"] = pandas.to_datetime(data[\"timestamp\"])\n    data = data[data[\"timestamp\"] &lt; pandas.to_datetime(timestamp)]\n\n    return data[data[\"state\"].str.contains(state)]\n\nNote that rather than specifying the leading and trailing candidates, I instead just convert the vote differential into a margin that is positive if Biden is leading and negative if Trump is leading. I also add columns for the total number of votes for Biden and Trump, which I will use later.\nFor instance, if I would like to see the data for Georgia:\n\ndf = load_data(\"Georgia\")\ndf.head()\n\n\n\n\n\n\n\n\nstate\ntimestamp\nleading_candidate_name\ntrailing_candidate_name\nleading_candidate_votes\ntrailing_candidate_votes\nvote_differential\nvotes_remaining\nnew_votes\nnew_votes_relevant\n...\ntrailing_candidate_partition\nprecincts_reporting\nprecincts_total\nhurdle\nhurdle_change\nhurdle_mov_avg\ncounties_partition\ntotal_votes_count\nbiden_votes\ntrump_votes\n\n\n\n\n112\nGeorgia (EV: 16)\n2020-12-07 20:17:22.360\nBiden\nTrump\n2473633\n2461854\n11779\n-2224\n-8\n1\n...\n75.000000\n2655\n2655\n0.0\n0.0\n0.360367\n{}\n4997716\n2473633\n2461854\n\n\n113\nGeorgia (EV: 16)\n2020-12-05 00:26:57.152\nBiden\nTrump\n2473707\n2461779\n11928\n-2232\n-842\n-858\n...\n0.067599\n2655\n2655\n0.0\n0.0\n0.368200\n{'Appling': 49, 'Ben Hill': 4, 'Bleckley': 2, ...\n4997724\n2473707\n2461779\n\n\n114\nGeorgia (EV: 16)\n2020-11-20 22:11:31.395\nBiden\nTrump\n2474507\n2461837\n12670\n-3074\n-954\n-917\n...\n0.214831\n2655\n2655\n0.0\n0.0\n0.364533\n{}\n4998566\n2474507\n2461837\n\n\n115\nGeorgia (EV: 16)\n2020-11-20 21:57:11.453\nBiden\nTrump\n2475227\n2462034\n13193\n-4028\n-770\n-532\n...\n0.855263\n2655\n2655\n0.0\n0.0\n0.349900\n{}\n4999520\n2475227\n2462034\n\n\n116\nGeorgia (EV: 16)\n2020-11-20 21:51:12.226\nBiden\nTrump\n2475304\n2462489\n12815\n-4798\n-1958\n-1851\n...\n0.518098\n2655\n2655\n0.0\n0.0\n0.356867\n{}\n5000290\n2475304\n2462489\n\n\n\n\n5 rows × 22 columns\n\n\n\nThe data contains a timestamp, the number of votes for each candidate, the margin, and an estimate of the number of votes remaining. Note that because the number of votes remaining is just an estimate, it can eventually become negative if the eventual number of ballots exceeds the estimate used to forecast the number of remaining votes. We can also have situations where the number of votes is corrected downwards, so we will need to take care that these do not trip up our model.\nWhen the data is captured in this way, we can see how the vote margin evolves over time as new ballots are counted. For example, we can look at the data for all states up to midnight on 5 November to see the evolution of the race:\n\nimport matplotlib.pyplot as plt\n\nstate_list = [\"Pennsylvania\", \"Georgia\", \"Arizona\"]\ntimestamp_list = [\"2020-11-05T00:00:00\"]*3\niter_vals = list(zip(state_list, timestamp_list))\n\ndef plot_data(state, timestamp=None):\n    \"Plot the election data for a given state up through a given time\"\n\n    df = load_data(state, timestamp)\n\n    plt.figure()\n    plt.plot(df[\"votes_remaining\"], df[\"vote_differential\"], \"o\")\n    plt.xlabel(\"Votes remaining\")\n    plt.ylabel(\"Biden margin\")\n    plt.title(\"{} Vote Updates through {}\".format(state, timestamp))\n\n\nfor (state, tstamp) in iter_vals:\n    plot_data(state, tstamp)\n\n\n\n\n\n\n\n\n\n\nNote that the trend shows that Biden is catching up as more votes are counted in both Georgia and Pennsylvania, while Trump is catching up in Arizona. The trend is fairly linear. Thus, one might first consider doing a simple regression to estimate the final margin."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#linear-regression-analysis",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#linear-regression-analysis",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Linear Regression Analysis",
    "text": "Linear Regression Analysis\nAs a first pass at building a model, I can do a simple regression analysis. A linear regression model will have two parameters that are fit: the slope will be related to the fraction of the outstanding votes that are for Biden, and the intercept, which will indicate the final margin when there are no votes remaining. (This is the initial analysis that was done by Camila for Pennsylvania and Martin for Arizona.)\n\nimport numpy as np\n\ndef linear_regression(state, timestamp=None):\n    \"\"\"\n    Fit a linear regression model to the election updates\n\n    Fits a line to the data updates for a given state and a given\n    timestamp. Plots the data and returns the fitting parameters\n    (slope, intercept) as a numpy array.\n    \"\"\"\n\n    plot_data(state, timestamp)\n\n    df = load_data(state, timestamp)\n\n    coeffs = np.polyfit(df[\"votes_remaining\"], df[\"vote_differential\"], 1)\n\n    plotvals = np.linspace(0, df[\"votes_remaining\"].iloc[-1])\n\n    plt.plot(plotvals, coeffs[0]*plotvals + coeffs[1])\n\n    return coeffs\n\n\nfor (state, tstamp) in iter_vals:\n    coeffs = linear_regression(state, tstamp)\n    print(\"Predicted margin for {} as of {}: {}\".format(state, tstamp, coeffs[1]))\n\nPredicted margin for Pennsylvania as of 2020-11-05T00:00:00: 139881.65460479335\nPredicted margin for Georgia as of 2020-11-05T00:00:00: 11362.338299788926\nPredicted margin for Arizona as of 2020-11-05T00:00:00: -87876.90143040325\n\n\n\n\n\n\n\n\n\n\n\nNote that at this point, the linear regression predicts a margin in Pennsylvania and Arizona that are quite different from the final margin. Georgia appears to be very close to the final margin. However, Arizona seems to have outlier points that muddles this analysis (which was first noted by Martin). Thus, while these models are useful starting points, they do not appear to be particularly robust and are somewhat dependent on when you choose to fit the data.\n\ndef get_margin(state, timestamp=None):\n    \"Extract margin for a state at a given time\"\n    \n    df = load_data(state, timestamp)\n    \n    return df[\"vote_differential\"].iloc[0]\n\n\nfor state in state_list:\n    print(\"Current margin in {}: {}\".format(state, get_margin(state)))\n\nCurrent margin in Pennsylvania: 81660\nCurrent margin in Georgia: 11779\nCurrent margin in Arizona: 10457\n\n\nHowever, one thing to note about this is that even though the trends point clearly in favor of Biden in this analysis, we do not have a good idea of the uncertainties. Without an uncertainty, we cannot robustly evaluate if the model is making good predictions. How might we develop a model that explicitly captures this uncertainty? And given such a model, when can we be confident that a candidate has won the state, and how does it align with the narrative from the news media? The following describes one approach for doing so."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#modelling-uncertainty-in-the-votes",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#modelling-uncertainty-in-the-votes",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Modelling Uncertainty in the Votes",
    "text": "Modelling Uncertainty in the Votes\nTo address this shortcoming, we turn to Bayesian Inference. Bayesian statisticians think of model parameters not as a single number, but rather probability distributions – in this way, we can get a sense of the range of values that the model thinks are consistent with the data.\n\nModel Structure\nAs noted above, the regression model has two different parameters: the slope (related to the fraction of votes that are cast for Biden), and the intercept (which is essentially the prediction of the final margin). Note that while the linear regression fit these two things simultaneously, there is no reason why I had to let the final margin be a “free” parameter that was adjusted in the fitting: I could have instead just fit a single parameter for the slope (for instance, simply using the fraction of mail ballots cast thus far for one of the candidates), and then used that estimate to project the votes remaining in order to extrapolate and obtain an estimate of the final margin.\nWe would like to adjust our model to account for uncertainty in both of these pieces of the model. The main source of uncertainty in the vote probability is related to the fact that not all voters are identical – the regression model assumes that this is the case, and the main challenge in building a more complex model is to relax this constraint while still ensuring that the model is simple enough that we can reliably fit it with the available data. For this, I will propose what is known as a hierarchical model, which is a common way of adding more complexity to a model in Bayesian inference. However, this is not the only way to do this, and there are certainly other methods based on Bayesian inference that would be able to account for this type of uncertainty.\nThere is also some uncertainty to account for in projecting the remaining votes, but it turns out that this is much smaller than the uncertainty in the vote probability. The way that I handle this type of uncertainty is a fairly standard problem in Bayesian inference, so I will focus most of my attention here on how to make a model that does not treat all voters identically, as this is the principal source of uncertainty. The following sections outline how to build such a model, fit its parameters, and then project the election outcome once it has been fit.\n\n\nBayesian Model of the Vote Probability\nBayesian inference tends to think of probability distributions as reflecting statements about our beliefs. Formally, I need to state my initial beliefs before I see any data, and then I can use that data to update my knowledge. This previous belief is known as a prior in Bayesian inference, and the updated beliefs once I look at the data is known as the posterior.\n\nBayesian Inference\nBayesian inference involves taking our previous beliefs about a system, described by a probability distribution of reasonable values we expect a particular parameter to take, and then using the data to update those beliefs about the distribution that we expect that parameter to take by computing the posterior. A key concept in Bayesian statistics is the idea of a conditional probability, written \\(p(A|B)\\), which means the probability of \\(A\\) given that we already know \\(B\\) (or conditioned on \\(B\\)). Inference allows us to update our beliefs (or in other words condition them on something we have observed) by applying Bayes’ rule:\n\\[ p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} \\]\nHere, \\(p(\\theta)\\) is the prior distribution (which we will specify before looking at the data), \\(p(y|\\theta)\\) is the likelihood (the probability that we would have gotten the data conditioned on a particular value of \\(\\theta\\)), and \\(p(y)\\) is known as the evidence (the probability of getting that particular observation over all possible outcomes of the experiment). Note that we have two standard probability distributions (the prior and the evidence), and two conditional probabilities (the likelihood and the posterior). The one we really care about is the posterior (our belief in the model parameters conditioned on the data, since the model parameters are uncertain but the data is known), but in practice it is much easier to compute the likelihood; Bayes’ rule tells us how these two are connected and makes these kind of computations more practical.\nIn practice, for most models one cannot compute the evidence very easily, so instead of computing the posterior directly, one draws samples from it. A common technique for this is Markov Chain Monte Carlo (MCMC) sampling. A number of software libraries have been written in recent years to make carrying out this sampling straightforward using what are known as probabilistic programming languages. These languages formally treat variables as probability distributions with priors and then draw samples from the posterior to allow the user to more easily perform inference.\nNote that because I will ultimately draw samples from the posterior, rather than compute the probability density function itself, most of the plots in this story will not have meaningful vertical axis scales, as the scale will depend on the number of samples that were drawn. Thus in most cases I simply omit labels on the vertical axes, as these are implicitly the number of samples in each case.\n\n\nA Hierarchical Bayesian Model For Voting\nIn the linear regression model, we effectively treat the vote probability as a single, unchanging value. This is the same as saying that every voter in our model is identical. Given the political polarization in the US, this is probably not a very good assumption. Although the data seems to strongly suggest that the mail-in votes are consistently in favor of one candidate, this is not the same as saying all voters are identical. In the following, I build a model to relax this assumption, using what is known as a hierarchical Bayesian model.\nIf the above model of assuming that every voter is identical is one extreme, then the other extreme is to assume that every voter is different and we would need to estimate hundreds of thousands to millions of parameters to fit our model. This is not exactly practical, so a hierarchical model posits a middle ground that the vote probability is itself drawn from a probability distribution. In this manner, we can use a probability distribution to describe the overall population of voters, and thus rather than fitting a parameter for every single voter, we just need to fit a few parameters to specify the distribution.\nThis structure is why hierarchical models are so powerful: probability distributions can be described with only a small number of parameters, so they are easy to fit, but we will see that it vastly expands the range of outcomes that the model is able to produce. This is in essence why hierarchical models can be so powerful – they guard against overfitting by using only a few parameters, but they can still produce a large variety of outcomes.\nIn the following, we model the vote probability by assuming that each vote update has a single vote probability associated with it, and that vote probability is drawn from a beta distribution. A beta distribution is a distribution defined over the interval \\([0,1]\\) with two shape parameters \\(a\\) and \\(b\\) that lets us flexibly specify a wide range of outcomes. If \\(a\\) and \\(b\\) are less than 1, then the distribution is biased towards the extreme values of 0 or 1, while if they are greater than 1 then the distribution is biased towards 0.5. If \\(a &gt; b\\), then the model is more biased towards 1, while if \\(b &gt; a\\) then the model is biased towards 0. Thus we can specify a huge range of distributions with just two parameters.\n\nfrom scipy.stats import beta\n\ndef plot_beta(a, b):\n    \"Plot the beta distribution for shape paramters (a, b)\"\n    \n    xvals = np.linspace(0, 1)\n\n    plt.plot(xvals, beta.pdf(xvals, a=a, b=b))\n    plt.xlabel(\"Biden vote probability\")\n    plt.title(\"PDF for the Beta distribution with a = {}, b = {}\".format(a, b))\n\n\nplot_beta(8., 4.)\n\n\n\n\nThus, instead of estimating the vote probability, I instead need to estimate \\(a\\) and \\(b\\), which will tell us what I expect the distribution of the vote probability to be. Having multiple levels of distributions described by distributions like this are what give hierarchical models their name – parameters are drawn from distributions, and this distribution must be described by some parameters. But because all parameters in Bayesian inference are probability distributions, these parameters are also distributions themselves, hence the model is hierarchical.\nSince all parameters in a Bayesian model must have priors, our task is now to encode our prior beliefs about the vote probability distribution by setting prior distributions for \\(a\\) and \\(b\\).\n\nPrior\nThe prior represents our previous beliefs about the value of the model parameters before we see any data, or in other words what we expect to be reasonable. If you are observing outcomes from flipping a coin, you might reasonably expect the probability of getting heads to be close to 0.5 in the absence of any data, and you might be unwilling to accept the idea of this probability of being 0 or 1. This is encoded in a probability distribution function, usually by using a particular distribution that is convenient given the constraints of the particular problem (for instance, the beta distribution described above is one that is often used to model priors where the parameter in question is a probability).\nOften, in Bayesian inference one does not have strong feelings about what values they might expect for a parameter. In those cases, we might prefer to use something simple, what is known as an uninformative prior. These might be expressed as a statement like “every value is equally probable”. Or in this case we might assume that the prior for the vote probability should be peaked close to 0.5, and then taper off towards 0 and 1, with the argument that US presidential elections are usually decided by a few percentage points difference in the national popular vote. This might seem very reasonable on the surface, as America is pretty evenly divided between Democrats and Republicans.\nHowever, mail in votes in practice can be extremely biased towards one party. Historically, a large majority of mail in ballots are Democratic, for a variety of reasons that are largely demographic. Trump also spent much of the campaign sowing doubt about mail-in ballots (telegraphing his post-election strategy of trying to throw them out in court), so his supporters may be much less likely to vote in this manner. However, there could also be a situation where the mail-in ballots fall heavily towards a Republican candidate (as we have seen already, more of the Arizona ballots tend to be in favor of Trump, which was due to late arriving ballots being from Republicans that registered to vote just before the election). Thus, based on this I would argue that what we actually want is a prior that is reasonably likely to include some extremes in the vote probability to ensure that our estimate of the final outcome prior to looking at the data doesn’t exclude a significant swing.\nThis issue illustrates a challenge with Bayesian Hierarchical models – when the parameter that I have some knowledge about is itself described by a distribution, the priors for the distribution parameters can be more difficult to specify. For this reason, modellers often go one level further and specify prior distributions on the parameters used to specify the priors on the model parameters, which are known as hyperpriors, and see how varying the priors changes the outcome of inference. I will not explore this level of Bayesian modelling, but in building this model I had to try a number of different choices for the priors before arriving at something that I thought accurately reflected my prior beliefs about the outcome.\nIn the priors that I finally settled on, I use a Lognormal distribution for my prior on \\(a\\) and \\(b\\). Lognormal distributions are frequently used as an alternative to a normal distribution in situations when the parameter in question must be positive (effectively, lognormal distributions can thought of as having the logarithm of the random variable in question following a normal distribution). I choose the parameters of the lognormal distributions for \\(a\\) and \\(b\\) to be slightly different such that the resulting distributions are more likely to lean democratically (as mail in votes are historically more democratic leaning), but still have a decent chance of producing extremes for Trump. I also choose the parameters such that I get a mix of values more biased towards the extremes as well as those biased towards values closer to 0.5.\nPriors like the ones used here are often referred to as subjective priors. Depending on who you ask, this is either a strength or a weakness in Bayesian inference. On one hand, the results are somewhat dependent on my parameter choices for my priors, but on the other hand, nearly every scientific study reflects some of the biases of the researchers that carry out the work. Subjective priors have the benefit of explicitly codifying the assumptions I made about what I expect to be reasonable outcomes, at the cost of some potential subjectivity in the final results. In this era of significant divides across party lines, exposing this part of the model as inherently subjective may also help ensure that the model parameters are not chosen in a way that explicitly favors one party over another.\nAs we will see, using this prior allows for the possibility that there is a decent chance based on historical data that the mail votes are heavily in favor of one candidate. The parameters I choose give a slight edge to the candidate from the Democratic party, but the prior includes a very reasonable chance that the swings will be towards the Republican candidate, which should help ensure that the model is not perceived to be biased against either candidate. Here are some histograms showing single samples of the vote probability drawn from this prior, and an aggregate histogram of 100 samples:\n\nfrom pymc3 import Lognormal\n\ndef plot_prior_samples(n_samples):\n    \"Plot a random draw of the vote probability from the prior\"\n\n    a = Lognormal.dist(mu=0.4, sd=0.5).random(size=n_samples)\n    b = Lognormal.dist(mu=0.2, sd=0.5).random(size=n_samples)\n\n    x = np.linspace(0., 1.)\n    \n    plt.figure()\n    plt.hist(beta.rvs(size=(1000, n_samples), a=a, b=b).flatten(), bins=100)\n    plt.xlabel(\"Biden vote probability\")\n    plt.title(\"Prior Vote Probability Distribution using {} samples\".format(i))\n\n\nfor i in [1, 1, 1, 100]:\n    plot_prior_samples(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these individual samples, as well as the aggregated histogram, we see that we get a range of outcomes, with a slight bias towards those that favor democrats. As we acquire enough data to reliably estimate the underlying distribution of the vote probabilites, we should see better estimates of the true distribution, which will eliminate more of the extremes and reduce the uncertainty in the final outcome.\n\n\nLikelihood\nFinally, we need to explicitly model the likelihood. The likelihood is how Bayesian models connect the parameters to the data – the likelihood tells us how probable the data is conditioned on a particular choice of parameters. As discussed earlier, this is actually the opposite of what we want, as we are interested in the parameters conditioned on the data. However, Bayes’ rule helps us compute the quantity we are interested in as long as we are able to to compute the likelihood.\nFor this particular problem, we can fortunately compute the likelihood easily in closed form. When one flips a fair coin a number of times, the distribution of outcomes follows a binomial distribution. Thus, I can use a binomial likelihood to model the range of vote probabilites that might be consistent with the votes that were cast. This can be computed analytically, and most probabilistic programming languages have built-in capacity for computing likelihoods of this type (so that we don’t have to worry about writing down the correct formula and implementing and testing the code to do this ourselves!). This is done automatically when using a probabilistic programming language by setting this particular variable to have a known value, which indicates to the probabilistic programming language that this variable is used to compute the likelihood. This means that I can focus on describing the model, rather than how to do the computation, which is one of the strengths of this type of modelling.\n\n\nPyMC3 Implementation\nThus, I can now write down a model in a probabilistic programming language in order to draw samples from the posterior. There are a number of popular lanaguages for this – here I use PyMC3 to implement my model. PyMC3 can easily handle all of the features I specified above (hierarchical structure, and a binomial likelihood), which is written out in the function below.\nIn coding up the model, I need to convert the election data into a series of “trials” consisting of a number of total votes cast and the corresponding number of votes for Biden. This is the way that the information needs to be captured to compute the binomial likelihood, which can then be used to draw samples from the posterior. Earlier, I noted that the data sometimes contains some inconsistencies (i.e. there are more votes cast for one candidate than the total number in that batch), so to protect against this I perform some checks for consistency and throw out any data that doesn’t make sense (in particular, I want to be sure the total number of votes is non-negative and the number of votes cast for Biden is smaller than the total number of ballots in this particular batch). I do this using a Numpy logical array in the extract_vote_trials function, as otherwise some inconsistent data can trip up the PyMC computations.\nOnce we have the trials defined, we can specify our model using PyMC3, perform inference, and plot the results:\n\nimport pymc3\n\nimport logging\nlogger = logging.getLogger(\"pymc3\")\nlogger.propagate = False\nlogger.setLevel(logging.ERROR)\n\ndef extract_vote_trials(state, timestamp=None):\n    \"\"\"\n    Convert vote data into a series of bernoulli trials. If no\n    data is valid (i.e. check that all numbers are nonzero and\n    the number of total ballots cast is larger than the number\n    of votes for, then return a list of a single zero for each.\n\n    Returns two lists of positive integers for the total number\n    of votes and the votes for Biden\n    \"\"\"\n\n    df = load_data(state, timestamp)\n\n    total_votes = np.diff(-df[\"biden_votes\"]) + np.diff(-df[\"trump_votes\"])\n    biden_votes = np.diff(-df[\"biden_votes\"])\n\n    # throw out combinations that don't make any sense using numpy\n    # logical arrays -- note that multiplication of logical arrays\n    # is equivalent to a logical \"and\";\n    # if none remain then just return (0,0)\n\n    trials_to_keep = ((total_votes &gt; 0)*(biden_votes &gt;= 0)*\n                      (biden_votes &lt;= total_votes))\n    total_votes = total_votes[trials_to_keep]\n    biden_votes = biden_votes[trials_to_keep]\n\n    if len(total_votes) == 0:\n       total_votes = [0]\n       biden_votes = [0]\n\n    return (np.array(total_votes, dtype=np.int64),\n            np.array(biden_votes, dtype=np.int64))\n\n\ndef estimate_theta_hierarchical(state, timestamp=None):\n    \"Estimate the vote probability distribution using a hierarchical model and MCMC sampling\"\n\n    total_votes, biden_votes = extract_vote_trials(state, timestamp)\n    num_trials = len(total_votes)\n\n    # build model and draw MCMC samples\n\n    with pymc3.Model() as model:\n        a = pymc3.Lognormal(\"a\", mu=0.4, sd=0.5)\n        b = pymc3.Lognormal(\"b\", mu=0.2, sd=0.5)\n        theta = pymc3.Beta(\"theta\", alpha=a, beta=b, shape=num_trials) \n        obs = pymc3.Binomial(\"obs\", p=theta, n=total_votes,\n                             observed=biden_votes, shape=num_trials)\n        trace = pymc3.sample(1000, progressbar=False,\n                             return_inferencedata=False)\n\n    return trace\n\n\ndef plot_posterior(state, timestamp):\n    \"Plot the posterior distribution of the vote probability\"\n\n    trace = estimate_theta_hierarchical(state, timestamp)\n\n    rvs_size = (100, len(trace[\"a\"]))\n    \n    plt.figure()\n    plt.hist(beta.rvs(size=rvs_size,\n                      a=np.broadcast_to(trace[\"a\"], rvs_size),\n                      b=np.broadcast_to(trace[\"b\"], rvs_size)).flatten(),\n                      bins=100)\n    plt.xlabel(\"Biden vote probability\")\n    plt.title(\"{} Vote Probability Posterior as of {}\".format(state, timestamp))\n\n\nfor (state, tstamp) in iter_vals:\n    plot_posterior(state, tstamp)\n\n\n\n\n\n\n\n\n\n\nOnce I draw MCMC samples for \\(a\\) and \\(b\\), I convert those samples into samples of \\(\\theta\\) to see our posterior estimate of the vote probability.\nLooking at these plots, I see that the model is now much more varied in its estimates for the vote probability (note that this is the posterior for the distribution expected for the vote probability, rather than the explicit values of the vote probability itself). The mean is still where I expect it based on the linear regression analysis, but the distribution is much wider due the fact that occasionally votes come in from places that are not as heavily in favor of Biden (or Trump in the case of Arizona). This wider distribution of the vote probability is how I quantify the larger uncertainty in the election outcome. Using this distribution to forecast the remaining votes should considerably increase the spread of the predicted final margin and assure that it is not overconfident in the final result.\n\n\n\n\nPredicting the Final Margin\nOnce I have samples from the vote probability, I need to simulate the remaining votes to predict the final outcome. This is known as estimating the posterior predictive distribution in Bayesian inference, when one uses the updated knowledge about one of the model parameters to predict something that was not used to fit the data.\nWhat is a reasonable way to simulate the remaining votes? As one can see from the data, the votes come in a steady drip feed as ballots are counted. Thus, I can simulate this by sampling randomly, with replacement, from the data for the number of ballots cast in each update until I get to the number of votes remaining. I can then use the posterior samples of \\(a\\) and \\(b\\) to generate a distribution of vote probabilities, and then draw from the vote probabilites to forecast the outcome of each batch of votes using a binomial distribution. I repeat this process 10 times to ensure that the result isn’t dependent on the particular realization of the drip feed simulation, and aggregate those samples to get the final estimate of the posterior predictive distribution. This should give a reasonable estimate of the final outcome based on the model.\n\nfrom scipy.stats import binom\n\ndef get_votes_remaining(state, timestamp=None):\n    \"Extract remaining votes for a state at a given timestamp\"\n    \n    df = load_data(state, timestamp)\n\n    return df[\"votes_remaining\"].iloc[0]\n\n\ndef draw_random_vote_updates(state, timestamp):\n    \"Draw a random set of simulated vote updates for the remaining votes\"\n\n    n_remain = get_votes_remaining(state, timestamp)\n\n    n, k = extract_vote_trials(state, timestamp)\n    \n    if np.all(n == 0):\n        n = np.array([1000], dtype=np.int64)\n\n    simulated_vote_updates = []\n        \n    while np.sum(simulated_vote_updates) &lt;= n_remain:\n        simulated_vote_updates.append(np.random.choice(n))\n            \n    simulated_vote_updates[-1] = n_remain - np.sum(simulated_vote_updates[:-1])\n    assert np.sum(simulated_vote_updates) == n_remain\n    \n    return np.array(simulated_vote_updates, dtype=np.int64)\n\n\ndef project_remaining_votes(trace, simulated_vote_updates):\n    \"Project the remaining votes using MCMC samples of the vote probability distribution parameters\"\n\n    assert np.all(trace[\"a\"] &gt;= 0.)\n    assert np.all(trace[\"b\"] &gt;= 0.)\n\n    rvs_size = (len(trace[\"a\"]), len(simulated_vote_updates))\n\n    return np.sum(binom.rvs(size=rvs_size,\n                            p=beta.rvs(size=rvs_size,\n                                       a=np.broadcast_to(trace[\"a\"][:, np.newaxis], rvs_size),\n                                       b=np.broadcast_to(trace[\"b\"][:, np.newaxis], rvs_size)),\n                            n=np.broadcast_to(simulated_vote_updates, rvs_size)), axis=-1)\n\n\ndef predict_final_margin(trace, state, timestamp=None):\n    \"\"\"\n    Use posterior samples of the vote probability to predict the remaining\n    votes.\n\n    The remaining votes are split into batches by sampling from\n    the previous votes until enough are accumulated. Then each batch is\n    forecast using the posterior samples, and the total is summed.\n\n    Returns a numpy array of samples of the final margin\n    \"\"\"\n\n    # simulate remaining votes\n\n    n_trials = 10\n\n    predicted_margin = np.zeros((n_trials, len(trace[\"a\"])))\n\n    for i in range(n_trials):\n\n        simulated_vote_updates = draw_random_vote_updates(state, timestamp)\n    \n        predicted_margin[i] = project_remaining_votes(trace, simulated_vote_updates)\n\n    n_remain = get_votes_remaining(state, timestamp)\n    margin = get_margin(state, timestamp)\n\n    return margin - n_remain + 2*predicted_margin.flatten()\n\n\ndef plot_predictions(state, timestamp):\n    \"Plot the posterior predictive distribution for the given state and time\"\n\n    trace = estimate_theta_hierarchical(state, timestamp)\n    predicted_margin = predict_final_margin(trace, state, timestamp)\n    \n    plt.figure()\n    plt.hist(predicted_margin, bins=100)\n    plt.xlabel(\"Biden Margin\")\n    plt.title(\"{} final predicted margin as of {}\".format(state, timestamp))\n    \nfor (state, tstamp) in iter_vals:\n    plot_predictions(state, tstamp)\n\n\n\n\n\n\n\n\n\n\nAs we can see from this, the model has fairly wide intervals surrounding the predicted final margin based on the original linear regression model. Interestingly, when I fit Georgia in this way, it looks much more likely that Trump would win through this point than the linear regression model would suggest, though the final margin found by the regression analysis is well within the error bounds suggested from the predictions. Arizona looks up for grabs, indicating that the outlier points were definitely biasing the regression analysis. Pennsylvania is much more firmly leaning towards Biden. We can look at the results again a day later to see how the race evolved:\n\nfor (state, tstamp) in zip(state_list, [\"2020-11-06T00:00:00\"]*3):\n    plot_predictions(state, tstamp)\n\n\n\n\n\n\n\n\n\n\nClearly, Georgia has swung in Biden’s favor over the course of the day. The mean final margin in Pennsylvania has not moved much, though the uncertainty has tightened up and made the result more likely for Biden. Arizona could still go either way."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#animating-the-updates",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#animating-the-updates",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Animating the Updates",
    "text": "Animating the Updates\nNow that I have built a model, I can build an animation that shows the evolution of the predicted results as a function of time. This will show how the uncertainty shrinks over time as fewer votes remain. I check for results every 30 minutes for the 12 days from 4 November onward, and update the model when new ballots are found. I also compute a Biden win probability and show the mean margin \\(\\pm\\) 2 standard deviations to give an idea of the equivalent regression result and its uncertainty. Building the animation requires some more clever manipulation of Matplotlib objects, so I will not go into detail to describe exactly what the plotting code is doing here. It is based on the code in this example, so please look at that example for a description of how to animate histograms.\nNote: Because new MCMC samples need to be drawn for each new update, creating this animation ends up being fairly expensive to run (this took several hours on my laptop). I speed things up by saving the current prediction each time the MCMC samples are drawn, so that if the previous iteration is the same we do not need to re-run the model. However, this is still fairly expensive, so don’t try and run this unless you are willing to wait!\n\n%%capture\n\nimport matplotlib.path as path\nimport matplotlib.patches as patches\nimport matplotlib.text as text\nimport matplotlib.animation as animation\n\ndef load_previous_model(filename, initialize=False):\n    \"\"\"\n    Load the previous model from file\n\n    Load results from the previous simulation from disk,\n    including the total votes cast, votes for Biden,\n    number of votes remaining, and the previous set of\n    samples from the predictive distribution.\n    If the file cannot be loaded, or if we pass\n    `initialize=True`, returns `None` for all values.\n    \"\"\"\n\n    total_votes = None\n    biden_votes = None\n    n_remain = None\n    preds = None\n    \n    if not initialize:\n        try:\n            model = np.load(filename)\n            total_votes = model[\"total_votes\"]\n            biden_votes = model[\"biden_votes\"]\n            n_remain = int(model[\"n_remain\"])\n            preds = model[\"preds\"]\n        except (KeyError, IOError):\n            total_votes = None\n            biden_votes = None\n            n_remain = None\n            preds = None\n\n    return total_votes, biden_votes, n_remain, preds\n\n\ndef fit_model(state, timestamp=None, initialize=False):\n    \"\"\"\n    Fit a model to predict the final margin for the given date/time.\n    Each iteration is saved as a numpy file, and the next step\n    checks for a model that matches the existing vote count before\n    doing the expensive MCMC fitting\n\n    Returns the simulated final margin samples at the given time\n    \"\"\"\n\n    filename = \"model.npz\"\n\n    total_votes_prev, biden_votes_prev, n_remain_prev, preds_prev = \\\n        load_previous_model(filename, initialize)\n\n    total_votes, biden_votes = extract_vote_trials(state, timestamp)\n\n    n_remain = get_votes_remaining(state, timestamp)\n    \n    if (np.array_equal(total_votes_prev, total_votes) and\n        np.array_equal(biden_votes_prev, biden_votes) and\n        n_remain_prev == n_remain):\n        return preds_prev\n    else:\n        theta = estimate_theta_hierarchical(state, timestamp)\n        preds =  predict_final_margin(theta, state, timestamp)\n        np.savez(filename,\n                 total_votes=total_votes, biden_votes=biden_votes,\n                 preds=preds, n_remain=n_remain)\n        return preds\n\n\ndef initialize_verts(bins):\n    \"Initialize the patch corners for the animation\"\n\n    # get the corners of the rectangles for the histogram\n    left = bins[:-1]\n    right = bins[1:]\n    vals = np.zeros(len(left))\n    nrects = len(left)\n\n    nverts = nrects * (1 + 3 + 1)\n    verts = np.zeros((nverts, 2))\n    codes = np.full(nverts, path.Path.LINETO)\n    codes[0::5] = path.Path.MOVETO\n    codes[4::5] = path.Path.CLOSEPOLY\n    verts[0::5, 0] = left\n    verts[0::5, 1] = vals\n    verts[1::5, 0] = left\n    verts[1::5, 1] = vals\n    verts[2::5, 0] = right\n    verts[2::5, 1] = vals\n    verts[3::5, 0] = right\n    verts[3::5, 1] = vals\n\n    return verts, codes\n\n\ndef update_verts(preds, bins, verts):\n    \"Update the verticies on the histogram patches for animation\"\n\n    n, bins = np.histogram(preds, bins)\n    verts[1::5, 1] = n\n    verts[2::5, 1] = n\n\n    return verts\n\n\ndef animate(i, state, start_time, bins_t, verts_t, patch_t, bins_b, verts_b, patch_b,\n            date_text, vote_text, mean_text, prob_text):\n    \"Updates the histogram patches and text to make the animated histogram plot\"\n        \n    hours = i//2\n    minutes = 30*i % 60\n        \n    timestamp = ((datetime.datetime.strptime(start_time, \"%Y-%m-%dT%H:%M:%S\") +\n                  datetime.timedelta(hours=hours, minutes=minutes)).strftime(\"%Y-%m-%dT%H:%M:%S\"))\n\n    if i == 0:\n        preds = fit_model(state, timestamp, initialize=True)\n    else:\n        preds = fit_model(state, timestamp)\n\n    verts_t = update_verts(preds, bins_t, verts_t)\n    verts_b = update_verts(preds, bins_b, verts_b)\n\n    date_text.set_text(datetime.datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S\").strftime(\"%Y-%m-%d %H:%M\"))\n\n    n_remain = get_votes_remaining(state, timestamp)\n    vote_text.set_text(\"{} Votes Remaining\".format(str(n_remain)))\n\n    mean_text.set_text(\"Margin = {:&gt;8} $\\pm$ {:&gt;7}\".format(int(np.mean(preds)),\n                                                           int(2.*np.std(preds))))\n\n    prob_text.set_text(\"Biden win prob = {:.2f}\".format(np.sum(preds &gt; 0)/len(preds)))\n        \n    return [patch_t, patch_b, date_text, vote_text, mean_text, prob_text]\n\n\ndef create_animation(state):\n    \"Create an animation of the vote updates for the given state\"\n\n    start_time = \"2020-11-04T14:00:00\"\n    \n    xlim = 100000\n    ylim = 500\n    nbins = 200\n    binsize = xlim//nbins\n\n    bins_b = np.linspace(0, xlim, binsize, dtype=np.int64)\n    bins_t = np.linspace(-xlim, 0, binsize, dtype=np.int64)\n    \n    verts_t, codes_t = initialize_verts(bins_t)\n    verts_b, codes_b = initialize_verts(bins_b)\n\n    patch_t = None\n    patch_b = None\n\n    fig, ax = plt.subplots()\n    barpath_t = path.Path(verts_t, codes_t)\n    patch_t = patches.PathPatch(barpath_t, facecolor='C3',\n                                edgecolor='C3', alpha=0.5)\n    ax.add_patch(patch_t)\n    barpath_b = path.Path(verts_b, codes_b)\n    patch_b = patches.PathPatch(barpath_b, facecolor='C0',\n                                edgecolor='C0', alpha=0.5)\n    ax.add_patch(patch_b)\n\n    lefttext = -9*xlim//10\n    righttext = 9*xlim//10\n    uppertext = 9*ylim//10\n    lowertext = 8*ylim//10\n\n    date_text = text.Text(lefttext, uppertext, \"\")\n    ax.add_artist(date_text)\n\n    vote_text = text.Text(lefttext, lowertext, \"\")\n    ax.add_artist(vote_text)\n\n    prob_text = text.Text(righttext, uppertext, \"\", ha='right')\n    ax.add_artist(prob_text)\n\n    mean_text = text.Text(righttext, lowertext, \"\", ha='right')\n    ax.add_artist(mean_text)\n\n    ax.set_xlim(-xlim, xlim)\n    ax.set_ylim(0, ylim)\n    ax.set_xlabel(\"Biden margin\")\n    ax.set_title(\"{} Final Margin Prediction\".format(state))\n    \n    ani = animation.FuncAnimation(fig, animate, frames=2*24*12, interval=200,\n                                  fargs=(state, start_time,\n                                         bins_t, verts_t, patch_t,\n                                         bins_b, verts_b, patch_b,\n                                         date_text, vote_text, mean_text, prob_text),\n                                  repeat=False, blit=True)\n    \n    return ani\n\n\nani_pa = create_animation(\"Pennsylvania\")\nani_ga = create_animation(\"Georgia\")\nani_az = create_animation(\"Arizona\")\n\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n\n\nDisplaying these, we can see how the race evolves over time.\n\nfrom IPython.display import HTML\n\nHTML(ani_pa.to_jshtml())\n\nThere were 4 divergences after tuning. Increase `target_accept` or reparameterize.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nHTML(ani_ga.to_jshtml())\n\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nHTML(ani_az.to_jshtml())\n\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nBased on this model, we can see that Pennsylvania was very clearly going in Biden’s direction from early on, despite Trump’s substantial lead at the end of Election Day. This was reflected by comments made by other election data journalists, all of whom were fairly confident that the numbers were good news for Biden even as early as 4 November. Biden’s win probability steadily increased, surpassing 99% on 6 November. The media called the state, and the election, for Biden on 7 November.\nGeorgia, on the other hand, was not a sure thing. For much of the early data, our model favored Trump, who had a win probability of 82% on the evening of 4 November. However, the uncertainties were wide enough at that point that Biden’s eventual victory was still not an unreasonable outcome. As the ballots shifted towards Biden, we can see a clear change on 5 November, and by that evening Biden’s win probability was 70%. Biden’s chances steadily increased and surpassed 99% on the evening of 7 November. However, since the final margin was still fairly small in absolute terms, the media did not call Georgia until 12 or 13 November.\nArizona, despite being the first state among these that many news outlets called, showed the largest uncertainties for much of the time period we have data, with no candidate having a clear advantage until 9 November when Biden took a slight lead in the model predictions. From there, Biden inched ahead as the remaining ballots came in, and the outcome shifted clearly in his favor on 12 November with his win probability exceeding 99% that evening. The remaining media outlets called Arizona for Biden on 13 November.\nAs we can see, our model is able to call the outcome of these states slightly before the media does so (possibly due to some level of conservatism). Seeing the range of uncertainties shrink is helpful to know what range of outcomes could still be reasonably expected, and can be a much more interesting way to visualize the results (particularly when animated as above)."
  },
  {
    "objectID": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#conclusion",
    "href": "stories/2021-02-26-US-election/2021-02-26-Modelling-Mail-In-Votes-In-the-2020-US-Election.html#conclusion",
    "title": "Modelling Mail-In Votes In the 2020 US Election",
    "section": "Conclusion",
    "text": "Conclusion\nThis Data Story examined how we could build a Bayesian hierarchical model for the US election data and use it to forecast the final outcome. The model showed how the outcome in three key battleground states evolved over the week following the election as mail ballots were counted, tipping the election in favor of Biden. Because the model includes uncertainties and prior beliefs about voting behavior, this gave a richer picture of how to forecast the final result than simply extrapolating using early returns (with considerable more thought and effort required, however!). Because of the time scales over which the election played out, we could imagine having put this in place to make prospective predictions (stay tuned for 2024!) in real time to see how this simple model aligns with more experienced election forecasters."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "",
    "text": "Jack Roberts, Alan Turing Institute (Github: @jack89roberts)\nNick Barlow, Alan Turing Institute (Github: @nbarlowATI)"
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#introduction",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#introduction",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Introduction",
    "text": "Introduction\nI was recently chatting with a friend about the respective merits of the French pastries available at our local bakery:\nFriend: What do you think of the pastries? Are they good?\nMe: Yes, though there is another bakery further out of town that I prefer.\nFriend: You don’t think these are as good?\nMe: No, but not so much worse that I’d walk a half hour each way to get those since I don’t have a car. They are definitely better than the ones you get at Pret A Manger which you can find in every train station and city center, so definitely well above a replacement level crossaint.\nFriend: What do you mean by that, replacement level?\nMe: Oh, it’s a term that baseball statisticians use to measure if it’s worth paying money for a particular player. The idea is to come up with some baseline that’s easily available, and then try to determine how much better something is than that. Economists call this “Marginal Utility.”\nFriend: [Silence]\nMe: [To myself] Did I really just say “replacement level crossaint?”\nWhile I took a course in Economics when I was an undergraduate, I definitely did not completely understand the concept of Marginal Utility until I started reading more about baseball statistics (no doubt beginning with my dog-eared copy of Moneyball by Michael Lewis). The idea is that to compare two goods that have different prices, I need to determine how much additional value I get out of the more expensive one to justify the extra expense. Regarding crossaints, the extra cost of the better bakery was my time, which I determined would be better spent doing something else and thus the marginal utility of the fancier bakery was not worth it.\nThe undergraduate version of me certainly nodded when the professor talked about this in class, as it sounds simple enough. But what I failed to appreciate back then is the idea that to really understand value in some absolute sense, we need to compare these things to some baseline level that I can always acquire without any effort and minimal expense, something that the baseball statisticians were able to very clearly articulate. In terms of pastries, this was the fare available at the Pret just around the corner – always available for 2 pounds no matter where I am without having to walk a long way, always the same product as they are no doubt churned out en masse at some factory bakery."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#replacement-level",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#replacement-level",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Replacement Level",
    "text": "Replacement Level\nIn baseball, this baseline level is referred to as “replacement level.” Conceptually, replacement level does not refer to any player in particular, but rather some theoretical player that any team can acquire at any time with minimal effort (either time or money). We can think of these players as being effectively in infinite supply (by infinite, what we really mean is that “there are far more players of this caliber than there are jobs available on Major League Baseball teams”). If we put a bunch of these theoretical players on the field and played 162 games (a full MLB season), they would not be a particularly good team, but they would be cheap. To win more games than this theoretical team would win, I need to spend some additional time or money in finding players that are better than this replacement level.\nOne thing to note is that this level is not the average of all players in the league – presumably all of the teams have made considerable effort to spend resources to make their team better, so the league average will reflect some amount of value above this level. How much more valuable is an average player than a replacement level? Most statisticians put replacement level at around 80% of the league average (see for instance the Wikipedia page).\nOn face, this all seems quite sensible. But recently, I asked myself: why is this a reasonable level? Why 80%? Why not 25%, or 95%? And can we be more precise about why 80% represents this “easily findable value” and what it really means in a statistical sense? This is one of the main criticisms of the statistical effort to estimate replacement value. In the following analysis, I present one way to try and make this estimate more concrete and not include any arbitrary numbers in it."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#a-primer-on-baseball",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#a-primer-on-baseball",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "A Primer on Baseball",
    "text": "A Primer on Baseball\nGiven that baseball is more popular in some parts of the world than others, I give a brief description of the rules of the game in this section. If you are already familiar with the sport, feel free to skip ahead. If the summary in this section is too brief, the Wikipedia page contains more details.\nBaseball is a bat and ball game where teams take alternate turns attempting to hit a ball thrown by the defensive team. The field of play is roughly diamond shaped, with four bases placed in a counterclockwise direction around a square:\n\n\n\nBaseball Diamond\n\n\nThe hitters start from “home plate” at the base of the diamond (not labelled on the above image, but it is between the batter’s and catcher’s boxes), then first, second, and third bases proceed around the square. The objective of the offensive team is to hit the ball, thrown from the pitcher’s mound at the center of the bases towards home plate, into the field of play between the foul lines in a way that the defensive players are (1) unable to catch the ball before it hits the ground and (2) not able to retrieve the ball and throw it to first base before the offensive player is able to run from home plate to first base. If the offensive player is not successful, then an “out” is recorded, while if the player is able to hit the ball safely, then they are able to remain on the bases and the next member of the team can attempt to hit the ball.\nHowever, the offensive player may not always be able to hit the ball. If the pitcher can successfully cause the hitter to miss the ball three times (called “strikes”), the offensive player makes an out, while if the pitcher throws four pitches that are deemed not to be hittable (called “balls”), then the hitter does not make an out and is awarded first base (a “walk” or “base on balls”, abbreviated BB). Alternatively, if the pitcher throws the ball and it hits the body of the batter (hit by pitch, abbreviated HBP), then that player is awarded first base in the same way as a walk. All attempts by an offensive player thus either result in an out, or the player successfully reaches the bases and the offensive team is able to continue hitting.\nIf the offensive team is able to advance a player completely around the bases (first, second, third, and then back to home plate), then a point is scored (also called a “run”). If the defensive team is able to record three outs, then the offensive team is out of chances to score and the defensive and offensive teams switch places. Each successive series of attempts is known as an “inning,” and a game consists of 9 innings.\nDifferent types of hits are able to advance runners more bases, so players that are able to do this more often are inherently more valuable. Hits that go for 2 bases are known as “doubles” (2B), 3 bases are known as “triples” (3B) and hits that leave the field of play are known as “home runs” (HR) and all players on the bases are able to advance to home plate. Additionally, one base hits (“singles”, or 1B) tend to advance existing runners multiple bases, as they are often able to get a running start, while walks only allow players to advance if the runner at the previous base is forced to advance. However, the analysis below will ignore the value of baserunning, and instead focus on the above types of hits.\nFinally, some types of outs are traditionally treated differently in baseball statistics, as they have a positive effect in that they are able to advance a runner along the bases despite the negative value associated with making an out. These are known as sacrifice hits (abbreviated SH or SF for sacrifice flies), and are usually not counted towards the number of attempts a player is credited with when looking at hitting statistics. Additionally, for historical reasons walks are not counted towards the number of attempts a player is credited with either. Because of these quirks, the number of official “at bats” (abbreviated AB) will be different from the number of “plate appearances” (abbreviated PA). However, since walks create positive value, while sacrifice hits still make an out, we will care more about the number of plate appearances rather than the more common at bats."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#data",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#data",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Data",
    "text": "Data\nIn this story, I use the publically available and widely used Lahman Baseball Database. This database is known for its full coverage of most of MLB history, and includes all of the standard batting and pitching statistics for all players for all seasons. It is maintained by a group of volunteers who tabulate statistics and have gone through past newspaper archives to collect the extensive data that it contains. It is available in a number of SQL formats, as well as the raw CSV files, and is updated every year.\nIn the following, I will use the SQLite version of the database that runs through the 2019 season (it can be found here). I choose this mostly because Python has built-in support for SQLite, and since I will be using this database as a standalone data source, I don’t have to worry about setting up a server, dealing with concurrent users, or modifying the database.\nThe tables of the Lahman database are described in the documentation. I will mainly be using the batting table, which contains the relevant hitting statistics and can easily be queried to pull out the specific data that I need to perform this analysis. As a reference, here is a table of the abbreviations used in the database and our data frame:\n\n\n\nSymbol\nDescription\nHow to Calculate\n\n\n\n\nAB\nAt Bats\n\n\n\nH\nHits\n\n\n\n2B\nDoubles\n\n\n\n3B\nTriples\n\n\n\nHR\nHome Runs\n\n\n\nBB\nWalks\n\n\n\nIBB\nIntentional Walks\n\n\n\nHBP\nHit By Pitch\n\n\n\nSH\nSacrifice Hits\n\n\n\nSF\nSacrifice Flies\n\n\n\nPA\nPlate Appearances\nAB + BB + IBB + HBP + SH + SF\n\n\nO\nOuts\nAB + SH + SF - H\n\n\n1B\nSingles\nH - 2B - 3B - HR\n\n\n\nNote there are a few statistics we need that are derived from the other statistics: * Plate Appearances (PA) are all at bats, plus all special cases (i.e. walks and sacrifices) where the player came up but did not result in an at bat. * Outs are the number at bats plus all sacrifices (i.e. outs that are not counted in at bats), minus the number of hits. * Singles (1B) are the number of hits minus all hits resulting on multiple bases (doubles, triples, and home runs).\n\n%matplotlib inline\n\n\nimport pandas\nimport numpy as np\nimport sqlite3\nimport scipy.stats\nimport matplotlib.pyplot as plt\n\nconn = sqlite3.connect(\"lahmansbaseballdb.sqlite\")\nyear = 2019\n\nOnce I have connected to the database, I can query the batting table to get the full statistics for all hitters that registered at least 10 at bats in the most recent full season (2019) and load into a data frame using pandas:\n\ndf = pandas.read_sql('select * from batting where yearID = {} and AB &gt; 10'.format(year), conn)\ndf.head()\n\n\n\n\n\n\n\n\nID\nplayerID\nyearID\nstint\nteamID\nteam_ID\nlgID\nG\nG_batting\nAB\n...\nRBI\nSB\nCS\nBB\nSO\nIBB\nHBP\nSH\nSF\nGIDP\n\n\n\n\n0\n105864\nabreujo02\n2019\n1\nCHA\n2900\nAL\n159\nNone\n634\n...\n123\n2\n2\n36\n152\n4\n13\n0\n10\n24\n\n\n1\n105865\nacunaro01\n2019\n1\nATL\n2897\nNL\n156\nNone\n626\n...\n101\n37\n9\n76\n188\n4\n9\n0\n1\n8\n\n\n2\n105866\nadamecr01\n2019\n1\nSFN\n2920\nNL\n10\nNone\n22\n...\n2\n0\n0\n2\n8\n0\n0\n0\n0\n0\n\n\n3\n105867\nadamewi01\n2019\n1\nTBA\n2922\nAL\n152\nNone\n531\n...\n52\n4\n2\n46\n153\n1\n3\n3\n1\n9\n\n\n4\n105874\nadamsma01\n2019\n1\nWAS\n2925\nNL\n111\nNone\n310\n...\n56\n0\n0\n20\n115\n1\n2\n0\n1\n7\n\n\n\n\n5 rows × 25 columns\n\n\n\nThis only gives us the raw numbers for these players, and the player is identified with a playerID rather than their full name. If I want to get the names, I need to do a join with the people table:\n\ndf = pandas.read_sql('''select nameLast, nameFirst, AB, H, \"2B\", \"3B\", HR, BB\n                        from batting inner join people on batting.playerID = people.playerID\n                        where yearID = {} and AB &gt; 10'''.format(year), conn)\ndf.head()\n\n\n\n\n\n\n\n\nnameLast\nnameFirst\nAB\nH\n2B\n3B\nHR\nBB\n\n\n\n\n0\nAbreu\nJose\n634\n180\n38\n1\n33\n36\n\n\n1\nAcuna\nRonald\n626\n175\n22\n2\n41\n76\n\n\n2\nAdames\nCristhian\n22\n7\n1\n0\n0\n2\n\n\n3\nAdames\nWilly\n531\n135\n25\n1\n20\n46\n\n\n4\nAdams\nMatt\n310\n70\n14\n0\n20\n20\n\n\n\n\n\n\n\nWhile it is useful to be able to identify particular players, I am most interested the distribution of outcomes over the entire league, I don’t particularly care which players are which. Thus, most of the time I will just be doing the same query of the batting table to pull out hitting statistics.\nHowever, one non-trivial thing to consider is that players that play different defensive positions have different expectations associated with their batting ability. The most important correction by far is the fact that pitchers tend to be significantly worse hitters than players that play other defensive positions (colloquially referred to as “position players”), though this also applies to other defensive positions to a lesser extent. Pitchers tend to be worse hitters than average, in large part because they get less regular at bats due to the fact that they only pitch every few days. To make a fair comparison, I would thus like to look only at position players, so for the query that I need I will need to do a join on the appearances table. This table lists how many games a player appeared as a pitcher, so I can filter out all players that appeared in a game as a pitcher to exclude them from the analysis."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#runs-created",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#runs-created",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Runs Created",
    "text": "Runs Created\nAt a basic level, baseball comes down to two things: runs and outs. Outs occur when a player fails to get a hit. Each game consists of 27 outs per team, so the goal is to not make outs, and instead produce a positive outcome. If a player does not make an out, then there are several possible outcomes: the player can safely hit the ball somewhere, which allows the player and the player’s teammates to advance around the bases a certain amount, or the player can be awarded a base without getting a hit (either through a walk or a hit by pitch). Hits can be singles, doubles, triples, or home runs which advance the batter 1, 2, 3, or 4 bases, respectively (n.b. hits tend to advance any teammates already on the bases a different number of bases, but it turns out that the metric below does a reasonable job of accounting for this as it turns out that a double is about twice as valuable as a single, a triple is three times as valuable, etc.).\nWhile the main goal is to not make outs, hitters that are more likely to advance players by more than one base at a time are more likely to create more runs, as if they have other players already on the bases, they are more likely to score runs. The simplest way that we can account for this is through a metric known as “Runs Created,” which was designed by the baseball statistician Bill James and tries to approximate the number of runs that a player is likely to have produced through their hitting ability. The formula is\n\\[RC = {\\rm Success\\: rate}\\times{\\rm Total\\: bases} = \\frac{H+BB}{PA}\\times TB\\]\nHere, \\(H\\) is the number of hits, \\(BB\\) is the number of walks, \\(PA\\) is the number of attempts, and \\(TB\\) is the total number of bases the player advanced through their hits. Note that this is a rate multiplied by a counting stat, ultimately resulting in a counting stat. Note however that the added rate factor means that if two players produce the same number of total bases, but one does it in fewer opportunities or draws more walks, the metric will give more value to the player with the better rate stat.\nThis statistic was derived mostly on an empirical basis, but for some intution as to why it works imagine that someone gets 4 hits in 16 opportunities, of which two are singles, one is a double, and one is a home run. Then the success rate is 1/4, while the total number of bases are eight, leading to two runs created. Obviously, the home run must have created one run. Thus, the combined value of all the other hits (plus the chances that someone else was on the bases when they hit the home run) is such that statistically, this player’s production increased the number of runs the team scored over the entire season by one. Note that it does not mean that those hits produced one actual run, but rather that given a typical set of scenarios under which those hits took place, the player’s team likely scored one additional run due to those hits.\nI can pull out the basic statistics that I need to compute Runs Created with the following SQL query, which I will wrap in a function for convenient further use. As mentioned above, to exclude pitchers I need to do an inner join on the appearances table, which allows me to filter out the pitchers and just focus on position players in this analysis.\nI generate 7 columns in this query: the total number of attempts (PA or “Plate Appearances”), number of outs (O), and then 5 additional columns for the types of hits/walks so that I can convert these to the total number of bases. Some statistics, such as sacrifice flies (SF) and intentional walks (IBB) are not recorded for all years, so I use the SQL coalesce function to convert the null values to zero if necessary.\n\ndef rc_base_query(year, conn):\n    \"Base query for getting statistics for computing Runs Created\"\n    \n    return pandas.read_sql('''select AB + BB + coalesce(IBB,0) + HBP + SH + coalesce(SF,0) as PA,\n                                     AB + SH + coalesce(SF, 0) - H as O,\n                                     BB + coalesce(IBB,0) + HBP as BB,\n                                     H - \"2B\" - \"3B\" - HR  as \"1B\",\n                                     \"2B\",\n                                     \"3B\",\n                                     \"HR\"\n                                     from batting inner join appearances on batting.playerID = appearances.playerID\n                                     and batting.yearID = appearances.yearID\n                                     where batting.yearID = {} and AB &gt; 10 and appearances.G_p = 0'''.format(year),\n                           conn)\n\nprint(len(rc_base_query(year, conn)))\nrc_base_query(year, conn).head()\n\n708\n\n\n\n\n\n\n\n\n\nPA\nO\nBB\n1B\n2B\n3B\nHR\n\n\n\n\n0\n627\n428\n58\n83\n33\n6\n19\n\n\n1\n38\n22\n7\n4\n3\n1\n1\n\n\n2\n78\n57\n6\n5\n4\n0\n6\n\n\n3\n452\n311\n49\n72\n11\n2\n7\n\n\n4\n702\n475\n56\n97\n29\n10\n35\n\n\n\n\n\n\n\nBecause players make outs in the process of producing runs, we will want to normalize the runs created by the outs consumed to create these runs, which turns this back into a rate stat and thus allows us to compare across players regardless of the number of attempts. We can thus view the results of all players over a full season on a histogram this way.\n\ndef runs_created(season_stats):\n    \"computes runs created per out from a data frame/array holding plate appearances, outs, singles, doubles, triples, and home runs\"\n    \n    if isinstance(season_stats, pandas.DataFrame):\n        plate_appearances = season_stats[\"PA\"]\n        outs = season_stats[\"O\"]\n        walks = season_stats[\"BB\"]\n        singles = season_stats[\"1B\"]\n        doubles = season_stats[\"2B\"]\n        triples = season_stats[\"3B\"]\n        home_runs = season_stats[\"HR\"]\n    else:\n        if season_stats.ndim == 1: # Only the stats of a single player\n            season_stats = np.reshape(season_stats, (1, -1))\n        plate_appearances = np.sum(season_stats, axis=-1)\n        outs = season_stats[:,0]\n        walks = season_stats[:,1]\n        singles = season_stats[:,2]\n        doubles = season_stats[:,3]\n        triples = season_stats[:,4]\n        home_runs = season_stats[:,5]\n    \n    # scrub any player that made no outs\n    \n    nz_outs = (outs &gt; 0)\n    \n    rc = (1.-outs/plate_appearances)*(singles + 2*doubles + 3*triples + 4*home_runs)\n    \n    return rc[nz_outs]/outs[nz_outs]\n\ndef rc_histogram(*args, scale=\"linear\"):\n    \"plot a histogram of any number of runs created arrays/sequences on a linear or logarithmic scale\"\n    \n    for rc in args:\n        if scale == \"log\":\n            rc_data = runs_created(rc)\n            dist = np.log(rc_data[rc_data &gt; 0.])/np.log(10.)\n            bins = np.linspace(-4., 1., 100)\n        else:\n            dist = runs_created(rc)\n            bins = np.linspace(0, 1.5, 100)\n        plt.hist(dist, bins=bins, density=True)\n    if scale == \"log\":\n        plt.xlabel(\"Log Runs Created/Out\")\n    else:\n        plt.xlabel(\"Runs Created/Out\")\n    plt.ylabel(\"Probability density\")\n\nseason_stats = rc_base_query(year, conn)\nrc_histogram(season_stats)\n\n\n\n\nAs we can see, the bulk of the distribution is around 0.2, and then the numbers fall off as we move out towards 0.4, with only a few players managing more than that.\nAs an aside, the raw numbers here are illustrative of the general idea behind using Runs Created to measure a player’s ability, but this simple version does not account for a number of well-known additional effects:\n\nWe have not measured baserunning ability. Total bases only includes the bases advanced by the players hitting, not their ability to run the bases once on base. More complex formulae can better account for this effect by including stolen bases. However, more general baserunning ability that allows a runner to advance on another teammate’s hits is not measured in the stats we have available in this database so we cannot account for that here.\nWe also have not corrected for the fact that different stadiums tend to produce more runs than others, so a player that has half of their opportunities in a stadium that is friendly to hitters will get a boost for which an empirical correction can be made.\nOne final, and more subtle issue, is the fact that the best players will be overvalued here because they cannot possibly be on base when they come up to hit (i.e. a player that is significantly better than their teammates will have fewer people on base than would be expected from the average). Since the statistic assumes that everyone has an average number of people on base for their hits, this will be biased towards the better players that will in general have fewer people on base.\n\nMore sophisticated techniques are needed to treat these effects, but won’t change the overall way that we convert this distribution into a replacement level value, so we will ingore these additional effects going forwards. As mentioned earlier, a good heuristic estimate of replacement level can be found by taking 80% of the mean, so from these calculations we can estimate replacement level:\n\ndef heuristic_replacement_level(season_stats):\n    \"calculate empirical replacement level statistic using 80% heuristic\"\n    return 0.8*np.mean(runs_created(season_stats))\n\nprint(\"Heuristic replacement level for {}: {} runs/out\".format(year, heuristic_replacement_level(season_stats)))\n\nHeuristic replacement level for 2019: 0.1321843895833081 runs/out\n\n\nThis suggests that creating 0.13 runs per out is a good baseline for comparison for a position player. However, it is not clear why this is the case. The distribution is strongly peaked near 0.2, then falls off towards zero, so it’s not obvious where in the distribution below the mode we should set a cut off for replacement level. The tail of the distribution is also quite heavy, with small numbers of players producing lots of runs. Are these players really so much better than the others, or is this just luck? To address this, we need a way to simulate the season many times to estimate what we expect this distribution to look like with more samples so that we can scrutinize it in more depth."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#multinomial-bayesian-inference",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#multinomial-bayesian-inference",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Multinomial Bayesian Inference",
    "text": "Multinomial Bayesian Inference\nTo simulate many versions of the same season, we turn to Bayesian Inference. Bayesian models treat their parameter values as probability distributions, which allows us to quantify the uncertainty in the underlying model and propagate that uncertainty forward in additional simulations. Formally, we treat each parameter \\(\\theta\\) in the model as a probability distribution, and determine what our updated beliefs about the parameter values are once we view the data \\(y\\), \\(p(\\theta|y)\\). Computing this probability distribution is done via Bayes’ Rule:\n\\[p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\\]\n\\(p(y|\\theta)\\) here is the likelihood, or the probability of the data given a particular value of the model parameters. \\(p(\\theta)\\) is the prior, our initial beliefs about reasonable values of the parameter. \\(p(y)\\) is known as the evidence, and is the probability of getting the data over all possible realizations of the experiment. This rule lets us convert the likelihood, which is generally more straightforward to compute, to the posterior by weighting it using the prior and evidence.\n\nMultinomial Distribution\nWhat model might we use to describe a hitter? At the most basic level, we can think of a hitter as a set of probabilities that they will make an out, draw a walk hit a single, hit a double, etc. Thus, a set of six probabilities that sum to 1 is a straightforward way of capturing the abilities of a hitter. Once we have those five probabilities, the outcome of a series of hitting attempts is described by a Multinomial distribution.\nFor example, if I am a hitter and I have a 60% chance of making an out, a 10% chance of drawing a walk, 20% chance of hitting a single, 5% chance of hitting a double, 1% chance of hitting a triple, and 4% chance of hitting a hume run, then if I come up to bat 100 times I would expect the following distribution of outcomes:\n\ndef plot_multiple_distributions(samples):\n    \"make a plot of multiple components of probability samples\"\n    \n    assert samples.shape[-1] == 6, \"samples must have 6 components\"\n    \n    labels = [\"Out\", \"Walk\", \"Single\", \"Double\", \"Triple\", \"Home Run\"]\n    \n    plt.figure(figsize=(12,4))\n    \n    for i in range(6):\n        plt.subplot(int(\"16\"+str(i+1)))\n        plt.hist(samples[:,i], bins=25)\n        plt.xlabel(labels[i])\n        \n    plt.tight_layout()\n\ndef plot_multinomial(n_samples, n_trials, p):\n    \"plot the distribution of outcomes from a set of multinomial trials\"\n    \n    assert n_samples &gt; 0, \"number of samples must be greater than zero\"\n    assert n_trials &gt; 0, \"number of trials must be greater than zero\"\n    assert p.ndim == 1, \"p must be a 1D array\"\n    assert np.all(p &gt;= 0.) and np.sum(p) == 1., \"all probabilities must be positive and sum to 1\"\n    \n    samples = scipy.stats.multinomial.rvs(n=n_trials, p=p, size=n_samples)\n    plot_multiple_distributions(samples)\n    \nhypothetical_player_probs = np.array([0.6, 0.1, 0.2, 0.05, 0.01, 0.04])\nplot_multinomial(1000, 100, hypothetical_player_probs)\n\n\n\n\nThus, if I repeated my hypothetical set of 100 hitting attemps many times, this shows the range of outcomes I might expect.\n\n\nPrior\nNow I need to specify my prior beliefs about what I probabilities I might expect a hitter to have before I see any data. One natural way to handle this is a Dirichlet distribution, which has the nice property that it plays nicely with a multinomial distribution and gives me an analytical way to compute the posterior. A Dirichlet distribution is a generalization of the Beta distribution in multiple dimensions (in this case, 6 as we need our 6 probabilities for our multinomial distribution).\nA Beta distribution takes 2 parameters, while an \\(n\\)-dimensional Dirichlet distribution takes \\(n\\) parameters, usually referred to as a vector \\(\\alpha\\). As with the beta distribution, if the parameters are small the probability density is concentrated towards one outcome being more likely, while if the weights are large then probability density is more even across the different dimensions. Combining small and large values let us tweak the distribution in a wide range of different ways, giving plenty of flexibility in describing our prior beliefs.\nFor this model, I am going to specify a prior that is rather pessimistic about a player’s ability, thus the probabilities should be large for making an out, and relatively small for the more valuable hits. Only once a player shows sustained ability to do better than this threshold will I have more faith in their run producing capabilities. (One caveat here is that I need to be sure at the end of my analysis that my estimation of replacement level is significantly higher than this level, though we will see in the end that the analysis here can identify those players and separate them out of the overall distribution). Note that these choices are inherently subjective and are based on my personal understanding of what I think a good baseball player is likely to do based on many years of watching and following the game.\nThe following illustrates the priors that I use in my analysis:\n\ndef plot_dirichlet(n_samples, alpha):\n    \"plot samples of the probabilities drawn from a Dirichlet distribution\"\n    \n    assert n_samples &gt; 0, \"number of samples must be greater than zero\"\n    assert np.all(alpha &gt;= 0.), \"all alpha components must be positive\"\n    \n    samples = scipy.stats.dirichlet.rvs(alpha, size=n_samples)\n    \n    plot_multiple_distributions(samples)\n\npriors = np.array([12., 1., 2., 0.05, 0.01, 0.03])\nplot_dirichlet(1000, priors)\n\n\n\n\n\n\nPosterior\nAs noted above, if I use a Dirichlet prior, I can compute the posterior analytically. In this case, the posterior is also a Dirichlet distribution, but with the \\(\\alpha\\) parameters summed with the number of observations of each of the outcomes. In this way, if the absolute number of outcomes is high the posterior weights will reflect that of the data, while if there are only a few observations the prior will still hold some sway over the distribution. Thus, if I take my hypothetical player described above with 100 plate appearances, the posterior given a particular set of observations will be:\n\nhypothetical_player_stats = scipy.stats.multinomial.rvs(size=1, n=100, p=hypothetical_player_probs).flatten()\n\nplot_dirichlet(1000, priors + hypothetical_player_stats)\n\n\n\n\n\n\nSimulating full seasons using the posterior\nGiven the posterior samples, we can use these samples from the posterior of the probabilities describing this player to project their performance over a full season (say over 600 plate appearances, a typical number for someone that plays a full season):\n\ndef project_player_season(player_stats, n_samples, plate_appearances, priors):\n    \"project multiple samples from the season of a player given their stats\"\n    \n    assert n_samples &gt; 0, \"number of samples must be greater than zero\"\n    assert plate_appearances &gt; 0, \"number of plate appearances must be greater than zero\"\n    assert player_stats.shape == (6,), \"player stats must be a 1D array of length 5\"\n    assert np.all(player_stats &gt;= 0), \"player stats must be non-negative integers\"\n    assert np.all(priors &gt; 0.), \"all prior weights must be greater than zero\"\n    \n    outcomes = np.zeros((n_samples, n_samples, 6), dtype=np.int64)\n    samples = scipy.stats.dirichlet.rvs(alpha=priors + player_stats, size=n_samples)\n\n    # occasionally roundoff error causes the sum of these sampled probabilities to be larger than 1\n    if np.any(np.sum(samples, axis=-1) &gt; 1.):\n        rounded_error = (np.sum(samples, axis=-1) &gt; 1.)\n        samples[rounded_error] = (samples[rounded_error]/\n                                  (np.sum(samples[rounded_error], axis=-1)[:,np.newaxis] + 1.e-15))\n    \n    for i in range(n_samples):\n        outcomes[i] = scipy.stats.multinomial.rvs(n=plate_appearances, p=samples[i], size=n_samples)\n        \n    return np.reshape(outcomes, (-1, 6))\n\nfull_season_pa = 600\nprint(\"Hypothetical {} plate appearance season [Outs, Walks, Singles, Doubles, Triples, Home Runs]: {}\".format(\n    full_season_pa, project_player_season(hypothetical_player_stats, 1, full_season_pa, priors)[0]))\n\nHypothetical 600 plate appearance season [Outs, Walks, Singles, Doubles, Triples, Home Runs]: [365  49 135  25   0  26]\n\n\nWith all of this, we are now in a position to simulate the Runs Created over many hypothetical seasons to see the variation in the value of this hypothetical player:\n\nhypothetical_player_seasons = project_player_season(hypothetical_player_stats, 100, full_season_pa, priors)\nrc_histogram(hypothetical_player_seasons)\n\n\n\n\nFrom this, we can see that after 100 chances, we still have quite a bit of uncertainty about how well a player will perform over an entire season, though we are somewhat confident that this player is better than average based on comparing this distribution to the observed data for the season."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#simulating-league-seasons",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#simulating-league-seasons",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Simulating League Seasons",
    "text": "Simulating League Seasons\nNow we can do this for every player in the 2019 season repeatedly to examine what we might expect if this season were to be re-run several times. Once we have simulated this, we can overlay the actual distribution to see what differences we notice.\n\ndef project_league_season(season_stats, n_samples, priors):\n    \n    if isinstance(season_stats, pandas.DataFrame):\n        season_stats_array = np.array(season_stats)[:,1:]\n    else:\n        season_stats_array = season_stats\n    \n    league_projections = np.zeros((len(season_stats), n_samples*n_samples, 6), dtype=np.int64)\n    \n    for i in range(len(season_stats)):\n        league_projections[i] = project_player_season(season_stats_array[i],\n                                                      n_samples, np.sum(season_stats_array[i]), priors)\n\n    return np.reshape(league_projections, (-1, 6))\n\ndef project_year(season_stats, stats, n_samples=10):\n    \"project multiple realizations of a full league season\"\n\n    return project_league_season(season_stats, n_samples, priors)\n\nsimulated_seasons = project_year(season_stats, priors)\nrc_histogram(simulated_seasons, season_stats)\nplt.legend([\"Simulated\", \"Actual\"])\n\n&lt;matplotlib.legend.Legend at 0x12da04d90&gt;\n\n\n\n\n\nAnd on a logarithmic scale, to better see the behavior for small values of runs/out:\n\nrc_histogram(simulated_seasons, season_stats, scale=\"log\")\nplt.legend([\"Simulated\", \"Actual\"])\n\n&lt;matplotlib.legend.Legend at 0x12d7f2bb0&gt;\n\n\n\n\n\nFrom this, we can see that our simulation method is able to reproduce the distribution in the real data, though it is smoother as we are able to draw many more samples than are available in a single season. However, one important difference is that since we needed to specify a prior (and one that was fairly pessimistic), there is an extra peak near zero runs/out in the simulation. These players are not really relevant for estimating replacement level, so we will need to determine a way to scrub them from the distribution.\nAdditionally, on the logarithmic scale we note that distribution is not symmetric – there are more players in the real data that are worse than average than players that are better than average. This suggests that there are some players getting attempts that are significantly worse than most major league players, which are likely to be indicative of replacement level.\nWe can check this by plotting what distribution we would expect if all players’ abilities were drawn from our prior:\n\nseason_prior_level = project_player_season(np.zeros(6), 1000, full_season_pa, priors)\nrc_histogram(season_prior_level)\n\n\n\n\nAnd again on a logarithmic scale:\n\nrc_histogram(season_prior_level, scale=\"log\")\n\n\n\n\nThis looks similar to the peak closer to zero in the above simulated distribution, confirming that we have a number of players for which we do not have evidence that they are significantly better than our prior. The remaining players that perform better than this level are players where we have evidence of how good they are. These are either “good” major league players, or if they are too far on the negative side of the distribution, then I propose that these are a good representation of replacement players. Thus, we need to determine a method to separate out these parts of the overall distribution."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#approximating-the-distributions",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#approximating-the-distributions",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Approximating the distributions",
    "text": "Approximating the distributions\nTo try and separate out distributions of the “good” players, “sub-replacement” players, and “not enough data” players, we need to first hypothesize the form we might expect the distributions of simulated player seasons to take. One constraint is that we know that Runs Created has to be positive. A commonly used distribution for positive random variables is a Lognormal distribution. It is fairly easy to check if a variable is Lognormal, as we simply can make a histogram with a log horizontal scale and check if the data looks roughly normal on this scale:\n\nrc_histogram(hypothetical_player_seasons, scale=\"log\")\n\n\n\n\nThis looks reasonably symmetric, so fitting a normal distribution to the data on a log scale is probably a reasonble way to approximate this distribution. While I might in some situations dig into the log-normality of these distributions more carefully, for the sake of this analysis this is probably a reasonable approximation to make.\n\nBayesian Gaussian Mixture Models\nTo separate out the three parts of the distribution, I will fit a Gaussian Mixture Model to the data on a logarithmic scale. A Gaussian Mixture Model approximates a probability distribution as the sum of several Normal distributions, allowing us to tease out the three components by scrutinizing the parameters of the distributions. I can fit this type of model readily using the Scikit-Learn package:\n\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n@ignore_warnings(category=ConvergenceWarning)\ndef fit_mixture_model(simulated_seasons, n_components):\n    \"Fit a Gaussian Mixture model to the given season\"\n    \n    rc = runs_created(simulated_seasons)\n    rc = np.reshape(rc[rc &gt; 0.], (-1, 1))\n    \n    return BayesianGaussianMixture(n_components=n_components).fit(np.log(rc)/np.log(10.))\n\nn_components = 3\ngmm = fit_mixture_model(simulated_seasons, n_components)\n\nI occasionally get ConvergenceWarnings when fitting this model. Further inspection has shown that these models usually seem to make reasonable predictions, so for the sake of clean presentation I suppress such warnings here. We can plot the mixture model on top of the data to which it has been fit as follows:\n\ndef get_normal_from_gmm(gmm, index):\n    \"converts gmm model to callable pdf function\"\n    \n    assert index &gt;= 0\n    assert index &lt; len(np.squeeze(gmm.weights_))\n    \n    weights_array = np.reshape(np.squeeze(gmm.weights_), (-1, 1))\n    mean_array = np.reshape(np.squeeze(gmm.means_), (-1, 1))\n    cov_array = np.reshape(np.squeeze(gmm.covariances_), (-1, 1))\n    \n    def norm_pdf(x):\n        return(weights_array[index]*\n               scipy.stats.norm.pdf(x, loc=mean_array[index],\n                                    scale=np.sqrt(cov_array[index])))\n    \n    return norm_pdf\n\ndef plot_distribution_sum(simulated_seasons, n_components):\n    \"plot the Gaussian mixture models with the underlying data\"\n    \n    rc_histogram(simulated_seasons, scale=\"log\")\n    \n    gmm = fit_mixture_model(simulated_seasons, n_components)\n\n    pdfs = [get_normal_from_gmm(gmm, i) for i in range(n_components)]\n    \n    logrc_vals = np.linspace(-4., 1., 101)\n    \n    for (pdf, i) in zip(pdfs, range(n_components)):\n        plt.plot(logrc_vals, pdf(logrc_vals), label=\"Distribution {}\".format(i))\n    plt.plot(logrc_vals, np.sum(np.array([pdf(logrc_vals) for pdf in pdfs]), axis=0), label=\"Sum\")\n    plt.legend()\n\n    print(\"Means: {}\".format(np.squeeze(gmm.means_)))\n    print(\"Variances: {}\".format(np.squeeze(gmm.covariances_)))\n    \nplot_distribution_sum(simulated_seasons, n_components)\n\nMeans: [-1.01128144 -1.39076184 -0.71722101]\nVariances: [0.07110075 0.33645528 0.02934296]\n\n\n\n\n\nWe see that the Gaussian Mixture Model was able to fit the three components approximately where we would expect. (Note that because the Gaussian Mixture Model does not always put the components in the same order after fitting, the numbering of the different distributions may not always be the same. Thus, I try not to refer to the legend labels in the following for consistency.)\nAt the lowest end, we see a very broad distribution over the full range of the plot, which is quite similar to what was observed when just plotting samples from the prior on a logarithmic scale above. These are players for which data is scarce, and we will ignore these.\nThen there is the high end, where most of the mass in the distribution can be found. These are the good, every day major league players. While we could use this distribution to understand how good a particular player is, it likely does not contain any replacement players so we cannot use it to independently assess what level replacement is. Note that this is precisely why we went to the trouble of doing this simulation – the standard 80% of league average heuristic is arbitrary because it is based on players that are clearly not replacement level.\nThis leaves the intermediate distribution – the players that are clearly below the typical major league player, but also those where we have evidence of how good they are over the prior distribution that I specify. Thus, perhaps this distribution is representative of the production of replacement players in this particular season. To get a replacement level estimate, I just have to separate out this distribution, and for simplicity’s sake, use the mean of this to estimate replacement level:\n\ndef extract_replacement_level(gmm):\n    \"\"\"\n    Determine the mean runs created/out for players in the sub-replacement distribution\n    (assumes the highest mean is associated with good players, while the second highest\n    mean is replacement level)\n    \"\"\"\n    \n    means = np.sort(np.squeeze(gmm.means_))\n    \n    assert len(means) &gt;= 2\n    \n    return 10.**(means[-2])\n\nprint(\"Estimated replacement level: {}\".format(extract_replacement_level(gmm)))\nprint(\"Heuristic replacement level: {}\".format(heuristic_replacement_level(season_stats)))\n\nEstimated replacement level: 0.09505133831076636\nHeuristic replacement level: 0.1321843895833081\n\n\nThis is somewhat below the heuristic estimate, but we have a much less arbitrary way of estimating it so we can better understand what it means. Our estimate of replacement level is the expected runs created per out from the population of players where (1) we have evidence from their hitting that they are better than our pessimistic prior, and (2) we cannot explain the presence of these players in the distribution of simulated season statistics based on an assumption of normality in the simulated data. This happens to be about 60% of the league average, rather than 80% of the league average, though I am less interested in getting the exact numbers to line up than better understanding the various distributions that fall out of the analysis and using them to explain replacement level more precisely, so this will be fine for these purposes.\nWith this machinery in place, one thing we can easily do is compute this for every year since 1900 in baseball history:\n\ndef bayesian_replacement_level(year, conn, priors, n_components):\n    \"Full function for Bayesian replacement level estimation\"\n    \n    season_stats = rc_base_query(year, conn)\n    \n    league_season = project_year(season_stats, priors)\n    \n    gmm = fit_mixture_model(league_season, n_components)\n    \n    return extract_replacement_level(gmm)\n\ndef compute_heuristic_replacement_level(year, conn):\n    \"full function for heuristic replacement level calculation\"\n    \n    season_stats = rc_base_query(year, conn)\n    \n    return heuristic_replacement_level(season_stats)\n\ndef make_time_plot(start, end, priors, n_components, conn):\n    \"plot the evolution of estimates for replacement level over time\"\n\n    start = int(start)\n    end = int(end)\n    n_components = int(n_components)\n    \n    years = np.arange(start, end)\n    \n    heuristic_estimates = [compute_heuristic_replacement_level(yr, conn) for yr in years]\n    bayesian_estimates = [bayesian_replacement_level(yr, conn, priors, n_components) for yr in years]\n\n    plt.plot(years, heuristic_estimates, label=\"Heuristic\")\n    plt.plot(years, bayesian_estimates, label=\"Bayesian\")\n    plt.legend()\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Replacement Runs Created/Out\")\n    \nmake_time_plot(1900, 2020, priors, n_components, conn)\n\n\n\n\nAs we can see, this method gives a good indication of the variability in replacement level over the course of the history of baseball that mirrors the traditional heuristic, though the level is somewhat lower (as mentioned above). We also note that the difference between the two measures is smaller prior to 1920, with more stable differences after that despite some significant fluctuations over time. We see that it captures the overall trends in run production that are well documented – runs were scarce in the “dead ball” era prior to 1920, then show an uptick, a decrease in the 1940’s that holds fairly stable until an increase in the late 1990’s and early 2000’s during the “steriod” era, and a lull again in the 2010’s that has since increased again."
  },
  {
    "objectID": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#discussion",
    "href": "stories/2021-07-21-Baseball-Replacement-Level/2021-07-21-Baseball-Replacement-Level.html#discussion",
    "title": "French Pastries, Baseball Players, and Marginal Utility",
    "section": "Discussion",
    "text": "Discussion\nAs mentioned above, our estimate of runs created is somewhat crude for this analysis. Extending this to be more accurate requires factoring in base running, correcting for the effect of each stadium on run production, and being more precise in how the runs each team scores can be attributed to each player (i.e. the “a player cannot be on base for themselves” effect). One additional factor to consider is fielding luck – more modern measurement techniques can track the ball when it is hit, and corrections can be made for hard hit balls that are likely to have been hits but the defense was positioned correctly to make an out. However, this data is only available for modern play, so projecting these effects over the course of baseball history is not possible.\nA similar analysis can be done for a pitchers. Pitchers are fundamentally measured by their ability to get outs and not giving up hits or walks, just with the desired outcomes reversed when compared to hitters. However, much analysis by statisticians have shown that there is significant randomness in the fraction of batted balls that turn into hits versus outs, so the simplest way to study pitchers is to look at the things that are entirely in the pitcher’s control: walks, strikeouts (i.e. the batter was unable to hit the ball and this out can entirely be credited to the pitcher’s ability), and home runs over a number of innings pitched. This is known as Fielding Independent Pitching (FIP), and can be converted into a number of runs saved. This can then be analyzed in a similar fashion to the runs that a hitter creates to measure pitcher value.\nHow would a team be likely to use this data and approach? First, we can simulate the range of seasons that any player would be likely to produce given their actual statistics, which is useful for evaluating individual players. This approach can also determine if a team has been lucky or unlucky in terms of their hits producing runs. Very often, a team that produces more runs than would be expected with a statistic like runs created is likely doing so based on luck, and would not necessarily be expected to sustain the same level over the remainder of a full season. There are a number of ways we could use this approach to further evaluate players and teams, though this is beyond the scope of this story.\nBeyond baseball, measuring marginal utility is more difficult because the value is often subjective and hard to boil down to a clear metric from which a baseline level can be estimated. For instance, I love getting coffee from my local cafe. Should I be comparing this to coffee I make at home? Get from Starbucks? From the instant machine at my local convenience store? Depending on the circumstance, “available with minimal effort” is highly dependent on context and mood. Until we find a way to quantify that in an objective way, sports will remain one of the most popular avenues to rigorously quantify and analyze marginal utility."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "",
    "text": "Ed Chalstrey. The Alan Turing Institute.\nMarkus Hauru. The Alan Turing Institute."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#python-setup",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#python-setup",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Python setup",
    "text": "Python setup\nLet’s get started by importing the packages that we’ll need for this story.\n\nimport ast\nimport collections\nimport itertools\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport pycountry as pc\nimport pycountry_convert as pcc\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set()"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#loading-the-data",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#loading-the-data",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Loading the data",
    "text": "Loading the data\n\n hc_data_url = (\n    \"https://hcommons.org/deposits/download/hc:42512/CONTENT/desert_island_discs.xlsx\"\n )\n\ncastaways_df = pd.read_excel(\n    hc_data_url,\n    sheet_name=\"castaways\",\n    parse_dates=[\"date\", \"date_of_birth\"],\n    index_col=0,\n)  # castaways, discs, spotify_data\ncastaways_df[\"year\"] = [i.year for i in castaways_df[\"date\"].dt.date]\ncastaways_df.head(1)\n\n\n\n\n\n\n\n\nepisode_ref\ndate\ncastaway_ref\nname\nstd_name\ngender\nprofession\ncountry_of_citizenship\nplace_of_birth\ncountry_of_birth\ndate_of_birth\nfavTrack\nluxury\nbook\nwiki_link\nlink\nyear\n\n\n\n\n0\n2828\n1942-01-29\n2855\nVic Oliver\nvic oliver\nmale\n['actor', 'performing artist', 'artist', 'prof...\nAustria-Hungary\nVienna\nAustria\n1898-07-08\nNA by NA\nNaN\nNaN\nhttps://en.wikipedia.org/wiki/Vic_Oliver\nhttps://www.bbc.co.uk/programmes/p009y0nq\n1942"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#plotting-functions",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#plotting-functions",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Plotting functions",
    "text": "Plotting functions\nIn order to look at the castaways who have been on the show in more detail, lets define some functions. These will help us to aggregate episodes in the same year, and then plot summary metrics as a function of year.\n\n# functions to help make stacked plots over time\n\n\ndef normalize_rows(df):\n    df = df.div(df.sum(axis=1), axis=0)\n    df = df.fillna(0)\n\n    return df\n\n\ndef cumulative_columns(df):\n    \"\"\"\n    Loop through columns and calculate running totals\n    per row.\n    \"\"\"\n\n    # for each column (except the first)\n    for i, col in enumerate(df):\n        if i == 0:\n            continue\n\n        # add the previous column to this column\n        df[col] += df[df.columns[i - 1]]\n\n    return df\n\n\ndef moving_average(df, n=3):\n    df = df.rolling(window=n).mean()  # moving average\n    df = df.dropna(how=\"all\")  # drop lost columns\n\n    return df\n\n\ndef make_timeplot(\n    df, ymin=0, horiz_line_y=0, leg_title=\"\", y_title=\"Fraction of Castaways\"\n):\n    c = sns.color_palette(\"Paired\", max(4, len(df.columns)))\n\n    plt.figure(figsize=(10, 3), dpi=100)\n    plt.gca().set_facecolor(c[0])\n    xs = df.index\n\n    for i, col in enumerate(reversed(df.columns)):\n        plt.fill_between(\n            xs, df[col], interpolate=False, color=c[i], label=col, alpha=1.0\n        )\n\n    if horiz_line_y &gt; 0:\n        plt.plot(\n            plt.xlim(),\n            [horiz_line_y, horiz_line_y],\n            color=\"black\",\n            ls=((0, (10, 5))),\n            lw=1.5,\n        )\n\n    plt.xlabel(\"Year\", fontsize=10, fontweight=\"bold\")\n    plt.ylabel(y_title, fontsize=10, fontweight=\"bold\")\n\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n\n    plt.xlim(df.index[0], df.index[-1])\n    plt.ylim(ymin, 1)\n    leg = plt.legend(\n        loc=\"upper left\",\n        bbox_to_anchor=(1.01, 1.01),\n        fontsize=10,\n        facecolor=\"white\",\n        frameon=True,\n        framealpha=1,\n        edgecolor=\"white\",\n    )\n    leg.set_title(\n        leg_title, prop={\"size\": 10, \"weight\": \"bold\", \"family\": \"sans-serif\"}\n    )\n    leg._legend_box.align = \"left\"\n\n    plt.show()"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#gender",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#gender",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Gender",
    "text": "Gender\nNow we have these functions to make the plots we need, we can first take a look at the distribution of gender through time.\n\ngender_df = pd.pivot_table(\n    castaways_df, columns=\"gender\", index=\"year\", values=\"date\", aggfunc=\"count\"\n)\ngender_df = normalize_rows(gender_df)\ngender_df = gender_df[[\"male\", \"female\"]]\ngender_df = cumulative_columns(gender_df)\nmake_timeplot(gender_df, horiz_line_y=0.5, leg_title=\"Gender\")\n\n\n\n\nOkay, there are a few things to notice here. The first is that, throughout the bulk of the show’s long history, there have been significantly more male castaways than female. But the plot is very spikey, demonstrating that from year to year the gender ratio of castaways on the show fluctuates by an appreciable amount. Since we are interested in looking at trends over several years, and not differences between consecutive years, lets try smoothing this plot by using a moving average of 5 years.\n\nmake_timeplot(moving_average(gender_df, n=5), horiz_line_y=0.5, leg_title=\"Gender\")\n\n\n\n\nThe smoothed plot confirms the observation that the show has had significantly more males than females.\nAround 1950, it looks as though there was a much more equal split between men and women. However, this conclusion is drawn a bit hurriedly. If we look at the number of episodes per year around this time, we see that the show was actually off the air for several years around this period. There was also a year (1946), where only one episode aired, and the castaway was female (actress Barbara Mullen). As we are giving each year equal visual weight with our plot, the show looks more balanced in terms of gender at it’s inception than it was.\nFrom the beginning of the show, up until the early 2010s, there were about 3/4 male castaways and 1/4 female castaways. A little after 2010, we see a qualitative shift, and the split between males and females quickly becomes much more even. The suddenness of this change suggests it was a active decision taken by the producers of the show. The fact that this happened relatively recently is perhaps surprising.\n\nprint(\"Percentages of different genders, integrated over time\")\nround(castaways_df[\"gender\"].value_counts(dropna=False) / len(castaways_df) * 100, 1)\n\nPercentages of different genders, integrated over time\n\n\nmale                  68.6\nfemale                29.7\nNaN                    1.6\ntransgender female     0.1\nName: gender, dtype: float64\n\n\nFinally, it’s worth noting that since Wikipedia does not have information on every castaway, we are missing information on some castaways. In this case, we don’t have a gender for 1.6% of castaways."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#age",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#age",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Age",
    "text": "Age\nWe might also wonder whether the age of castaways (at the time of them appearing on the show) has changed over time. For example, might recent guests on the show be younger to appeal to a younger demographic of listeners?\nUsing the date of birth of the castaway from Wikidata, and the broadcast date of the episode, we can calculate how old the castaways were when they were on the show.\n\n# Calculate Age\ncols_of_interest = [\"name\", \"std_name\", \"gender\", \"date\", \"date_of_birth\", \"year\"]\ncastaways_age_df = castaways_df[cols_of_interest].assign(\n    age=lambda x: (\n        (x[\"date\"].dt.year - pd.to_datetime(castaways_df[\"date_of_birth\"]).dt.year)\n        - (x[\"date\"].dt.month &lt; pd.to_datetime(castaways_df[\"date_of_birth\"]).dt.month)\n    )\n)\ncastaways_age_df.head(1)\n\n\n\n\n\n\n\n\nname\nstd_name\ngender\ndate\ndate_of_birth\nyear\nage\n\n\n\n\n0\nVic Oliver\nvic oliver\nmale\n1942-01-29\n1898-07-08\n1942\n43.0\n\n\n\n\n\n\n\n\n# find oldest castaway(s)\ncastaways_age_df.loc[castaways_age_df[\"age\"] == castaways_age_df[\"age\"].max()]\n\n\n\n\n\n\n\n\nname\nstd_name\ngender\ndate\ndate_of_birth\nyear\nage\n\n\n\n\n1455\nSir Robert Mayer\nrobert mayer\nmale\n1979-06-05\n1879-06-05\n1979\n100.0\n\n\n2258\nKathleen Hale\nkathleen hale\nfemale\n1998-06-05\n1898-05-24\n1998\n100.0\n\n\n\n\n\n\n\n\n# find youngest castaway(s)\ncastaways_age_df.loc[castaways_age_df[\"age\"] == castaways_age_df[\"age\"].min()]\n\n\n\n\n\n\n\n\nname\nstd_name\ngender\ndate\ndate_of_birth\nyear\nage\n\n\n\n\n1024\nQuentin Poole\nquentin poole\nmale\n1970-12-28\n1957-01-01\n1970\n13.0\n\n\n\n\n\n\n\nThe youngest castaway was musician Quentin Poole, who at the age of 13 appeared on the 1970 Christmas episode when he was the head chorister at King’s College Cambridge. Businessman, philanthropist and patron of music Sir Robert Mayer appeared on the programme on his 100th birthday.\n\n# Make a dataframe of all teenagers\nteens = castaways_age_df[castaways_age_df[\"age\"] &lt; 20]\nteens.head(10)\n\n\n\n\n\n\n\n\nname\nstd_name\ngender\ndate\ndate_of_birth\nyear\nage\n\n\n\n\n84\nPetula Clark\npetula clark\nfemale\n1951-05-02\n1932-11-15\n1951\n18.0\n\n\n313\nJanette Scott\njanette scott\nfemale\n1956-12-24\n1938-12-14\n1956\n18.0\n\n\n736\nHayley Mills\nhayley mills\nfemale\n1965-05-10\n1946-04-18\n1965\n19.0\n\n\n1024\nQuentin Poole\nquentin poole\nmale\n1970-12-28\n1957-01-01\n1970\n13.0\n\n\n\n\n\n\n\n\n# and one of all those over 95 years old\nolder = castaways_age_df[castaways_age_df[\"age\"] &gt; 95]\nolder.head(10)\n\n\n\n\n\n\n\n\nname\nstd_name\ngender\ndate\ndate_of_birth\nyear\nage\n\n\n\n\n1455\nSir Robert Mayer\nrobert mayer\nmale\n1979-06-05\n1879-06-05\n1979\n100.0\n\n\n1497\nCommissioner Catherine Bramwell-Booth\ncommissioner catherine bramwell-booth\nfemale\n1980-04-11\n1883-07-20\n1980\n96.0\n\n\n1712\nGeorge Abbott\ngeorge abbott\nmale\n1984-09-07\n1887-06-25\n1984\n97.0\n\n\n1885\nAthene Seyler\nathene seyler\nfemale\n1989-03-19\n1889-05-31\n1989\n99.0\n\n\n1903\nGwen Ffrangcon-Davies\ngwen ffrangcon-davies\nfemale\n1989-09-01\n1891-01-25\n1989\n98.0\n\n\n2258\nKathleen Hale\nkathleen hale\nfemale\n1998-06-05\n1898-05-24\n1998\n100.0\n\n\n2889\nJeremy Hutchinson\njeremy hutchinson\nmale\n2013-10-25\n1915-03-28\n2013\n98.0\n\n\n2961\nHarry Rabinowitz\nharry rabinowitz\nmale\n2015-07-03\n1916-03-26\n2015\n99.0\n\n\n\n\n\n\n\nFrom the two dataframes, teens and older, we can see there have been 4 castaways under the age of 20, and 8 over the age of 95. There have been no teenaged castaways for over 51 years, and the last castaway over 95 was broadcast seven years ago.\n\n# calculate average age of male and female castaways\nav_age_m = castaways_age_df[castaways_age_df[\"gender\"] == \"male\"][\"age\"].mean()\nav_age_f = castaways_age_df[castaways_age_df[\"gender\"] == \"female\"][\"age\"].mean()\nprint(\n    f\"The average age of the male castaways is {av_age_m:.1f} and the average age of the female castaways is {av_age_f:.1f}\"\n)\n\nThe average age of the male castaways is 53.5 and the average age of the female castaways is 53.7\n\n\nA quick calculation shows that the average age for male and female castaways is very similar, around 53 and a half years old.\nWe now will segment the castaways by decade to explore trends in age over time.\n\n# Add function to add median line to kde ridge plots\n\n\ndef kdeplot_med(data, **kwargs):\n\n    data = data.dropna(subset=[kwargs[\"x\"]])\n\n    _ax = plt.gca()\n\n    density = stats.gaussian_kde(data[kwargs[\"x\"]].dropna())\n\n    _, ymax = _ax.get_ybound()\n\n    _ax.axvline(\n        data[kwargs[\"x\"]].median(),\n        color=kwargs[\"color\"],\n        ymax=(density(data[kwargs[\"x\"]].median())[0] / ymax),\n    )\n    _ax.annotate(\n        int(data[kwargs[\"x\"]].median()),\n        (data[kwargs[\"x\"]].median() + 1, density(data[kwargs[\"x\"]].median())[0] * 0.65),\n        xycoords=\"data\",\n        fontweight=\"bold\",\n    )\n\n    return _ax\n\n\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\ng = sns.FacetGrid(\n    castaways_age_df.assign(decade=lambda x: (x[\"year\"] // 10) * 10),\n    palette=\"Paired\",\n    hue=\"decade\",\n    row=\"decade\",\n    aspect=7,\n    height=1.0,\n)\nBW = 0.9\ng.map_dataframe(\n    sns.kdeplot,\n    x=\"age\",\n    fill=True,\n    alpha=1,\n    bw_adjust=BW,\n    clip_on=False,\n)\ng.map_dataframe(sns.kdeplot, x=\"age\", color=\"black\", bw_adjust=BW)\ng.map_dataframe(\n    kdeplot_med, x=\"age\", fill=True, alpha=1, bw_adjust=BW, clip_on=False, color=\"black\"\n)\n\n\ndef label1(k, color, label):\n    ax = plt.gca()\n    ax.text(\n        0,\n        0.2,\n        label + \"s\",\n        fontweight=\"bold\",\n        color=\"black\",\n        ha=\"left\",\n        va=\"center\",\n        transform=ax.transAxes,\n    )\n\n\ng.map(label1, \"age\")\ng.set_ylabels(\"\")\ng.set_xlabels(\"Age\")\ng.fig.subplots_adjust(hspace=-0.5)\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(left=True)\nplt.show()\n\n\n\n\nLooking at the range of ages by decade, the plot above indicates that the castaways are aging over time: the media age has increased from the early 40s in the 1940s to late 50s in the 2020s. For comparison, the median age of the British population increased from 34.9 in 1950 to 40.2 in 2015.\nA great example of how the guests (and audience?) are aging is castaway Sir Cliff Richard. The pop singer first appeared on the show at the age of 20 in 1960 (favourite song: Rock Around the Clock by Bill Haley and his Comets, book: The Swiss Family Robinson by Johann Wyss, luxury: guitar) and returned for the 2020 Christmas epsiode at the age of 80 (favourite song: It Is Well by Sheila Walsh featuring Cliff Richard, book: Wuthering Heights by Emily Brontë, luxury: a Gibson acoustic guitar).\n\ncastaways_df.query(\"std_name == 'cliff richard'\")\n\n\n\n\n\n\n\n\nepisode_ref\ndate\ncastaway_ref\nname\nstd_name\ngender\nprofession\ncountry_of_citizenship\nplace_of_birth\ncountry_of_birth\ndate_of_birth\nfavTrack\nluxury\nbook\nwiki_link\nlink\nyear\n\n\n\n\n511\n2654\n1960-10-31\n484\nCliff Richard\ncliff richard\nmale\n['film actor', 'actor', 'performing artist', '...\nUnited Kingdom\nLucknow\nIndia\n1940-10-14\nRock Around The Clock by Bill Haley and His Co...\nGuitar\nThe Swiss Family Robinson - Johann Wyss\nhttps://en.wikipedia.org/wiki/Cliff_Richard\nhttps://www.bbc.co.uk/programmes/p009y6z4\n1960\n\n\n3190\n3259\n2020-12-20\n484\nSir Cliff Richard\ncliff richard\nNaN\nNaN\nNaN\nNaN\nNaN\nNaT\nNaN\nWuthering Heights¬†by¬†Emily Bront√´\nA Gibson¬†acoustic guitar\nhttps://en.wikipedia.org/wiki/Sir_Cliff_Richard\nhttps://www.bbc.co.uk/programmes/m000qhg8\n2020"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#country-of-birth",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#country-of-birth",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Country of birth",
    "text": "Country of birth\nFrom Wikipedia, we can also ask for information about the country of birth for the castaways on the show. For visual clarity, we have used the pycountry package to get the continent of each country, breaking the United Kingdom out into its own category due to the majority of castaways being born in the UK. This information is not always available, as reflected by the “other” category on the plot.\n\ndef get_continent(country):\n    \"\"\"\n    Fuzzy search for Continent based on input country, treating GB separately\n    \"\"\"\n\n    # first search for country\n    try:\n        code = pc.countries.search_fuzzy(country)[0].alpha_2\n    except:\n        code = \"\"\n\n    if code == \"\":\n        return \"Unknown\"\n\n    if code == \"GB\":\n        return \"UK\"\n\n    # next get continent code from country code\n    continent_code = pcc.country_alpha2_to_continent_code(code)\n\n    return continent_code\n\n\ncastaways_df[\"continent_of_birth\"] = castaways_df.country_of_birth.apply(get_continent)\nbirth_df = pd.pivot_table(\n    castaways_df,\n    columns=\"continent_of_birth\",\n    index=\"year\",\n    values=\"date\",\n    aggfunc=\"count\",\n)\n\nbirth_df = normalize_rows(birth_df)\nbirth_df = birth_df[[\"UK\", \"EU\", \"NA\", \"AF\", \"AS\", \"SA\", \"OC\", \"Unknown\"]]\nbirth_df = cumulative_columns(birth_df)\n\nmake_timeplot(moving_average(birth_df, n=5), ymin=0.0, leg_title=\"Country of Birth\")\n\n\n\n\nLooking as a function of time, there are no drastic changes in the country of birth of castaways on the show. The fraction of castaways who have this information missing seems to have been slightly reduced since the beginning of the show. This could reflect changes in the notability of guests, or perhaps the increasing prevalence of information on Wikipedia.\nFor a BBC radio programme, perhaps it isn’t a surprise that most castaways are from the UK, with other English-speaking countries with cultural connections (Australia, USA) being very prominent. In fact, the large number of castaways from the USA from the start of the show might make sense in the context of castaway profession."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#profession",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#profession",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Profession",
    "text": "Profession\nWe can also ask Wikipedia for information about professions that the castaways have, in order to get a better idea of which roles in society are reflected on the show.\nWe obtain from Wikipedia a list of professions of decreasing granularity for each castaway. As such, the data isn’t all comparable without some initial work. For example, Vic Oliver is at the highest level of granularity an actor (as far as Wikipedia is concerned), but he is also a performing artist, an artist, and ultimately a professional.\n\ncastaways_df.std_name.iloc[0], castaways_df.profession.iloc[0]\n\n('vic oliver', \"['actor', 'performing artist', 'artist', 'professional']\")\n\n\nSince the number of possible professions that Wikipedia has knowledge of is too large to easily digest in a plot, we need to group similar professions into broad enough categories for visualisation. Do do this, we next define some functions to cluster professions, aiming to have the most granular profession for each castaway, while still maintaining a manageable number of possible categories. These functions are a little ad hoc, and rely on some parameters that have been manually selected to achieve sensible results. A more sophisticated approach could reduce the need for this.\n\ndef reduce(prof_list, common_professions):\n    \"\"\"\n    Given a list of professions for a single castaway, try and return the most\n    granular profession for this castaway if it is common enough.\n    \"\"\"\n\n    # no options available, just take whatever profession is left\n    if len(prof_list) == 1 or \"professional\" in prof_list[1]:\n        return [prof_list[0]]\n\n    # take the most granular profession if it is common\n    for prof in prof_list:\n        if prof in common_professions and prof != \"professional\":\n            return [prof]\n\n    # if none are common, restrict the set of professions to cluster\n    else:\n        return prof_list[1:]\n\n\ndef cluster_step(profession_lists, n=10):\n    \"\"\"\n    At each step, take the n most common professions that appear globally,\n    and attempt to match castaways to one of these professions.\n    \"\"\"\n\n    # get the most common categories\n    common_professions = pd.Series(\n        [i for x in profession_lists for i in x]\n    ).value_counts()[:n]\n\n    # apply clustering\n    out = [reduce(p, common_professions) for p in profession_lists]\n\n    return out\n\n\ndef simplify_professions(profession_lists, n=25):\n    \"\"\"\n    Run the clustering process.\n    \"\"\"\n\n    # run clustering iteratively for ten steps\n    for i in range(10):\n        profession_lists = cluster_step(profession_lists)\n\n    # clean\n    professions = [p[0] for p in profession_lists]\n    least_common = pd.Series(professions).value_counts(ascending=True)[:n]\n    professions = [p if p not in least_common else \"other\" for p in professions]\n\n    return professions\n\nNow we have the clustering functions defined, we can define one additional function which does some preprocessing, runs the clustering, and makes a plot.\n\ndef get_profession_plot(df, n=20, window=10, leg_title=\"\"):\n\n    profession_df = df.copy()\n    profession_df.loc[profession_df.profession == \"[]\", \"profession\"] = np.nan\n    profession_df = profession_df.fillna({\"profession\": \"['unknown']\"})\n    profession_df.profession = profession_df.profession.apply(\n        lambda x: ast.literal_eval(x)\n    )\n    profession_df[\"profession\"] = simplify_professions(profession_df[\"profession\"], n=n)\n\n    prof_df = pd.pivot_table(\n        profession_df,\n        columns=\"profession\",\n        index=\"year\",\n        values=\"date\",\n        aggfunc=\"count\",\n    )\n\n    prof_df = prof_df[\n        [c for c in prof_df.columns if c not in [\"other\", \"unknown\"]]\n        + [\"other\", \"unknown\"]\n    ]\n    prof_df = normalize_rows(prof_df)\n    prof_df = cumulative_columns(prof_df)\n\n    make_timeplot(moving_average(prof_df, n=window), ymin=0.0, leg_title=leg_title)\n\n\nget_profession_plot(castaways_df, n=34, leg_title=\"Profession\")\n\n\n\n\nAs you can see, the professions most frequently represented on Desert Island Discs are people who work in the media and creative industries – actors, artists, journalists, musicians and writers. This makes sense, as it is an entertainment programme with a foundation of music. But we were surprised not to see more representation from the many professions that are influential in British society - politicians, athletes, academics and people involved in business, health, and law are all very small percentages of castaways.\nThere are also clear trends visible over time. In the first half of the show’s history actors and musicians made up the bulk of guests. In more recent years the proportion of castaways being grouped into the “other” category has increased, indicating an increase diversity in castaways’ occupations."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#genres",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#genres",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Genres",
    "text": "Genres\nFrom Spotify, we can get a list of genres associated with the musical artist and additional info and analysis for each song (for example, danceability). It turns out Spotify has an overwhelming number of microgenres – see Glenn McDonald’s everynoise.com for a mind blowing micro-genre experience – and we need to do some work to get a small number of the most common and most easily recognised genres. We initial had hoped to algorithmically develop a series of meta-genres to use for analysis, but this proved beyond the scope of this data story.\nTo start, we can load the discs data, and the dataset pulled from Spotify API. These are then merged.\n\n# load discs data\ndiscs_df = pd.read_excel(\n    hc_data_url, sheet_name=\"discs\", parse_dates=[\"date\"], index_col=0\n)\n\n# load spotify data\nspotify_df = pd.read_excel(\n    hc_data_url, sheet_name=\"spotify_data\", parse_dates=[\"date\"], index_col=0\n)\nspotify_df.genres_artist = spotify_df.genres_artist.apply(lambda x: ast.literal_eval(x))\nspotify_df = spotify_df.dropna(subset=[\"track\"])\n\n# combine disks and spotify data\nmerged_df = pd.merge(\n    discs_df,\n    spotify_df,\n    left_on=[\"track_name\", \"date\"],\n    right_on=[\"track_name_original\", \"date\"],\n    how=\"inner\",\n)\n\n# extract year\nmerged_df[\"year\"] = merged_df[\"date\"].dt.year\n\nNow the interesting part. After a bit of a manual look through the most common microgenres returned by Spotify, we came up with the following manual grouping.\n\nmeta_genres = {\n    \"pop\": [\"easy listening\"],\n    \"rock\": [],\n    \"folk\": [\n        \"bothy\",\n        \"celtic\",\n        \"scottish\",\n        \"bagpipe\",\n        \"irish\",\n        \"welsh\",\n        \"bluegrass\",\n        \"country\",\n    ],\n    \"jazz\": [\n        \"blues\",\n        \"british dance band\",\n        \"big band\",\n        \"ragtime\",\n        \"boogie\",\n        \"brass band\",\n    ],\n    \"classical\": [\n        \"choral\",\n        \"bolero\",\n        \"opera\",\n        \"operetta\",\n        \"orchestral\",\n        \"orchestra\",\n        \"romantic era\",\n        \"light music\",\n        \"baroque\",\n    ],\n    \"vintage\": [\n        \"adult standards\",\n        \"music hall\",\n        \"british comedy\",\n        \"broadway\",\n        \"cabaret\",\n        \"tin pan alley\",\n    ],\n    \"world\": [\n        \"italian\",\n        \"chanson\",\n        \"french\",\n        \"bossa\",\n        \"samba\",\n        \"flamenco\",\n        \"salsa\",\n        \"latin\",\n        \"african\",\n        \"zither\",\n        \"cuban rumba\",\n        \"arpa paraguaya\",\n    ],\n    \"modern\": [\n        \"disco\",\n        \"soul\",\n        \"r&b\",\n        \"reggae\",\n        \"ska\",\n        \"hip hop\",\n        \"rap\",\n        \"electronic\",\n        \"electronica\",\n        \"house\",\n        \"ambient\",\n        \"downtempo\",\n        \"garage\",\n        \"synthesizer\",\n        \"drum and bass\",\n    ],\n    \"spoken & soundtrack\": [\n        \"soundtrack\",\n        \"hollywood\",\n        \"show tunes\",\n        \"movie\",\n        \"poetry\",\n        \"comic\",\n        \"reading\",\n        \"comedy\",\n        \"oratory\",\n    ],\n}\n\n\ndef determine_genre(genre_list):\n    \"\"\"\n    Given a list of genres for a particular song, return the first\n    matched meta genre.\n    \"\"\"\n    if genre_list == []:\n        return \"\"\n    for genre in genre_list:\n        for meta_genre in meta_genres.keys():\n            search_strings = [meta_genre] + meta_genres[meta_genre]\n            if any([s in genre for s in search_strings]):\n                return meta_genre\n    return \"other\"\n\nWe can use the groups of genres defined above to turn the list of genres associated with each track into a single metagenre. Anything not captured in our very coarse categorisation goes into the “other” metagenre.\n\n# get metagenre from the genre list\nmerged_df[\"genre\"] = merged_df.genres_artist.apply(determine_genre)\n\n# drop rows without any genre\nmerged_df = merged_df[merged_df[\"genre\"] != \"\"].copy()\n\n# make a timeplot\ngenre_df = pd.pivot_table(\n    merged_df, columns=\"genre\", index=\"year\", values=\"date\", aggfunc=\"count\"\n)\ngenre_df = genre_df[[c for c in genre_df.columns if c not in [\"other\"]] + [\"other\"]]\ngenre_df = normalize_rows(genre_df)\ngenre_df = cumulative_columns(genre_df)\nmake_timeplot(moving_average(genre_df, n=5), ymin=0.0, leg_title=\"Genres\", y_title=\"\")\n\n\n\n\nThe plot above clearly illustrates the shift away from classical music, which doesn’t really start until Plomley’s reign as host ends in the mid-1980s. This trend accelerated after 2000 and rock music now represents a larger proportion of dics than classical music.\nThe “vintage” category was the second largest genre for much of the programme. Including artists like Frank Sinatra, it captures the popular music of the 1950s and now reflects more retro tastes. The “modern” category combines a number of more recent genres, from disco and ska to hip-hop and rap.\nNext we see who were the most popular artists per year. First we merge our castaway and disc datasets, in order to have all the available information in one single dataframe.\n\ntotal_df = pd.merge(\n    merged_df,\n    castaways_df,\n    left_on=[\"std_name\", \"date\", \"episode_ref_x\", \"castaway_ref\"],\n    right_on=[\"std_name\", \"date\", \"episode_ref\", \"castaway_ref\"],\n    how=\"inner\",\n)\n\n\n# choose only after 1950, before data looks weird given few entries and group by year\ndf1_grouped = total_df[total_df[\"year_x\"] &gt; 1949].groupby(\"year_x\")\n\ndates = []\nlevels = []\nnames = []\n\nfor group_name, df_group in df1_grouped:\n\n    # year\n    dates.append(group_name)\n\n    # percentage of times it is chosen\n    levels.append(\n        df_group[\"std_artist\"].value_counts()[:1].values[0]\n        / df_group[\"std_artist\"].shape[0]\n    )\n\n    # artist chosen (last name)\n    names.append(df_group[\"std_artist\"].value_counts()[:1].index[0].split(\" \")[-1])\n\n\n# associating names to colours and order them chronologically\nnames_cronological = list(dict.fromkeys(names))\nc = sns.color_palette(\"Spectral\", max(4, len(np.unique(names_cronological))))\ncolours = {g: c[i] for i, g in enumerate(names_cronological)}\n\n# Create figure and bar plot\nfig, ax = plt.subplots(figsize=(20, 5))\nax.set_title(\"Most popular artists by year\", fontsize=20)\n\nax.bar(dates, levels, 1, color=[colours[x] for x in names])\n\nlabels = list(colours.keys())\nhandles = [plt.Rectangle((0, 0), 1, 1, color=colours[x]) for x in names_cronological]\nax.legend(handles, labels, facecolor=\"white\", ncol=2, fontsize=14)\nax.grid(False)\n\n# remove y axis and spines\nplt.ylabel(\"Percentage of times chosen by Castaways\", fontsize=12)\n\nax.margins(y=0.1)\nplt.show()\n\n\n\n\nThe most popular artist by year parallels the overall look at genres. Classical composers dominate until the early 2000s, with a classical grudge match between Bach, Beethoven and Mozart. Over the past two decades, The Beatles consistently top the charts, joined by Frank Sinatra, Bob Marley, David Bowie, Joni Mitchell, and Nina Simone."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#network-analysis",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#network-analysis",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Network analysis",
    "text": "Network analysis\nAn interesting way of looking at this data is to think of it as a network. You can have two kind of networks: one where the castaways are the nodes and they are connected by having chosen the same musical artist, and the other one is where the musical artists are the nodes and their connections are defined by how many castaways chose them together.\nA network analysis can give us an idea of castaway/artist similarity and popularity, as well as uncovering some non-trivial patterns (in the best of cases).\nLet’s start building a network for our castaways. We will use our castaway and disc merged dataset, in order to have all the available information in one single dataframe.\nSome of our castaways have appeared in more than one episode. Let’s check who:\n\n# if the is a person with more than one episode keep indexes of the last episode only,\n# given that our dataset is sorted we do this by only keeping the last entry\nlist_index = []\ncount_repeats = 0\nfor name in np.unique(total_df[[\"std_name\"]]):\n\n    castaway = total_df[total_df[\"std_name\"] == name]\n\n    dates = np.unique(castaway[\"date\"].values)\n\n    if len(dates) &gt; 1:\n        count_repeats = count_repeats + 1\n        if len(dates) &gt; 3:\n            print(name, \":\", [str(d)[:10] for d in dates])\n\n    # saving the index of the latest episode avalaible for each cast away\n    index = castaway[(castaway[\"date\"] == dates[-1])].index.values\n\n    list_index = [*list_index, *index]\n\nprint(\n    count_repeats,\n    \" recurrent guests out of\",\n    len(np.unique(total_df[[\"castaway_ref\"]])),\n)\n\narthur askey : ['1942-04-02', '1955-04-21', '1968-12-23', '1980-12-26']\ndavid attenborough : ['1957-05-06', '1979-03-13', '1998-12-27', '2012-02-03']\n202  recurrent guests out of 2450\n\n\nAs you can see above, both David Attenborough and was Arthur Askey were invited to the show four times!\nHaving repeated guests in our dataset can be problematic for building the network, as these castaways will have more connections than the ones that have been invited only once. To resolve this problem, let’s make sure we only have one episode per castaway, by selecting the most recent episode available.\nThis count of total and repeat castaways is a little less than at the top of this notebook. As we are using a dataframe that has been merged with other data sources (Wikpedia, Spotify) we have lost some castaways where there was missing information. This is discussed in more detail in the conclusions.\n\n# select only one episode per cast away using the indexes stored above\ndf_network = total_df.iloc[list_index]\ndf_network.shape\n\n(12857, 53)"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#castaway-network",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#castaway-network",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Castaway network",
    "text": "Castaway network\nLet’s now build the castaway network. For this we have written the function below: for each available pair of nodes (castaways in this case) it counts how many connections it has (common artists). This is typically called an edge list that defines a source and target (nodes), and it gives it a weight (edge).\n\ndef build_network(df, nodes_variable, edges_variable):\n\n    \"\"\"Function that builds a newtork based on given nodes or edges that are\n    columns names on the input dataframe\"\"\"\n\n    # loop over all the unique values of the edges columns\n    df_list = []\n    for edge in np.unique(df[edges_variable].astype(str).values):\n\n        # get the all the nodes that have that edge\n        nodes = np.unique(\n            df[df[edges_variable] == edge][nodes_variable].dropna().values\n        )\n\n        # get all combination of nodes existing for that edge\n        # and save it to a list where source and\n        # targes are the nodes conected by that edge\n        if len(nodes) &gt; 1:\n            source, target = zip(*itertools.combinations(nodes, 2))\n\n            df_net = pd.DataFrame()\n            df_net[\"source\"] = source\n            df_net[\"target\"] = target\n\n            df_list.append(df_net)\n\n        else:\n            continue\n\n    # turns list to a dataframe and aggregate repeated rows\n    # (source and targets can appear several times)\n    # weight is the number of times that appears\n    network = pd.concat(df_list)\n    network = network[network[\"source\"] != network[\"target\"]]\n    df_network = (\n        network.groupby([\"source\", \"target\"])\n        .size()\n        .reset_index(name=\"weight\")\n        .sort_values(by=\"weight\", ascending=False)\n    )\n\n    return df_network\n\n\ndf_castaway_network = build_network(df_network, \"std_name\", \"std_artist\")\n\n# most conected pair of people\ndf_castaway_network[:10]\n\n\n\n\n\n\n\n\nsource\ntarget\nweight\n\n\n\n\n209492\ndenis matthews\nitzhak perlman\n6\n\n\n36398\nandor foldes\nhephzibah menuhin\n6\n\n\n297911\ngeorge cansdale\nrowland emett\n5\n\n\n522899\nvernon scannell\nvladimir ashkenazy\n5\n\n\n209556\ndenis matthews\njeremy thorpe\n5\n\n\n36724\nandor foldes\nmitsuko uchida\n5\n\n\n209389\ndenis matthews\ngerald moore\n5\n\n\n36108\nandor foldes\ncatherine gaskin\n5\n\n\n285657\nfred hoyle\nvladimir ashkenazy\n5\n\n\n209425\ndenis matthews\nhardy amies\n5\n\n\n\n\n\n\n\nTo understand what the table above describes, we can look at the choices of the top two pairs, in which the castaways are connected by some of the usual suspects of classical music. Pianist Denis Matthews (1967) and violinist Itzhak Perlman (1978) both went with Bach, Beethoven, Brahms, Elgar, Mozart and Schubert. Hephzibah Menuhin (1958) and Andor Foldes (1959) - both pianists - included Bach, Bartók, Beethoven, Brahms, Mozart and Schubert in their lists.\nNow we can feed this data to a network library to build our network.\n\nG = nx.from_pandas_edgelist(df_castaway_network, edge_attr=[\"weight\"])\nprint(nx.info(G))\n\nGraph with 2447 nodes and 523560 edges\n\n\nNetworks have interesting metrics that can help us understand our dataset better. For example, we have the node degree metric, that tells us the number of edges connected to a given node. In our case, the node degree of a castaway will tell us how popular or rare their taste in music is.\nLet’s explore who have the highest and lowest node degrees in our network.\n\n# castaways with more degrees (more artist in common with others)\nnode_degrees = G.degree()\ndegree_sequence = sorted([d for n, d in node_degrees], reverse=True)  # degree sequence\ndegreeCount = collections.Counter(degree_sequence)\ndeg, cnt = zip(*degreeCount.items())\n\nsorted(node_degrees, key=lambda x: x[1], reverse=True)[:10]\n\n[('denis matthews', 1345),\n ('vernon scannell', 1243),\n ('hardy amies', 1220),\n ('itzhak perlman', 1206),\n ('bernard levin', 1205),\n ('claudio abbado', 1205),\n ('jeremy thorpe', 1191),\n ('alan alda', 1184),\n ('john updike', 1170),\n ('c day lewis', 1168)]\n\n\nIn the top ten, we’ve got three writers, two musicians, a conductor, an actor, a politician, a designer, and a cartoonist/sculptor. Eight of them appeared on the show before 1990, and chose almost exclusively classical music. The two guests from the 1990s - Alan Alda and John Updike – chose music spanning classical, jazz, vintage and rock.\n\n# castaways with less degrees (the ones that chose niche stuff)\nsorted(node_degrees, key=lambda x: x[1])[:10]\n\n[('joan turner', 1),\n ('jack hylton', 1),\n ('dustin hoffman', 1),\n ('jamie cullum', 2),\n ('fenella fielding', 3),\n ('kimberley motley', 4),\n ('m m kaye', 5),\n ('vittorio radice', 5),\n ('roberto alagna', 5),\n ('courtney pine', 5)]\n\n\nLooking at the three castaways with the lowest node degrees, we’ve got actor Dustin Hoffman (2012), jazz band leader Jack Hylton (1942), and comedian and singer Joan Turner (1988). Both Hylton and Turner chose their own songs, which might explain their position. For Hoffman, his ranking appears to be as a result of particularly esoteric tastes, picking lesser known artists from jazz, rockabilly and film soundtracks."
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#artists-network",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#artists-network",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Artists network",
    "text": "Artists network\nLet’s now explore how a network where the nodes are the artists and edges are weighted by the number of castaways that chose them together.\n\n# artist more likely to be chosen toguether\ndf_artists_network = build_network(df_network, \"std_artist\", \"std_name\")\ndf_artists_network[:10]\n\n\n\n\n\n\n\n\nsource\ntarget\nweight\n\n\n\n\n16590\nludwig van beethoven\nwolfgang amadeus mozart\n162\n\n\n14441\njohann sebastian bach\nwolfgang amadeus mozart\n161\n\n\n14247\njohann sebastian bach\nludwig van beethoven\n148\n\n\n10607\nfranz schubert\nwolfgang amadeus mozart\n84\n\n\n10521\nfranz schubert\nludwig van beethoven\n75\n\n\n10491\nfranz schubert\njohann sebastian bach\n72\n\n\n14568\njohannes brahms\nludwig van beethoven\n62\n\n\n14651\njohannes brahms\nwolfgang amadeus mozart\n62\n\n\n14192\njohann sebastian bach\njohannes brahms\n60\n\n\n8276\nedward elgar\nludwig van beethoven\n51\n\n\n\n\n\n\n\nThis list comes as no surprise, as we’ve got another “who’s who” of classical music. In fact, the first nine are simply the various possible combinations of Bach, Beethoven, Brahms, Mozart and Schubert.\n\ndf_artists_network[-10:-1]\n\n\n\n\n\n\n\n\nsource\ntarget\nweight\n\n\n\n\n6899\ndella reese\njohannes brahms\n1\n\n\n6900\ndella reese\npeggy lee\n1\n\n\n6901\ndella reese\nthe beatles\n1\n\n\n6902\ndella reese\nthe modern jazz quartet\n1\n\n\n6903\ndenim\nirma thomas\n1\n\n\n6904\ndenim\npixies\n1\n\n\n6905\ndenim\nradiohead\n1\n\n\n6906\ndenim\nthe beatles\n1\n\n\n6907\ndenis king\nneil diamond\n1\n\n\n\n\n\n\n\nThis list of the bottom ten is less illuminating. Moving down the long tail of more obscure musical artists, we expect there will be lots of pairings that only happen once. For example, in this list is the lesser known Birmingham indie rock band Denim, which was chosen by Charlie Brooker (2018), the satirist and creator of Black Mirror. His unique musical tastes were captured in the BBC website summary of the episode:\n\nAnd for his final track, he selects The New Potatoes by Denim, a “wilfully ridiculous”, high-pitched electronic song that Charlie’s children find as funny as he does. It’s also the disc he would keep over all the others: “There is something darkly amusing about the thought of being stranded there [on the desert island] and the only form of music that you have is that.”\n\nDenim appearing on Desert Island Discs this one time was worthy of including on their somewhat slim Wikipedia entry, so it would be surprising to see them with many more pairings. Also, the reason there are only four pairings for Denin is that the other three picks by Brooker were so random that they weren’t found on Spotify (including a song from the children’s programme the Magic Roundabout and the Gameboy theme for Robocop).\nNow we can feed this data to a network library to build our network.\n\nG_artists = nx.from_pandas_edgelist(df_artists_network, edge_attr=[\"weight\"])\nprint(nx.info(G_artists))\n\nGraph with 2088 nodes and 19717 edges\n\n\nLet’s take a look at the the node degree of our metric.\n\n# get the artist that get more chosen with others\nnode_degrees = G_artists.degree()\ndegree_sequence = sorted([d for n, d in node_degrees], reverse=True)  # degree sequence\ndegreeCount = collections.Counter(degree_sequence)\ndeg, cnt = zip(*degreeCount.items())\n\n\ndef artist_bar_plot(artist_names,artist_centrality,genres_central,genre_order, title,ylabel):\n    \n    \"\"\"Function to make a bar plot of artists based on a metric (node degree or centrality) \"\"\"\n\n\n    # Create figure and bar plot\n    fig, ax = plt.subplots(figsize=(24, 6))\n    ax.set_title(title, fontsize=20)\n\n    c = sns.color_palette(\"Spectral\", max(4, len(np.unique(genre_order))))\n    colours = {g: c[i] for i, g in enumerate(genre_order)}\n\n    ax.bar([a.split(\" \")[-1] for  a in artist_names], artist_centrality, 1, color=[colours[x] for x in genres_central])\n\n    labels = list(colours.keys())\n    handles = [plt.Rectangle((0, 0), 1, 1, color=colours[x]) for x in genre_order]\n    ax.legend(handles, labels, facecolor=\"white\", fontsize=14)\n\n    # remove y axis and spines\n    plt.ylabel(ylabel, fontsize=14)\n    plt.xticks(ha='right', rotation=55, fontsize=14)\n\n    ax.margins(y=0.1)\n    plt.show()\n\n\n# get info out of dictionary\nartist_dictionary = sorted(node_degrees, key=lambda x: x[1], reverse=True)[:30]\n\nartist_names = [artist_dictionary[i][0] for i in range(0,len(artist_dictionary))]\nartist_degree = [artist_dictionary[i][1] for i in range(0,len(artist_dictionary))]\n\n# get genres for the artists and\ngenres_central = [total_df[total_df['std_artist']==artist]['genre'].iloc[0] for artist in artist_names]\n    \ngenre_order = list(dict.fromkeys(genres_central))\n\nartist_bar_plot(artist_names,artist_degree,genres_central,genre_order,\"Most popular artists\",\"Artist degree\")\n\n\n\n\nLooking at the artists with the highest node degrees: in addition to some familiar classical composers, we’ve got the Beatles (the most popular artists for much of the past two decades) and Frank Sinatra (the king of our vintage category).\nNow we can look at the artists with less node degree, or the ones less chosen in our dataset.\n\nsorted(node_degrees, key=lambda x: x[1])[:10]\n\n[('gerard philipe', 1),\n ('lisa kirk', 1),\n ('josef suk', 1),\n ('teddy wilson', 1),\n ('nellie lutcher', 1),\n ('black dyke band', 1),\n ('arthur askey', 1),\n ('morgana king', 1),\n ('iarla o lionaird', 2),\n ('shankar mahadevan', 2)]\n\n\nWith the lowest node degrees, one performer that is worthy of note is Arthur Askey, an actor/comedian who appeared as a castaway on the show four times. On two of those appearances, he picked his own comedy routines from the films The Proposal and Band Waggon. Two other castaways included his musical comedy – actress Pat Kirkwood picked his playful song Worm in 1942 and actor John Mills picked his WWII parody Adolf in 2000 – but that wasn’t enough to increase his node degrees above 1.\nAnother interesting network analysis metric is node centrality, which assigns numbers or rankings to nodes within a graph corresponding to their network position. Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. We can investigate in our case which artists are the more central of our network.\n\n# the more \"central artists\" (interesting results!)\n\nbet_cen = nx.betweenness_centrality(G_artists)\n\n\ndef get_top_keys(dictionary, top): \n    sorted_list = sorted(dictionary.items(), key=lambda item: item[1],reverse=True) \n    return sorted_list[:top]\n    \n# top ten more central artists\ntop_bet_cen = get_top_keys(bet_cen, 10)\n\n\n# get info out of dictionary\nartist_dictionary = top_bet_cen\n\nartist_names = [artist_dictionary[i][0] for i in range(0,len(artist_dictionary))]\nartist_centrality = [artist_dictionary[i][1] for i in range(0,len(artist_dictionary))]\n\n# get genres for the artists and\ngenres_central = [total_df[total_df['std_artist']==artist]['genre'].iloc[0] for artist in artist_names]\n    \nartist_bar_plot(artist_names,artist_centrality,genres_central,genre_order,\"Most central artists\",\"Artist network centrality\")\n\n\n\n\nIn terms of betweenness, Mozart tops the classical composers, the Beatles come second, and Sinatra, Dylan and Bowie make the top ten.\nThe network analysis we have presented in this story is a very simple and naive one, a lot more could be explored using these kinds of techniques. For example, we could use community detection algorithms to explore the network structure and group artists or castaways together, also we could look at the time evolution of the networks to better understand how the cultural landscape of the UK has changed in the last 80 years. We invite the reader to investigate these ideas further!"
  },
  {
    "objectID": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#invited-back-to-the-island",
    "href": "stories/2022-03-09-Desert-Island-Discs/2022-03-09-Desert-Island-Discs.html#invited-back-to-the-island",
    "title": "Desert Island Discs - Famous people and their musical tastes",
    "section": "Invited back to the island",
    "text": "Invited back to the island\nAs a listener, each Friday and Sunday morning we are invited back to the island to hear someone’s life story through music. But there are some very special guests who are invited back more than once. We close this story by looking at environmentalist and broadcaster David Attenborough’s four episodes.\nAs a fresh 30-year-old TV presenter for his first appearance with Roy Plomley (06/05/1957), his first disc was Trouble in Mind by Northern Irish blues singer Ottilie Patterson and Chris Barber Jazz Band. When he returned at the age of 52 (10/03/1979), he requested to take the book Shifts and Expedients of Camp Life by William Barry Lord to the island, which he consistently requested for his following two appearances. Sue Lawley welcomed back a 72-year-old Attenborough (25/12/1998) who asked for the luxury of a guitar, having previously taken a piano and binoculars to the island. And finally, for the 70th Anniversary Episode (29/01/2012), Kirsty Young invited him back again, now 85, where the track he saved from the waves was the 3rd of Johann Sebastian Bach’s Goldberg Variations.\nWith any luck, Lauren Laverne will invite Sir David back again soon, at which point he will have the record for the most episodes all to himself. As he reflects once more on his life and favourite songs, perhaps he will talk about his evolution into a trusted voice on climate action. Hopefully, the rising seas of climate change don’t threaten this island haven of music and autobiography."
  },
  {
    "objectID": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html",
    "href": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html",
    "title": "Unveiling London’s mobility patterns with Boris Bikes",
    "section": "",
    "text": "Ryan Chan\nDavid Llewellyn-Jones"
  },
  {
    "objectID": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#table-of-contents",
    "href": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#table-of-contents",
    "title": "Unveiling London’s mobility patterns with Boris Bikes",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nData description\nStatistics on bike usage\nA day in the life of a bike\nBike mobility patterns\nConclusions\n\nBefore we get to actual work, in the below cell we import all the Python modules we will need in the course of this story, and set a couple of global constants related to e.g. plotting. In addition to running pip install -r requirements.txt to install all these dependencies, you may find that you need to install some binary libraries that the geospatial packages depend on. On the authors’ Macs brew install gdal does the trick.\n\nimport datetime\nimport json\nimport os\nimport pickle\nimport re\nimport urllib\n\nimport community\nimport contextily as cx\nimport folium\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport movingpandas as mpd\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport pyproj\nimport requests as requests\nimport seaborn as sns\nfrom folium.plugins import TimestampedGeoJson\nfrom geopandas import GeoDataFrame, points_from_xy\nfrom shapely.geometry import Point\nfrom tqdm.auto import tqdm\n\n# Set the default figure size\nplt.rcParams[\"figure.figsize\"] = (10, 6)\n\n# This sets the environment variable PROJ_LIB to the directory containing the data files\n# needed by the pyproj library. This allows the pyproj library to find and access the\n# data files it needs.\n\nos.environ[\"PROJ_LIB\"] = pyproj.datadir.get_data_dir()\n\nThe story makes use of the Cycle Streets API, which in turn uses OpenStreetMap map data."
  },
  {
    "objectID": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#data-processing-and-cleaning",
    "href": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#data-processing-and-cleaning",
    "title": "Unveiling London’s mobility patterns with Boris Bikes",
    "section": "Data processing and cleaning",
    "text": "Data processing and cleaning\nThe data covers a wide time range, and the format has seen several small changes in that period. Consequently having a single table with all the journey data requires a bit of cleaning, harmonising, and general data-wrangling. Some examples of the kinds of things one needs to deal with are: * Data files are provided at the weekly level and need to be downloaded programatically from the TfL web page, * Most of the data comes in CSV files, but some time periods are in Excel spreadsheets, * Station name (string) to station ID (integer) mappings are many-to-many: The same station often has slightly differently spelled names in different files, sometimes the same station has gone by several IDs, and sometimes the same ID has been used for entirely different stations at different times, * Station IDs are integers, except for one (there’s always one isn’t there?) that is a string, * Some rows have missing data, * Columns in CSV files have changed names a few times.\nDealing with all this is a boring affair and can deviate from the purpose of this data story, and therefore we have decided to do the cleaning elsewhere. However, if you enjoy boredom, you can find our data-cleaning script here.\nIn the end, we have around 100M bicycle journeys of cleaned data (we had to drop a negligibly small number of rows that were missing/malformatted beyond repair), all in one pandas DataFrame, that is, thankfully, at around 7 gigabytes just small enough to handle in memory on a modern laptop.\nIn addition to bike journeys, we also make use of another data set the TfL provides, that has coordinates for the locations of all stations.\nWe have uploaded all the necessary data sets for this story onto Zenodo . The below cell downloads the cleaned data into a subfolder called data.\n\n# This downloads the data from Zenodo if it is not already present in the data folder\n# The ! syntax is a IPython Notebook specialty, that allows running shell commands\n# in the middle of a Python control structure.\nif not os.path.exists(\"data\"):\n    !mkdir data\nif not os.path.exists(\"data/BorisBikes_journeys_cleaned_data.pickle\"):\n    !wget https://zenodo.org/record/7509993/files/BorisBikes_journeys_cleaned_data.pickle -P data/\nif not os.path.exists(\"data/BorisBikes_station_names.pickle\"):\n    !wget https://zenodo.org/record/7509993/files/BorisBikes_station_names.pickle -P data/\nif not os.path.exists(\"data/BorisBikes_stations_coordinates.json\"):\n    !wget https://zenodo.org/record/7509993/files/BorisBikes_stations_coordinates.json -P data/"
  },
  {
    "objectID": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#load-processed-data",
    "href": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#load-processed-data",
    "title": "Unveiling London’s mobility patterns with Boris Bikes",
    "section": "Load processed data",
    "text": "Load processed data\nLet’s load the data into a pandas DataFrame and take a look at it.\n\nRAW = \"data/BorisBikes_journeys_cleaned_data.pickle\"\nLOCATION_REF = \"data/BorisBikes_stations_coordinates.json\"\n\nstation_locations_df = pd.read_json(LOCATION_REF).T\nstation_locations_df.head()\n\n\n\n\n\n\n\n\nlat\nlon\n\n\n\n\n85\n51.500647\n-0.078600\n\n\n86\n51.489479\n-0.115156\n\n\n87\n51.516468\n-0.079684\n\n\n88\n51.518587\n-0.132053\n\n\n89\n51.526250\n-0.123509\n\n\n\n\n\n\n\n\n# load raw data\ndf = pd.read_pickle(RAW)\nlen(df)\n\n97656969\n\n\nAs mentioned above we have around 100 million rows.\nSome trips have bogus dates from before the scheme started, or from the future. We drop those rows from our data set. We do allow for journeys that have no end date, since there’s quite a lot of them, and presumably they could mark bikes that were lost or broke down.\n\nEARLIEST_DATE = datetime.datetime(2010, 1, 1)\n# filter out of range dates\ndf = df[df[\"start_date\"] &gt; EARLIEST_DATE]\n# allow NA for end dates\ndf = df[(df[\"end_date\"] &gt; EARLIEST_DATE) | df[\"end_date\"].isna()]\n# also drop entries where start date is after the end date\ndf = df[df[\"start_date\"] &lt; df[\"end_date\"]]\n# recalc duration\ndf[\"duration\"] = df[\"end_date\"] - df[\"start_date\"]\n\nWe still have the vast majority of our data left after this:\n\nlen(df)\n\n97120201\n\n\nThe last journey date observed in our data set is the following:\n\nmax(df[\"end_date\"])\n\nTimestamp('2022-01-04 23:59:00')"
  },
  {
    "objectID": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#long-lived-bikes",
    "href": "stories/2022-10-06-Boris-Bikes/2023-07-07-Boris-Bikes.html#long-lived-bikes",
    "title": "Unveiling London’s mobility patterns with Boris Bikes",
    "section": "Long-lived bikes",
    "text": "Long-lived bikes\nWe can use this distribution to calculate the average lifetime of a bike. We define as lifetime the time period in which a bike is present (has recorded trips) in our data set.\nTo do this, for a given bike we find the earliest start date and the latest end date of the bike available in the data set.\n\nbike_start = bike_groups[\"start_date\"].first()\nbike_end = bike_groups[\"end_date\"].last()\nbike_lifetime = bike_end - bike_start\n\nThe distribution of the lifetimes in days is the following:\n\nfig, ax = plt.subplots()\nax = bike_lifetime.dt.days.hist(bins=50)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Bikes\")\nplt.title(\"Lifetime of a bike\")\nplt.show()\n\n\n\n\nThe distribution looks quite flat, with a large peak on around ~3600 days. These are very likely bikes that have been part of the scheme from the beginning.\nNow, let’s see what the average utilisation of a bike is. By this we mean the total ride duration divided by its total lifetime.\n\nduration_sums = bike_groups[\"duration\"].sum()\nbike_utilisation = duration_sums / bike_lifetime\n\n\nfig, ax = plt.subplots()\nax = bike_utilisation.hist(bins=500)\nplt.xlim([0, 0.15])\nplt.xlabel(\"Fraction of time in use\")\nplt.ylabel(\"Bikes\")\nplt.title(\"Utilisation\")\nplt.show()\n\n\n\n\nThe distribution above shows that during its lifetime an average bike gets used around 45 minutes per day.\nWe must note the caveat that the lifetime definition does not consider the time bikes are in maintenance or not in service for any given reason. As a consequence this utilisation metric may be an underestimate, because it counts any downtime as the bike being available but not in use.\n\nTime series of bike usage\nThe number of bikes and stations has evolved since the beginning of the scheme, with more being added every year. Furthermore, London has changed since 2010 and bike usage might have changed with it. Hence we’ll take a look at the number of bikes and stations available and bike utilisation as functions of time.\nOur utilisation measure here will be slightly different to our previous figure. Previously we looked at the utilisation at the bike level and averaged this. Now, we’re looking at sum of use over the entire fleet and dividing this by the max possible usage per month (all bikes in use 24/7).\nLet’s start by creating a monthly index for the time series:\n\n# don't want to incude first and last months as may be incomplete, use in filter later\nincomplete_months = df[\"start_date\"].iloc[[0, -1]].dt.to_period(\"M\")\n# create a complete monthly index that covers ALL months in period\ncomplete_monthly_index = pd.date_range(\n    start=df[\"start_date\"].iloc[0], end=df[\"end_date\"].iloc[-1], freq=\"M\"\n).to_period(\"M\")\n\nNext we create a function that counts how many bikes or stations are in use in a given month.\n\ndef calc_alive_per_month(starts, ends, incomplete_months, complete_monthly_index):\n    starts_per_month = starts.dt.to_period(\"M\").value_counts()\n    ends_per_month = ends.dt.to_period(\"M\").value_counts()\n\n    counts_df = (\n        complete_monthly_index.to_frame(name=\"foo\")\n        .join(starts_per_month)\n        .join(ends_per_month)\n        .sort_index()\n        .fillna(0)\n    )\n    # ending items should only be counted at the start of next month, so shift\n    counts_df[\"end_date\"] = counts_df[\"end_date\"].shift(fill_value=0)\n\n    alive_per_month = counts_df[\"start_date\"].cumsum() - counts_df[\"end_date\"].cumsum()\n    return alive_per_month[~alive_per_month.index.isin(incomplete_months)]\n\nUsing the function defined above we create a dataframe of bikes available at the monthly level.\n\nalive_bikes_per_month = calc_alive_per_month(\n    starts=bike_start,\n    ends=bike_end,\n    incomplete_months=incomplete_months,\n    complete_monthly_index=complete_monthly_index,\n)\n\nWe are also interested in seeing the total usage of the bikes per month in terms of the sum of the duration of each journey and the total utilisation.\n\nduration_sums_per_month = (\n    df[[\"duration\"]].groupby(df[\"start_date\"].dt.to_period(\"M\"))[\"duration\"].sum()\n)\nduration_sums_per_month = duration_sums_per_month.to_frame()\nduration_sums_per_month[\"max_possible_duration\"] = duration_sums_per_month.index.map(\n    lambda x: x.end_time - x.start_time\n)\nutilisation_per_month = (\n    duration_sums_per_month[\"duration\"]\n    / duration_sums_per_month[\"max_possible_duration\"]\n    / alive_bikes_per_month\n)\n# remove incomplete months\nutilisation_per_month = utilisation_per_month[\n    ~utilisation_per_month.index.isin(incomplete_months)\n]\n\nLet’s also look at how many stations are available in each month.\n\nstation_groups = df.groupby(\"start_station_id\")\n# relies on time ordering of df via rental_id\nstation_start = station_groups[\"start_date\"].first()\nstation_end = station_groups[\"end_date\"].last()\n\n\nalive_stations_per_month = calc_alive_per_month(\n    starts=station_start,\n    ends=station_end,\n    incomplete_months=incomplete_months,\n    complete_monthly_index=complete_monthly_index,\n)\n\nLet’s merge our bike usage dataframe with our new station usage data. This results in a monthly indexed dataframe that we can use for visualisation later on:\n\n# forward fill gaps\nstats_df = (\n    complete_monthly_index.to_frame(name=\"date\")\n    .join(alive_bikes_per_month.rename(\"alive_bikes\"))\n    .join(alive_stations_per_month.rename(\"alive_stations\"))\n    .join(utilisation_per_month.rename(\"utilisation\"))\n    .fillna(method=\"ffill\")\n)\n\n\nstats_df.head()\n\n\n\n\n\n\n\n\ndate\nalive_bikes\nalive_stations\nutilisation\n\n\n\n\n2012-01\n2012-01\nNaN\nNaN\nNaN\n\n\n2012-02\n2012-02\n6592.0\n521.0\n0.030871\n\n\n2012-03\n2012-03\n8659.0\n560.0\n0.046002\n\n\n2012-04\n2012-04\n8663.0\n567.0\n0.036303\n\n\n2012-05\n2012-05\n8813.0\n569.0\n0.050691\n\n\n\n\n\n\n\nLet’s look at our time series from March 2012.\n\nstats_df[1:].plot.area(subplots=True)\nplt.xlabel(\"Years\")\nplt.show()\n\n\n\n\nWe can see clearly the expansion of the scheme in 2012 where new stations were added to include East London. Another prominent feature is the annual periodicity, where bike utilisation is, unsurprisingly, higher in the summer. It ranges from roughly ~2% in most winters to 5-6% in the summers, so at any given moment, on average something between 1-in-20 to 1-in-50 bikes are being ridden.\nThe effect of the pandemic in 2020 is also evident in data as a spike in utilisation in 2020, which generally seems to have slowly decreased after the excitement of the first couple of years of the scheme. Note that the number of “alive” (in use) bikes also has a temporary spike in the pandemic, which we guess is the TfL responding to increased demand."
  }
]