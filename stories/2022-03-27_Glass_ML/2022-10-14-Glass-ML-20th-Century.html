<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joe Cerniglia">
<meta name="dcterms.date" content="2022-10-14">
<meta name="keywords" content="Amelia Earhart, Fred Noonan, aviation mystery, classification, machine learning in social sciences, SMOTE, covariate drift, Random Forest">

<title>Turing Data Stories - Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Turing Data Stories</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alan-turing-institute/turingdatastories" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Machine Learning for the 20th century - Artifact Classification From an Aviation Mystery</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.linkedin.com/in/joe-cerniglia-81079839/">Joe Cerniglia</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 14, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="abstract-title">Abstract</div>
      Can a United Kingdom database from 1987 classify a glass cosmetics jar that might have belonged to Amelia Earhart? (Photograph credit ID 98648556. Copyright Mickem, Dreamstime.com.)
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="reviewers" class="level3">
<h3 class="anchored" data-anchor-id="reviewers">Reviewers:</h3>
<ul>
<li><strong>Jennifer Ding,</strong> Research Application Manager, The Alan Turing Institute</li>
<li><strong>Eirini Zormpa,</strong> Community Manager of Open Collaboration, The Alan Turing Institute</li>
</ul>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In the summer of 2010, researchers from The International Group for Historic Aircraft Recovery excavated, in fragments, a small semi-opaque cosmetic jar from the Pacific island of Nikumaroro. They were searching for evidence to support their hypothesis that Amelia Earhart and her navigator Fred Noonan perished there in 1937, after very nearly succeeding in their pathbreaking mission to circumnavigate the globe by air at its widest point, the equator.</p>
<p>My interest in glass artifacts from the island ultimately led to a personal quest to become a resident glass expert of the team. In partnership with archaeologists Thomas King and Bill Lockhart, and chemist Greg George, we produced a <a href="https://www.academia.edu/40823470/A_Freckle_In_Time_or_a_Fly_in_the_Ointment">paper</a> that summarized our findings. By then, our work on the jar had already generated some <a href="https://www.nbcnews.com/id/wbna47623025">press</a>.</p>
<p>As we continued our research in 2013, Greg George and I worked with EAG Laboratories to determine the chemical composition of the Nikumaroro jar and a sibling glass jar, used as a control, that I had purchased on eBay and had named the clear facsimile. While the two jars were similar in most respects, interestingly, the clear facsimile jar was transparent, while the Nikumaroro jar was semi-opaque. Both jars were manufactured by the Hazel-Atlas Glass Company of Wheeling, West Virginia, but EAG Laboratories’ <a href="https://tighar.org/Projects/Earhart/Archives/Research/ResearchPapers/freckleintime/Document_02_Facsimileclearjarreport.pdf">ICP-MS analysis</a> revealed them to have considerably different chemical profiles.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>In the social sciences, “data” often mean a collection of facts. In industry, however, we often consider data to be a collection of numbers and vectors, and we use computational tools such as statistics and machine learning to aid in their analysis. As part of an eCornell machine learning course, in 2022 I had the opportunity to revisit my work on the Nikumaroro jar as a learning exercise, and to use the data returned by EAG Laboratories to attempt to solve a novel machine learning classification problem I had proposed.</p>
<p>Thus was my interest in analyzing the Nikumaroro jar and comparing it with its sibling rekindled. Instead of using the jars’ data as a collection of facts, I now wanted to use them to study machine learning classification.</p>
<p>By analyzing the weight percent of the chemical elements of which each jar’s glass was made, plus refractive index, machine learning can build a model that will predict a pre-defined class to which it believes a given glass sample belongs. We may then assess the skill of the model by comparing all predictions to the actual correct classes for each sample. The correct class of the Nikumaroro jar and of the clear facsimile is that of a <strong>container.</strong></p>
<p>Two sets of measurements for the purposes of machine learning are not of much value, however, because machine learning models require a full range of known examples upon which to ‘train’ a model to recognize examples it has never seen before.</p>
<p>EAG Laboratories had been thorough, and for the purposes of our 2013 analysis, their data was ample enough. For the purposes of machine learning, however, the data, while eagerly anticipated by our team, was paltry. Where would I find the data upon which to train a machine learning model? I did not need to search very far for my answer. British forensic scientists <code>Ian W. Evett</code> and <code>Ernest J. Spiehler</code> assembled a database of glass samples in 1987 for the purposes of solving crimes, such as break-ins. Their paper was first presented at the 1987 conference of the KBS (Knowledge-Based Systems) in Goverment and is titled: <br><br>“Rule Induction in Forensic Science.” http://gpbib.cs.ucl.ac.uk/gp-html/evett_1987_rifs.html. <br><br>Their database is available at the University of California Machine Learning Repository <a href="http://archive.ics.uci.edu/ml/datasets/Glass+Identification">here</a>.</p>
<p>The weight percent of eight chemical elements and refractive index comprise the features of their database and each sample is represented by a target variable called Type. The types of glass represented in the sample are: Window Float, Window Non-Float, Vehicle Float, Container, Tableware, and Headlamp. There are no Vehicle Non-Float types represented in the data.</p>
<p>My two glass samples, as luck would have it, were independently measured for most of the same elements as found in the U.K. database, with a few caveats:</p>
<ul>
<li>Because silicon was measured in my lab data as ‘Matrix,’ with no actual number stated, I estimated the silicon for both containers based on the average for containers in the database (72.37).</li>
<li>Because K (potassium) was measured at 980 ppm for the clear facsimile, rather than by wt%, I assigned it a value of 50% of the wt% of the Nikumaroro jar: .5 X .24 = .12.</li>
<li>Because Fe (iron) is at very low wt% levels in the 1987 database, and the levels in my data are listed at very low parts per million (ppm), I assigned to the Nikumaroro jar and to the clear facsimile a wt% of Fe of .02 and .01, respectively.</li>
<li>Refractive index was not measured for either the Nikumaroro jar or the clear facsimile. Since the clear facsimile is completely transparent, I assigned it the minimum refractive index of containers from the 1987 dataset. Since the Nikumaroro jar is semi-opaque, I assigned it the maximum refractive index of containers from the 1987 dataset.</li>
</ul>
<p>These educated guessed were necessary to ensure the ability of the machine learning algorithm to compare the data from EAG Laboratories with the training dataset from the U.K.*</p>
<section id="note-in-a-later-section-of-this-story-we-will-use-a-technique-to-evaluate-whether-or-not-the-decision-to-supply-a-few-of-the-values-for-the-jars-makes-a-significant-difference-in-our-machine-learning-algorithms-ability-to-predict-the-class-of-the-two-jar-samples." class="level4">
<h4 class="anchored" data-anchor-id="note-in-a-later-section-of-this-story-we-will-use-a-technique-to-evaluate-whether-or-not-the-decision-to-supply-a-few-of-the-values-for-the-jars-makes-a-significant-difference-in-our-machine-learning-algorithms-ability-to-predict-the-class-of-the-two-jar-samples.">*Note: In a later section of this story, we will use a technique to evaluate whether or not the decision to supply a few of the values for the jars makes a significant difference in our machine learning algorithm’s ability to predict the class of the two jar samples.</h4>
</section>
</section>
<section id="research-agenda" class="level2">
<h2 class="anchored" data-anchor-id="research-agenda">Research Agenda</h2>
<p>There are three main research questions I wish to answer in my machine learning classification problem. I follow each of these questions with a rationale that explains the derived benefits in answering each question.<br><br> <strong>1) What can Python’s Matplotlib and Seaborn graphing capabilities reveal about the similarities and/or dissimilarities between the Nikumaroro jar, clear facsimile, and the glass samples in the 1987 database?</strong></p>
<p>Rationale: Before tackling the machine learning problem, we need to learn whether and how the Nikumaroro jar and the clear facsimile, which precede the samples in the U.K. database by many decades, differ from the samples in that U.K. database. If the difference is too great, it may not be possible to use machine learning to classify the Nikumaroro jar and the clear facsimile at all.</p>
<p><br><strong>2) What do the correlations between elements for the different types of glass in the 1987 database reveal about late 20th century glassmaking, as compared with early 20th century glassmaking?</strong></p>
<p>Rationale: The Nikumaroro jar and the clear facsimile precede the samples in the U.K. database by many decades. During these many decades, the recipes used in the glassmaking batches may have changed considerably. It is always fascinating to observe historical changes made manifest in data, but beyond this historical interest, the graphs that may be built from these correlations may also provide visual explanations for some difficulties a machine learning model may be having with classifying the Nikumaroro jar and the clear facsimile.</p>
<p><br><strong>3) Using machine learning to train a model on the 1987 database, could that model be used to classify the type (container) of one or both of the older samples unseen by the model?</strong></p>
<p>Rationale: A successful classification of the Nikumaroro jar may suggest that it is not quite so unusual as we had supposed in our earlier research. The U.K. database was never built to classify highly unusual or historical glass samples. It was built to classify ordinary glass samples retrieved from break-ins that occurred in the 1980s. Successful classification would strengthen the argument, often expressed but without evidence, that the jar is not a rare item likely to be from the 1930s, but of far more recent provenance, too recent to have possibly been brought to the island in 1937 by Amelia Earhart.</p>
<p>The outcome of this classification problem has the potential to disverify or perhaps weaken a part of our <a href="https://www.academia.edu/9015080/Amelia_Earhart_on_Nikumaroro_A_Summary_of_the_Evidence">hypothesis</a> that Earhart and her navigator, Fred Noonan, perished on Nikumaroro. Efforts to disverify are an important element of scientific research, for as <a href="https://faculty.mtsac.edu/cbriggs/Biol-1%20Readings%20Cargo%20Cult%20Science.pdf">Richard Feynman once said</a>, “The first principle is that you must not fool yourself — and you are the easiest person to fool.”</p>
<section id="first-before-we-start-to-analyze-these-questions-we-will-read-in-the-data-file-and-create-a-simple-report." class="level3">
<h3 class="anchored" data-anchor-id="first-before-we-start-to-analyze-these-questions-we-will-read-in-the-data-file-and-create-a-simple-report.">First, before we start to analyze these questions, we will read in the data file and create a simple report.</h3>
<p>Our first task is to read in a copy of the 1987 glass.data file that has been uploaded to Zenodo.org. Then, we can assign the names of the variables in this file to a variable called names. Notice I am carefully avoiding the variable name ‘Type’ because it is a reserved word in Python. (This was obviously not true in 1987 when the database was built, because Python did not exist then.) Using that reserved word will co-opt the type function, thus disabling my ability to check the types of specific variables. Instead, I will use the variable name ‘GlassType’ to contain the various types of glass.</p>
<p>I also include a docstring explaining what these variables mean and their general significance to glassmaking.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">ID: an integer that identifies the 1987 sample number. This column should always be droppped in any analysis.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">Ref_ix: Refractive index. There are many scientific definitions of refractive index, but in basic terms,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">refractive index refers to how much the path of light is refracted when it enters the glass. The higher the </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">refractive index, the less transparent the glass is.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">Na: Sodium oxide reduces the temperature at which silica melts to around 1500-1650 Celsius. A lower temperature</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">equates to a more efficient and less costly manufacturing process.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">Mg: Magnesium oxide is used as a reducing agent in glassmaking. It reduces sulfates, which are used to fine the</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">glass, and other impurities, such as iron.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">Al: Aluminum oxide is added to increase durability and heat resistance and to reduce crystallization.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">Si: Silicon oxide. The main constituent (matrix) of glass is silica sand.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">K: Potassium oxide is used for strengthening and fining the glass.</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">Ca: Calcium oxide is considered a stabilizer. It is used for increasing the chemical durability of glass, and </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">also to lower the melting point of the silica. Calcium oxide can also be used to raise refractive index.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">Ba: Barium oxide. Barium was, and is, employed in glass manufacture to add luster or brilliance to glass, </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">and also to raise the refractive index.[1] Museum glass, salt shakers, shot glasses, and microscopic slides </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">are often high in barium. The Nikumaroro jar is also high in barium.</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">Fe: Iron oxide can be used as a colorant or it can exist by chance as an impurity.</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">GlassType: This is the integer assigned in 1987 to signify the type of glass of the sample. This column should</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">only appear in the validation dataset; it should be dropped from the training dataset.</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> urllib.request.urlopen(<span class="st">"https://zenodo.org/record/6913745/files/glass.data"</span>) <span class="im">as</span> f:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    html<span class="op">=</span>f.read().decode(<span class="st">'utf-8'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'glass.csv'</span>, <span class="st">'w'</span>) <span class="im">as</span> out:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        out.write(html)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>filename<span class="op">=</span><span class="st">'glass.csv'</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>names<span class="op">=</span>[<span class="st">'ID'</span>,<span class="st">'Ref_ix'</span>,<span class="st">'Na'</span>,<span class="st">'Mg'</span>,<span class="st">'Al'</span>,<span class="st">'Si'</span>,<span class="st">'K'</span>,<span class="st">'Ca'</span>,<span class="st">'Ba'</span>,<span class="st">'Fe'</span>,<span class="st">'GlassType'</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The three questions of the research agenda (listed above) are quite different. It will be convenient to modify the dataset in different ways for each problem. Therefore, we will read in the file twice to create two Pandas dataframes of the glass data, thus allowing us to have separate data pipelines to keep the machine learning question separate from the other two. The pandas library in Python provides a convenient function, read_csv, for this purpose. One of the columns is imported as an object, which I will coax to be an integer for more reliable data processing.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> read_csv(filename, header<span class="op">=</span><span class="va">None</span>, sep<span class="op">=</span><span class="st">','</span>,names<span class="op">=</span>names)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>dataset_ml <span class="op">=</span> read_csv(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    filename, header<span class="op">=</span><span class="va">None</span>, sep<span class="op">=</span><span class="st">','</span>,names<span class="op">=</span>names)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert clunky object formats to ints</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'GlassType'</span>] <span class="op">=</span> dataset[<span class="st">'GlassType'</span>].astype(<span class="st">'string'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'GlassType'</span>] <span class="op">=</span> dataset[<span class="st">'GlassType'</span>].astype(<span class="st">'int'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>dataset_ml[<span class="st">'GlassType'</span>] <span class="op">=</span> dataset[<span class="st">'GlassType'</span>].astype(<span class="st">'string'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>dataset_ml[<span class="st">'GlassType'</span>] <span class="op">=</span> dataset[<span class="st">'GlassType'</span>].astype(<span class="st">'int'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will provide the list of variables in the file with descriptions of their counts and data types. I could use the info() method for this purpose, but an anonymous user on <a href="https://stackoverflow.com/questions/64067424/how-to-convert-df-info-into-data-frame-df-info">StackOverflow</a> has provided a function, which I have adopted, to give this information a more attractive display. It would be useful as well to have a simple statistical report of this dataset, which is provided by the describe method. Notice that the columns ID and GlassType, as supplied from the 1987 study, are integers, but they are also categorical variables. I will therefore drop these columns from my simple statistical report because they would be meaningless in terms of the measures of central tendency (mean, max, etc.). However, because GlassType will be<b> very </b>important to the rest of this code, I will take a copy of the dataset prior to making this report, to ensure I do not drop this variable from the original Pandas dataframe.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> infoOut(data,details<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    dfInfo <span class="op">=</span> data.columns.to_frame(name<span class="op">=</span><span class="st">'Column'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    dfInfo[<span class="st">'Non-Null Count'</span>] <span class="op">=</span> data.notna().<span class="bu">sum</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    dfInfo[<span class="st">'Dtype'</span>] <span class="op">=</span> data.dtypes</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    dfInfo.reset_index(drop<span class="op">=</span><span class="va">True</span>,inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> details:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        rangeIndex <span class="op">=</span> (dfInfo[<span class="st">'Non-Null Count'</span>].<span class="bu">min</span>(),dfInfo[<span class="st">'Non-Null Count'</span>].<span class="bu">min</span>())</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        totalColumns <span class="op">=</span> dfInfo[<span class="st">'Column'</span>].count()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        dtypesCount <span class="op">=</span> dfInfo[<span class="st">'Dtype'</span>].value_counts()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        totalMemory <span class="op">=</span> dfInfo.memory_usage().<span class="bu">sum</span>()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dfInfo, rangeIndex, totalColumns, dtypesCount, totalMemory</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dfInfo</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>display(infoOut(dataset))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>temp<span class="op">=</span>dataset.copy()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>display(temp.iloc[:,<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>].describe())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Column</th>
<th data-quarto-table-cell-role="th">Non-Null Count</th>
<th data-quarto-table-cell-role="th">Dtype</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>ID</td>
<td>214</td>
<td>int64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Ref_ix</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Na</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Mg</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Al</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Si</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>K</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>Ca</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>Ba</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>Fe</td>
<td>214</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>GlassType</td>
<td>214</td>
<td>int64</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Ref_ix</th>
<th data-quarto-table-cell-role="th">Na</th>
<th data-quarto-table-cell-role="th">Mg</th>
<th data-quarto-table-cell-role="th">Al</th>
<th data-quarto-table-cell-role="th">Si</th>
<th data-quarto-table-cell-role="th">K</th>
<th data-quarto-table-cell-role="th">Ca</th>
<th data-quarto-table-cell-role="th">Ba</th>
<th data-quarto-table-cell-role="th">Fe</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
<td>214.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>1.518365</td>
<td>13.407850</td>
<td>2.684533</td>
<td>1.444907</td>
<td>72.650935</td>
<td>0.497056</td>
<td>8.956963</td>
<td>0.175047</td>
<td>0.057009</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>0.003037</td>
<td>0.816604</td>
<td>1.442408</td>
<td>0.499270</td>
<td>0.774546</td>
<td>0.652192</td>
<td>1.423153</td>
<td>0.497219</td>
<td>0.097439</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.511150</td>
<td>10.730000</td>
<td>0.000000</td>
<td>0.290000</td>
<td>69.810000</td>
<td>0.000000</td>
<td>5.430000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>1.516522</td>
<td>12.907500</td>
<td>2.115000</td>
<td>1.190000</td>
<td>72.280000</td>
<td>0.122500</td>
<td>8.240000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>1.517680</td>
<td>13.300000</td>
<td>3.480000</td>
<td>1.360000</td>
<td>72.790000</td>
<td>0.555000</td>
<td>8.600000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>1.519157</td>
<td>13.825000</td>
<td>3.600000</td>
<td>1.630000</td>
<td>73.087500</td>
<td>0.610000</td>
<td>9.172500</td>
<td>0.000000</td>
<td>0.100000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>1.533930</td>
<td>17.380000</td>
<td>4.490000</td>
<td>3.500000</td>
<td>75.410000</td>
<td>6.210000</td>
<td>16.190000</td>
<td>3.150000</td>
<td>0.510000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>There are no nulls in the dataset. The column types are all numeric. All 214 samples of the database have been included.</p>
</section>
<section id="create-a-dictionary-of-glass-types." class="level3">
<h3 class="anchored" data-anchor-id="create-a-dictionary-of-glass-types.">Create a dictionary of glass types.</h3>
<p>We need a way to translate the numerical glass types found in the 1987 database to their English equivalents. This is accomplished by creating a Python dictionary.</p>
<p>Note we could have omitted Vehicle Non-Float from this dictionary, since there are no examples in the data of this type; however, for the sake of clarity we will retain it.</p>
<p>We include also an exhibit that lists the counts for the various glass types.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a> <span class="bu">dict</span> <span class="op">=</span> {<span class="dv">1</span>: <span class="st">'Window Float'</span>,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="dv">2</span>: <span class="st">'Window Non-Float'</span>, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="dv">3</span>: <span class="st">'Vehicle Float'</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="dv">4</span>: <span class="st">'Vehicle Non-Float'</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="dv">5</span>: <span class="st">'Container'</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="dv">6</span>: <span class="st">'Tableware'</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">7</span>: <span class="st">'Headlamp'</span>}</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>vc<span class="op">=</span>temp.replace({<span class="st">"GlassType"</span>: <span class="bu">dict</span>},inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>vc<span class="op">=</span>temp[<span class="st">'GlassType'</span>].value_counts().rename_axis(<span class="st">'unique_values'</span>).reset_index(name<span class="op">=</span><span class="st">'Counts'</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>vc.set_index(<span class="st">'unique_values'</span>,inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>vc.rename_axis([<span class="st">'GlassType'</span>],inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>display(vc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Counts</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">GlassType</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Window Non-Float</td>
<td>76</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Window Float</td>
<td>70</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Headlamp</td>
<td>29</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Vehicle Float</td>
<td>17</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Container</td>
<td>13</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Tableware</td>
<td>9</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="what-do-the-types-of-glass-represent-in-everyday-life" class="level3">
<h3 class="anchored" data-anchor-id="what-do-the-types-of-glass-represent-in-everyday-life">What do the types of glass represent in everyday life?</h3>
<p>The terms “float” and “non-float” are glass industry terms. They describe the process by which window glass is made. Prior to 1959, all windows were made with non-float processes. These commonly involved grinding and polishing sheets of plate glass, which were cast on an iron surface. The result was not the perfectly smooth and clear glass we know today but rather was somewhat more translucent and sometimes <a href="https://brennancorp.com/blog/why-is-the-glass-in-my-windows-wavy/">‘wavy’</a>. Today, this kind of window glass is prized for its artistic appearance and historical significance, if not for its lack of clarity. Float glass is made by floating molten glass on a bed of molten tin under controlled environmental conditions in a bath chamber. An excellent video describing this process is <a href="https://www.youtube.com/watch?v=pMKHs_FF0bw&amp;t=200s">here:</a></p>
<p>If a glass sample is Window Float, in all probability it was manufactured no earlier than 1959. Window Non-Float samples are probably older than 1959.</p>
<p>Vehicle Float glass describes the type of glass that has been used for vehicle windshields since 1959.</p>
<p>Headlamps are vehicle lamps, which illuminate the road ahead.</p>
<p>Container glass describes commercial product containers, whether for bottles or jars.</p>
<p>Tableware glass describes glassware for the table or any glass table item used in dining or the serving of food.</p>
</section>
<section id="create-some-utility-functions-for-plotting-graphs-from-the-1987-dataset." class="level3">
<h3 class="anchored" data-anchor-id="create-some-utility-functions-for-plotting-graphs-from-the-1987-dataset.">Create some utility functions for plotting graphs from the 1987 dataset.</h3>
<p>Before using machine learning on the data, there is much we can learn about the two jars’ likeness to glass samples in the 1987 database, simply by observing counts based on specific criteria. By comparing each jar with its “nearest neighbors,” based on measurements for three elements in the periodic table, we may gain a sense of which type of glass in the database each jar most closely resembles.</p>
<p>We will therefore write functions to create three graphical reports, one for each of the three elements magnesium, calcium, and barium. Each of the three reports will display three things:<br> 1) The complete range of counts for each glass type. (This graph will be repeated for each report.) <br> 2) The range of counts for all glass types within 0.15 above and below the elemental measurement of the <b>clear facsimile</b>. <br> 3) The range of counts for all glass types within 0.15 above and below the elemental measurement of the <b>Nikumaroro jar</b>.</p>
<p>The key function, numgroups, requires an element from the periodic table so that it can look up parameters in the included dictionary. This dictionary contains the measurements obtained from the lab for each element for the clear facsimile and for the Nikumaroro jar. Low and High parameter values for each element are also required, even when the complete range of counts is to be displayed. These will be passed to the function as hard-coded values that are either <br> &gt; a. exactly 0.15 above and below the measurement for each jar; or <br> &gt; b. the lowest and highest measured values in the database for each element.</p>
<p>Last, the jar_spec parameter is simply the type of jar to be analyzed, clear facsimile or Nikumaroro jar. The jar_spec is used to create the title for each graph.</p>
<p>If the graph’s input parameters result in a graph with a single type of glass, no graph is created. (Bar graphs with but a single bar are, after all, neither very attractive nor useful!) Instead, a message is displayed that mirrors the text that accompanies the other graphs.</p>
<p>If the number of glass types returned by the criteria is equal to zero, we cause an error to be thrown with int(‘d’), which attempts to convert the letter ‘d’ to an integer, rather than waiting for the error that would have been thrown naturally by the plot statement. If we had waited for the plot statement to throw the error, useless information about the size of the graph (which cannot be drawn in this case) is displayed. In the case of this error, a simple message is printed to inform us that no relevant glass samples were found in the database.</p>
<p>The other function, ticks_restrict_to_integer, is a utility function that controls the number of tick marks on the y-axis of each graph. The y-axis represents the count. Because the scale of counts varies with each graph, it was necessary to make the y-axes more uniform with one another by having this function.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.ticker <span class="im">import</span> MultipleLocator</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ticks_restrict_to_integer(axis):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Restrict the ticks on the given axis to be at least integer;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    that is, no half ticks at 1.5 for example.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    major_tick_locs <span class="op">=</span> axis.get_majorticklocs()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(major_tick_locs) <span class="op">&lt;</span> <span class="dv">2</span> <span class="kw">or</span> major_tick_locs[<span class="dv">1</span>] <span class="op">-</span> major_tick_locs[<span class="dv">0</span>] <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        axis.set_major_locator(MultipleLocator(<span class="dv">1</span>))    </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numgroups(element,low,high,jar_spec):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    non-fruitful function. Outputs a graph if the number of types of</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">    glass within the DataFrame is &gt; 1.  Otherwise, the function prints</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">    a message, for aesthetic reasons.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters:</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    element: an element in the periodic table</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">    low: A minimum value for the measured element, which acts as the </span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">    minimum value to allow into the dataset sample</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">    high: A maximum value for the measured element, which acts as the </span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">    maximum value to allow into the dataset sample</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co">    jar_spec: A value of either 'clear facsimile' or 'Nikumaroro jar', used</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co">    in the title of the graph. </span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">    However, when the function is called to obtain the complete distri-</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">    bution of counts, the value of jar_spec is set equal to None. </span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Preconditions:</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co">    element is a string with value of: 'Mg' or 'Ca' or 'Ba'</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co">    Low cannnot be greater than high.</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    jar_spec<span class="op">=</span><span class="st">'full range of '</span> <span class="op">+</span> element <span class="op">+</span> <span class="st">' measurements in the database'</span> <span class="cf">if</span> jar_spec<span class="op">==</span><span class="va">None</span> <span class="cf">else</span> jar_spec</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    rangedict <span class="op">=</span> {<span class="st">'Ba'</span>:[<span class="st">'Barium'</span>,[<span class="fl">.37</span>,<span class="fl">.74</span>]],</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'Mg'</span>:[<span class="st">'Magnesium'</span>,[<span class="fl">2.4</span>,<span class="fl">4.3</span>]],</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'Ca'</span>:[<span class="st">'Calcium'</span>,[<span class="fl">3.6</span>,<span class="fl">8.5</span>]]}</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    facsimile_ref<span class="op">=</span>rangedict[element][<span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    artifact_ref<span class="op">=</span>rangedict[element][<span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    element_full_name<span class="op">=</span>rangedict[element][<span class="dv">0</span>]</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    sampl <span class="op">=</span> dataset[(dataset[element] <span class="op">&gt;=</span> low) <span class="op">&amp;</span> (</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        dataset[element] <span class="op">&lt;=</span> high)]</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    number_groups<span class="op">=</span>sampl[<span class="st">'GlassType'</span>].value_counts().shape[<span class="dv">0</span>]</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    titletext1<span class="op">=</span><span class="st">"Count of Glass Types in the 1987 database </span><span class="ch">\n</span><span class="st"> with a range of measured values of "</span> <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    element_full_name <span class="op">+</span> <span class="st">" </span><span class="ch">\n</span><span class="st"> between "</span> <span class="op">+</span> <span class="bu">str</span>(low) <span class="op">+</span> <span class="st">" and "</span> <span class="op">+</span> <span class="bu">str</span>(high) <span class="op">+</span> <span class="st">" wt%. </span><span class="ch">\n</span><span class="st"> "</span> <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This is the range of counts for the "</span> <span class="op">+</span> jar_spec.upper()</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'clear'</span> <span class="kw">in</span> jar_spec <span class="kw">or</span> <span class="st">'Niku'</span> <span class="kw">in</span> jar_spec:</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Tolerance provides information in the report about the range of values considered in the graph.</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        tolerance<span class="op">=</span><span class="bu">round</span>((high<span class="op">-</span>low)<span class="op">/</span><span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        titletext2<span class="op">=</span><span class="st">" that fall within a tolerance of +-"</span> <span class="op">+</span> <span class="bu">str</span>(tolerance) <span class="op">+</span> <span class="st">' of its measurement.'</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        tolerance<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        titletext2<span class="op">=</span><span class="st">"."</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> low<span class="op">&gt;</span>high:</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Low value must be smaller than the high value. Try again.'</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> number_groups<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(titletext1 <span class="op">+</span> titletext2)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'There is only ONE sample Type.'</span>)</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(sampl[<span class="st">'GlassType'</span>].values[<span class="dv">0</span>],<span class="st">'='</span>,sampl.shape[<span class="dv">0</span>])</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'reference: Clear facsimile jar ='</span>,facsimile_ref,<span class="st">'wt%'</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>              <span class="st">'</span><span class="ch">\n</span><span class="st"> Nikumaroro jar ='</span>,artifact_ref,<span class="st">'wt%'</span>)</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'reference: Clear facsimile jar ='</span>,facsimile_ref,<span class="st">'wt%'</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>              <span class="st">'</span><span class="ch">\n</span><span class="st"> Nikumaroro jar ='</span>,artifact_ref,<span class="st">'wt%'</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>            vc<span class="op">=</span>sampl[<span class="st">'GlassType'</span>].value_counts()</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>            <span class="bu">int</span>(<span class="st">'d'</span>) <span class="cf">if</span> <span class="bu">len</span>(vc)<span class="op">==</span><span class="dv">0</span> <span class="cf">else</span> <span class="bu">int</span>(<span class="st">'1'</span>)</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>            vc.plot(</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>                kind<span class="op">=</span><span class="st">'bar'</span>, figsize<span class="op">=</span>(<span class="fl">2.4</span><span class="op">*</span>number_groups, <span class="dv">6</span>), rot<span class="op">=</span><span class="dv">0</span>, cmap<span class="op">=</span><span class="st">'Spectral'</span>)<span class="op">;</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>            plt.xlabel(<span class="st">"Glass Type"</span>, labelpad<span class="op">=</span><span class="dv">14</span>,fontsize<span class="op">=</span><span class="dv">16</span>,rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            plt.ylabel(<span class="st">"Count of Type"</span>, labelpad<span class="op">=</span><span class="dv">70</span>, </span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>                       fontsize<span class="op">=</span><span class="dv">16</span>,rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            plt.xticks(fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            plt.yticks(fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>            plt.figtext(<span class="fl">0.5</span>, <span class="fl">1.0</span>, titletext1<span class="op">+</span>titletext2, wrap<span class="op">=</span><span class="va">True</span>, horizontalalignment<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)  </span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> plt.subplot()</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>            ticks_restrict_to_integer(ax.yaxis)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e: </span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(e)</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'For the '</span> <span class="op">+</span> jar_spec <span class="op">+</span> <span class="st">', no glass samples in the U.K. data are in the range of '</span> <span class="op">+</span> <span class="bu">str</span>(low) <span class="op">+</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>                <span class="st">" and "</span> <span class="op">+</span> <span class="bu">str</span>(high) <span class="op">+</span> <span class="st">' wt</span><span class="sc">% f</span><span class="st">or '</span> <span class="op">+</span> element_full_name <span class="op">+</span> <span class="st">'.'</span>)</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="what-types-of-glass-in-the-1987-database-are-most-similar-to-the-nikumaroro-jar-and-clear-facsimile-in-terms-of-magnesium-barium-or-calcium-content" class="level3">
<h3 class="anchored" data-anchor-id="what-types-of-glass-in-the-1987-database-are-most-similar-to-the-nikumaroro-jar-and-clear-facsimile-in-terms-of-magnesium-barium-or-calcium-content">What types of glass in the 1987 database are most similar to the Nikumaroro jar and clear facsimile in terms of magnesium, barium or calcium content?</h3>
<p>To review what was stated above, examining Mg, Ba, and Ca individually in the U.K. database, we can perform the following steps to produce a report: 1. Obtain the complete distribution of counts in the 1987 database by restricting the element’s values to the range between its minimum and maximum wt% values. 2. See the distribution of counts in the 1987 database that results from setting the range of measured wt% for the element to a tolerance 0.15 wt% above and below its measurement for the <code>clear facsimile</code>. 3. See the distribution of counts in the 1987 database that results from setting the range of measured wt% for the element to a tolerance 0.15 wt% above and below its measurement for the <code>Nikumaroro jar</code>. 4. Repeat steps 1 to 3 for the next element until all elements have been reported.</p>
</section>
</section>
<section id="report-for-magnesium" class="level2">
<h2 class="anchored" data-anchor-id="report-for-magnesium">Report for Magnesium</h2>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>dataset.replace({<span class="st">"GlassType"</span>: <span class="bu">dict</span>},inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>element<span class="op">=</span><span class="st">'Mg'</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>Mgdict<span class="op">=</span>{<span class="st">'clear facsimile'</span>:[<span class="fl">2.25</span>, <span class="fl">2.55</span>],<span class="st">'Nikumaroro jar'</span>:[<span class="fl">4.15</span>,<span class="fl">4.45</span>]}</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>low<span class="op">=</span><span class="dv">0</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>high<span class="op">=</span>dataset[element].<span class="bu">max</span>()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Run report for all glass types</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>numgroups(element,low,high,<span class="va">None</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Run reports for the clear facsimile and Nikumaroro jar</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jar_type <span class="kw">in</span> Mgdict:</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    low<span class="op">=</span>Mgdict[jar_type][<span class="dv">0</span>]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    high<span class="op">=</span>Mgdict[jar_type][<span class="dv">1</span>]</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    numgroups(element,low,high,jar_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>reference: Clear facsimile jar = 2.4 wt%
 Nikumaroro jar = 4.3 wt%
reference: Clear facsimile jar = 2.4 wt%
 Nikumaroro jar = 4.3 wt%
reference: Clear facsimile jar = 2.4 wt%
 Nikumaroro jar = 4.3 wt%
For the Nikumaroro jar, no glass samples in the U.K. data are in the range of 4.15 and 4.45 wt% for Magnesium.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-7-output-3.png" class="img-fluid"></p>
</div>
</div>
<section id="magnesium-analysis" class="level3">
<h3 class="anchored" data-anchor-id="magnesium-analysis">Magnesium Analysis</h3>
<p>Magnesium:<br> The clear facsimile jar has 2.4 wt% Mg. There are only two glass types, tableware and non-float windows, with values between 2.25 and 2.55 wt% Mg. Magnesium, if it were the only feature, would seem to predict the clear facsimile jar to be in the window or the tableware family, a close cousin to the container family. <br>The Nikumaroro jar has 4.3 wt% Mg. Checking back to the report we created with the describe() method, this value is well above the 75th percentile. We know from the literature on glassmaking that any Mg measurement from a modern glass sample that is above 3.5 wt% is likely to be a window, and not a container.[2]</p>
</section>
</section>
<section id="report-for-calcium" class="level2">
<h2 class="anchored" data-anchor-id="report-for-calcium">Report for Calcium</h2>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>dataset.replace({<span class="st">"GlassType"</span>: <span class="bu">dict</span>},inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>element<span class="op">=</span><span class="st">'Ca'</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>Cadict<span class="op">=</span>{<span class="st">'clear facsimile'</span>:[<span class="fl">3.45</span>, <span class="fl">3.75</span>],<span class="st">'Nikumaroro jar'</span>:[<span class="fl">8.35</span>,<span class="fl">8.65</span>]}</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>low<span class="op">=</span><span class="dv">0</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>high<span class="op">=</span>dataset[element].<span class="bu">max</span>()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Run report for all glass types</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>numgroups(element,low,high,<span class="va">None</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Run reports for the clear facsimile and Nikumaroro jar</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jar_type <span class="kw">in</span> Cadict:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    low<span class="op">=</span>Cadict[jar_type][<span class="dv">0</span>]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    high<span class="op">=</span>Cadict[jar_type][<span class="dv">1</span>]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    numgroups(element,low,high,jar_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>reference: Clear facsimile jar = 3.6 wt%
 Nikumaroro jar = 8.5 wt%
reference: Clear facsimile jar = 3.6 wt%
 Nikumaroro jar = 8.5 wt%
For the clear facsimile, no glass samples in the U.K. data are in the range of 3.45 and 3.75 wt% for Calcium.

reference: Clear facsimile jar = 3.6 wt%
 Nikumaroro jar = 8.5 wt%</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-8-output-3.png" class="img-fluid"></p>
</div>
</div>
<section id="calcium-analysis" class="level3">
<h3 class="anchored" data-anchor-id="calcium-analysis">Calcium Analysis</h3>
<p>Calcium:<br> The clear facsimile jar has 3.6 wt% Ca. We can see from the report we created with the describe() method that this is off-scale low, a value less than all the samples in the database. No glass samples are in the range of 3.45% and 3.75% weight calcium. <br>The Nikumaroro jar has 8.5 wt% Ca. This is close to the mean in the database (8.96) for all glass types, but no containers are displayed on the graph within +- .15 of the calcium value measured on the Nikumaroro jar.</p>
</section>
</section>
<section id="report-for-barium" class="level2">
<h2 class="anchored" data-anchor-id="report-for-barium">Report for Barium</h2>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>dataset.replace({<span class="st">"GlassType"</span>: <span class="bu">dict</span>},inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>element<span class="op">=</span><span class="st">'Ba'</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>Badict<span class="op">=</span>{<span class="st">'clear facsimile'</span>:[<span class="fl">0.22</span>, <span class="fl">0.52</span>],<span class="st">'Nikumaroro jar'</span>:[<span class="fl">0.59</span>,<span class="fl">0.89</span>]}</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>low<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>high<span class="op">=</span>dataset[element].<span class="bu">max</span>()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Run report for all glass types</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>numgroups(element,low,high,<span class="va">None</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Run reports for the clear facsimile and Nikumaroro jar</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jar_type <span class="kw">in</span> Badict:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    low<span class="op">=</span>Badict[jar_type][<span class="dv">0</span>]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    high<span class="op">=</span>Badict[jar_type][<span class="dv">1</span>]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    numgroups(element,low,high,jar_type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>reference: Clear facsimile jar = 0.37 wt%
 Nikumaroro jar = 0.74 wt%
reference: Clear facsimile jar = 0.37 wt%
 Nikumaroro jar = 0.74 wt%
reference: Clear facsimile jar = 0.37 wt%
 Nikumaroro jar = 0.74 wt%</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-9-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-9-output-4.png" class="img-fluid"></p>
</div>
</div>
<section id="barium-analysis" class="level3">
<h3 class="anchored" data-anchor-id="barium-analysis">Barium Analysis</h3>
<p>Barium:<br> The ratio of 2:1 for the element Barium in the two glass samples (.74 for the Nikumaroro jar and .37 for the clear facsimile jar) suggests that the glass maker, Hazel-Atlas, used a recipe. This is not uncommon in the glass industry. For the clear facsimile, one window, one container and one headlamp have measurements of barium in the database that are within +- 0.15 of its measured value of barium. For the Nikumaroro jar, eight headlamps and one window have measurements of barium in the database that are within +-0.15 of its measured value of barium.</p>
<p>Conclusion: We have not yet applied machine learning to the identification of the Nikumaroro jar or the clear facsimile, but it would appear from the weight percent of some of the key ingredients from these jars that windows, of the float or non-float variety, and, to a lesser extent, headlamps, are strong candidates for how the 1987 database might predict their identity, if machine learning were employed as a predictive tool to do this. Nevertheless, some containers did appear in the bar graphs for the clear facsimile, indicating that there is at least a slight resemblance between that jar and containers from the U.K. database. The results are not exactly encouraging to the success of a machine learning model, but they are not discouraging enough to dampen curiosity at discovering what machine learning might be able to do with the data from the Nikumaroro jar and the clear facsimile.</p>
</section>
<section id="turning-the-question-around-if-one-were-to-subset-the-1987-database-only-for-containers-how-would-that-database-compare-with-the-two-jars-both-of-the-jars-are-of-course-containers." class="level3">
<h3 class="anchored" data-anchor-id="turning-the-question-around-if-one-were-to-subset-the-1987-database-only-for-containers-how-would-that-database-compare-with-the-two-jars-both-of-the-jars-are-of-course-containers.">Turning the question around: If one were to subset the 1987 database only for containers, how would that database compare with the two jars? Both of the jars are, of course, containers.</h3>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sampcont <span class="op">=</span> dataset[dataset[<span class="st">'GlassType'</span>].isin([<span class="st">'Container'</span>])]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>sampcont<span class="op">=</span>sampcont.drop([<span class="st">'ID'</span>, <span class="st">'GlassType'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Container summary report:'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>display(sampcont.describe())</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>rangedict <span class="op">=</span> {<span class="st">'Ba'</span>:[<span class="st">'Barium'</span>,[<span class="fl">.37</span>,<span class="fl">.74</span>]],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'Mg'</span>:[<span class="st">'Magnesium'</span>,[<span class="fl">2.4</span>,<span class="fl">4.3</span>]],</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'Ca'</span>:[<span class="st">'Calcium'</span>,[<span class="fl">3.6</span>,<span class="fl">8.5</span>]]}</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>ELEMENTS<span class="op">=</span>[<span class="st">'Mg'</span>,<span class="st">'Ca'</span>,<span class="st">'Ba'</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> element <span class="kw">in</span> ELEMENTS:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        element,<span class="st">': min to max:'</span>,sampcont[element].<span class="bu">min</span>(),<span class="st">'to'</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        ,sampcont[element].<span class="bu">max</span>())</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    facsimile_ref<span class="op">=</span>rangedict[element][<span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    artifact_ref<span class="op">=</span>rangedict[element][<span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'reference: Clear facsimile jar ='</span>,facsimile_ref,<span class="st">'wt%'</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>              <span class="st">'</span><span class="ch">\n</span><span class="st"> Nikumaroro jar='</span>,artifact_ref,<span class="st">'wt%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Container summary report:

Mg : min to max: 0.0 to 2.68
reference: Clear facsimile jar = 2.4 wt%
 Nikumaroro jar= 4.3 wt%

Ca : min to max: 5.87 to 12.5
reference: Clear facsimile jar = 3.6 wt%
 Nikumaroro jar= 8.5 wt%

Ba : min to max: 0.0 to 2.2
reference: Clear facsimile jar = 0.37 wt%
 Nikumaroro jar= 0.74 wt%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Ref_ix</th>
<th data-quarto-table-cell-role="th">Na</th>
<th data-quarto-table-cell-role="th">Mg</th>
<th data-quarto-table-cell-role="th">Al</th>
<th data-quarto-table-cell-role="th">Si</th>
<th data-quarto-table-cell-role="th">K</th>
<th data-quarto-table-cell-role="th">Ca</th>
<th data-quarto-table-cell-role="th">Ba</th>
<th data-quarto-table-cell-role="th">Fe</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
<td>13.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>1.518928</td>
<td>12.827692</td>
<td>0.773846</td>
<td>2.033846</td>
<td>72.366154</td>
<td>1.470000</td>
<td>10.123846</td>
<td>0.187692</td>
<td>0.060769</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>0.003345</td>
<td>0.777037</td>
<td>0.999146</td>
<td>0.693920</td>
<td>1.282319</td>
<td>2.138695</td>
<td>2.183791</td>
<td>0.608251</td>
<td>0.155588</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.513160</td>
<td>11.030000</td>
<td>0.000000</td>
<td>1.400000</td>
<td>69.890000</td>
<td>0.130000</td>
<td>5.870000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>1.516660</td>
<td>12.730000</td>
<td>0.000000</td>
<td>1.560000</td>
<td>72.180000</td>
<td>0.380000</td>
<td>9.700000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>1.519940</td>
<td>12.970000</td>
<td>0.000000</td>
<td>1.760000</td>
<td>72.690000</td>
<td>0.580000</td>
<td>11.270000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>1.521190</td>
<td>13.270000</td>
<td>1.710000</td>
<td>2.170000</td>
<td>73.390000</td>
<td>0.970000</td>
<td>11.530000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>1.523690</td>
<td>14.010000</td>
<td>2.680000</td>
<td>3.500000</td>
<td>73.880000</td>
<td>6.210000</td>
<td>12.500000</td>
<td>2.200000</td>
<td>0.510000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>When we restrict the U.K. database to only containers, the comparisons with the Nikumaroro jar and the clear facsimile enter into sharper relief. The clear facsimile has a high magnesium content for a container but this is still within range of the lowest and highest values within the U.K. database. The jar from Nikumaroro has a magnesium content much higher than that of the highest nearest neighbor in the U.K. database.</p>
<p>The clear facsimile has a calcium measurement lower than that of all the containers in the U.K. database. This indicates, perhaps, that calcium was not as heavily used in glassmaking in the early part of the twentieth century. The Nikumaroro jar has a calcium content, 8.5, that is lower than the 25th percentile. This is a low value for a container, especially when one considers that the mean for containers in the U.K. database, 10.12, is skewed toward the higher end of the range.</p>
<p>The clear facsimile jar and the Nikumaroro jar are within the range of minimum to maximum in barium weight percentage for containers; however, most of the measured values of barium in the database are zero. The measured values of barium for the clear facsimile and the Nikumaroro jar are still far higher than those usually found in the U.K. database.</p>
</section>
<section id="what-do-the-correlations-between-elements-for-the-different-types-of-glass-in-the-1987-database-reveal-about-late-20th-century-glassmaking-as-compared-with-early-20th-century-glassmaking" class="level3">
<h3 class="anchored" data-anchor-id="what-do-the-correlations-between-elements-for-the-different-types-of-glass-in-the-1987-database-reveal-about-late-20th-century-glassmaking-as-compared-with-early-20th-century-glassmaking">What do the correlations between elements for the different types of glass in the 1987 database reveal about late 20th century glassmaking, as compared with early 20th century glassmaking?</h3>
<p>To answer this question, we can generate custom diverging colormaps from the U.K. database (excluding our samples of clear facsimile and Nikumaroro jar). The correlation colormaps, also known as heatmaps, show, for example, which elements of the glass most strongly correlate with refractive index, which may be considered synonymous with brilliance.</p>
<p>To keep the number of graphs to a manageable amount, we will restrict them to the glass types of Window Float, Window Non-Float and Containers. These three heatmaps provide a good summary of the correlations of ingredients in glassmaking of the late twentieth century.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># There is a warning about setting a copy of a slice from a dataframe: ignore</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>TYPES <span class="op">=</span> [<span class="st">'Window Float'</span>, <span class="st">'Window Non-Float'</span>, <span class="st">'Container'</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataset(src, glass_type):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#subset the df into a new df</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> src[src.GlassType <span class="op">==</span> glass_type]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    df.drop([<span class="st">'ID'</span>], axis<span class="op">=</span><span class="dv">1</span>,inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_heatmap(source, title):</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    cmap <span class="op">=</span> sns.diverging_palette(<span class="dv">230</span>, <span class="dv">20</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    corr<span class="op">=</span>source.corr()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a mask for the lower triangle</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> np.tril(np.ones_like(corr, dtype<span class="op">=</span><span class="bu">bool</span>))</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(corr, mask<span class="op">=</span>mask, cmap<span class="op">=</span>cmap, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">".2f"</span>,</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    vmax<span class="op">=</span><span class="fl">.3</span>, center<span class="op">=</span><span class="dv">0</span>,square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="fl">.5</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">.5</span>})</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    plt.title(title, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">14</span>,rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">14</span>,rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    plt.gcf().set_size_inches(<span class="dv">15</span>, <span class="dv">8</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt, title</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_heatmap(glass_type):</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    src <span class="op">=</span> get_dataset(dataset, glass_type)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> make_heatmap(src, glass_type <span class="op">+</span> <span class="st">' glass correlation'</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    plt.title.text <span class="op">=</span> title</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kind_of_glass <span class="kw">in</span> TYPES:</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    update_heatmap(kind_of_glass)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Instead of using a for-loop, it is also possible to have an interactive exhibit, using this Python syntax:</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="co">#interact(update_heatmap, glass_type=TYPES);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-11-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-11-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="analysis-of-correlation-heat-maps" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-correlation-heat-maps">Analysis of Correlation Heat Maps</h3>
<p>One might expect barium, an ingredient used to increase brilliance, thus increasing refractive index, would be positively correlated with refractive index in these graphs. Barium, however, is frequently measured at 0.0 weight percent in the 1987 dataset or present in only trace amounts. It may be that late 20th century glassmaking does not incorporate barium to the same degree that it was incorporated into glass production of the early twentieth century. For an example of the importance of barium in the early 20th century, see a 1936 Hazel Atlas glass patent <a href="https://tighar.org/Projects/Earhart/Archives/Research/ResearchPapers/freckleintime/Document_05_FrancisFlint1936patent.pdf">here</a>.</p>
<p>Calcium, in the form of calcium oxide, is highly correlated in the 1987 database with refractive index. This is due to the fact that calcium oxide increases refractive index. See https://www.academia.edu/12254939/Optical_and_mechanical_properties_of_calcium_phosphate_glasses for a study of this effect. There is good evidence that calcium was the element of choice to increase brilliance in 1980s glass production, rather than barium. The toxic effects of barium, which is a heavy metal, were becoming much better understood by the time the U.K. researchers assembled their database. The World Health Organization published a <a href="https://inchem.org/documents/hsg/hsg/hsg046.htm#SectionNumber:4.1">memorandum</a> in 1991 in which it specifically warned of the dangers of barium in glass production. One prominent U.S. <a href="https://patentimages.storage.googleapis.com/7e/18/e1/18b38abaca0806/US8877663.pdf">patent</a> from 2013 specifically mentions CaO (calcium oxide) as an ideal substitute for metals such as barium and lead.</p>
<p>By contrast, in the early twentieth century barium was a favorite ingredient of glassmakers. As Francis Flint described in his patent (cited above), the use of barium sulfate increased brilliance, but the sulfates needed then to be reduced to prevent small seeds forming in the glass mixture, thus reducing the quality of the glass. Flint recommended zinc, magnesium, aluminum or tin as reducing agents. Sodium and calcium have been recommended in more modern literature of the art.[3] In window non-float glass, aluminum is positively correlated with barium.</p>
<p>It would seem good practice to analyze the correlations of each of these glass types separately, as we have done, since obviously the desired qualities of the glass will differ depending on the uses to which the glass will be put, and thus recipes will differ accordingly. The desired refractive index and brilliance of vehicle float glass will be far different than that of container or tableware glass, for example.</p>
<p>The elemental correlations in this 1987 database suggest changing priorities between production techniques of the early twentieth century and production techniques of the 1980s, toward more utilitarian styles and techniques. One does not require statistics to observe that container glass with high refractive index has become less common, if it ever was, giving way to containers in which seeing the contents clearly through the glass is the overriding concern. This change will probably affect what specific types of glass that a machine learning model can correctly predict for much earlier samples that it has not seen.</p>
<p>Now we turn to our last question.</p>
</section>
<section id="using-machine-learning-to-train-a-model-on-the-1987-database-can-that-model-be-used-to-identify-the-type-container-of-one-or-both-of-the-jars-unseen-by-the-model" class="level3">
<h3 class="anchored" data-anchor-id="using-machine-learning-to-train-a-model-on-the-1987-database-can-that-model-be-used-to-identify-the-type-container-of-one-or-both-of-the-jars-unseen-by-the-model">Using machine learning to train a model on the 1987 database, can that model be used to identify the type (container) of one or both of the jars unseen by the model?</h3>
<p>This should be a straightforward machine learning classification problem. The first step is to gather up relevant modules from various sci-kit learn and imbalanced learning libraries, using multiple import statements.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, mean_absolute_error, cohen_kappa_score, classification_report</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold <span class="im">as</span> SKF</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression, LogisticRegression, Lasso, ElasticNet</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor, KNeighborsClassifier</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RepeatedStratifiedKFold</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest, f_classif</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> set_printoptions, where</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="setting-the-stage-for-an-attractive-output" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-stage-for-an-attractive-output">Setting the stage for an attractive output</h3>
<p>Next, we will address the fact that the output we want to see needs to be prepared for attractive presentation. After we have generated predictions for the identity of the jars, we will generate, additionally, probability reports that outline the likelihood of the predictions. The standard probability scores supplied by the predict_proba function appear rather clumsily, as:</p>
<p>[[0.42 0.4 0.07 0.04 0. 0.07]]</p>
<p>This output is a sequential list of probabilities that corresponds to the sequence of integer types supplied by the original 1987 database. The sequence is implied; that is, the 0.42 above is presumed to be the first type, Window Float, the 0.4 is presumed to be the second, and so on.</p>
<p>In their stead, it would be nice to have an equivalent and more readable report that formats the 2d array to appear like this:</p>
<pre><code>The Nikumaroro jar has a probability of:
A% to be a Window Float
B% to be a Window Non-Float
C% to be a Vehicle Float
D% to be a Headlamp
E% to be a Container
F% to be a Tableware</code></pre>
<p>To achieve this, we will want to re-sort the 2D list by descending order of probability, but when this is done, the implied sequence will no longer be of any use in identifying the glass types.</p>
<p>To solve for this, we will make an explicit counter variable. This new explicit counter, once created, will correspond with the original integer type values supplied by the 1987 database and will not be subject to the alteration that the implied index would experience when sorting.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> colorama <span class="im">import</span> Fore, Back, Style</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_nicer_probability_output(array_to_convert,title):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Author: Joe Cerniglia</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Date: March 20, 2022</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    A function to convert the standard probability scores in Python machine learning libraries</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    to a report that supplies the categories from a dictionary and sorts the list of probabilities in</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    descending order.</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    array_to_convert: an array to convert to a sorted 2d list </span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    title: some text to be placed in the title or headings of the report</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Preconditions:</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">    The array must have the exact number of elements and format needed for the dictionary</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">    and must be the output of a call to model.predict_proba</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">    returns None. The function itself prints the report.</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    prob_list <span class="op">=</span> array_to_convert.tolist()</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(prob_list)</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The counter can be used as a dictionary key for a </span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dictionary we will create in the next step</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    counter<span class="op">=</span><span class="dv">0</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> probability <span class="kw">in</span> prob_list[<span class="dv">0</span>]:</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        prob_list[<span class="dv">0</span>][counter]<span class="op">=</span>[counter,probability]</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        counter<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(prob_list)</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We have a 3d list; get back to the 2d list</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    prob_list<span class="op">=</span>prob_list[<span class="dv">0</span>]</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort in descending order the second column of each row in the 2d list</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This allows for a descending order of probability and for the</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predicted type (highest probability) to rise to the top of the list</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The lambda expression below takes each line (l) of the list as its </span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># argument. The expression after the colon, l[1], is the function, </span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># which takes the second variable in the line and sorts the line in </span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># descending order by that variable</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    prob_list<span class="op">=</span><span class="bu">sorted</span>(prob_list,key<span class="op">=</span><span class="kw">lambda</span> l:l[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    counter<span class="op">=</span><span class="dv">0</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> prediction <span class="kw">in</span> prob_list:</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> counter<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">'The '</span> <span class="op">+</span> title <span class="op">+</span> <span class="st">' has a probability of:'</span>)</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> prediction[<span class="dv">0</span>]<span class="op">==</span><span class="dv">3</span>:</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>            pred<span class="op">=</span>Fore.RED <span class="op">+</span> probdict[prediction[<span class="dv">0</span>]]</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>            pred<span class="op">=</span>Fore.BLACK <span class="op">+</span> probdict[prediction[<span class="dv">0</span>]]</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">"</span><span class="sc">{:.0%}</span><span class="st">"</span>.<span class="bu">format</span>(prediction[<span class="dv">1</span>]),<span class="st">'to be a'</span>, pred)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        counter<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There is one last piece of code that is needed to have this function work properly. We need to define a dictionary that will translate the counter variable above into English. This dictionary is called in the last for-loop of the function above. Notice that this dictionary is different than the one we created at the beginning, since it is now indexed sequentially starting at 0 and explicitly omits Vehicle Non-Float, a category for which there exist no examples in the database. Note also that we will only use this dictionary inside this function. When we need to translate the numerical glass types found in the 1987 database elsewhere in this code, we will still use the dictionary (dict) that we defined earlier.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This dictionary orders the possibilities of the 2d</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>probdict <span class="op">=</span> {<span class="dv">0</span>: <span class="st">'Window Float'</span>,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span>: <span class="st">'Window Non-Float'</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="dv">2</span>: <span class="st">'Vehicle Float'</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="dv">3</span>: <span class="st">'Container'</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="dv">4</span>: <span class="st">'Tableware'</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">5</span>: <span class="st">'Headlamp'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="split-the-data-into-feature-set-x-and-target-set-y." class="level3">
<h3 class="anchored" data-anchor-id="split-the-data-into-feature-set-x-and-target-set-y.">Split the data into feature set X and target set Y.</h3>
<p>Now with the correct modules imported, and our utility function defined, we can split the database into two arrays. First, we can use the pandas dataframe values method to convert the entire dataframe we set aside earlier to an array. Next, we can slice the array column-wise into X and Y arrays, with X as our features array and Y as our target array. Note that our columns are sliced from the number 1, which is actually the second variable in the array, not from the number 0. The reason is that we need to drop the ID variable. The ID variable is an index. It would improve the accuracy of our machine learning model (to 100% in fact!), but this accuracy would be a mirage. It would not accomplish our goal of training the model in how to classify unseen examples.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>array <span class="op">=</span> dataset_ml.values</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(array))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> array[:,<span class="dv">1</span>:<span class="dv">10</span>]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> array[:,<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
</div>
</section>
<section id="show-the-list-of-data-features-ranked-by-their-power-to-influence-prediction-of-the-target-variable." class="level3">
<h3 class="anchored" data-anchor-id="show-the-list-of-data-features-ranked-by-their-power-to-influence-prediction-of-the-target-variable.">Show the list of data features ranked by their power to influence prediction of the target variable.</h3>
<p>Earlier, we stated that we would make some educated guesses as to the values of the jars’ features that were either unavailable (refractive index) or not stated in the required units of weight percent (Fe, K, Si). We did not do this without some trepidation, since tampering with these features’ values would appear to reduce the rigor of our analysis. However, the benefits of having a complete training dataset appeared to outweigh these drawbacks.</p>
<p>The impact of this decision was unknown, but it was not unknowable. There is a method to assess the power of a given feature to influence the classification of a given sample of glass. This method is known as feature selection. Using feature selection, we may see a list of all the features in the dataset ranked in order of strongest to least strongest influence on the prediction of the class. If the features we modified were ranked highly in this list, we should be concerned about the integrity of the analysis. If the features we modified were not ranked highly in this list, we can proceed with our analysis with the confidence that the algorithm will be untroubled by our expedient modifications to the original data.*</p>
<p>The results were as favorable as one might have hoped. Using the SelectKBest class from the scikit-learn library, we can see that the variables for which we needed to supply values (highlighted in gold) were ranked toward the bottom of the ranked list.</p>
<p>The code that appears below takes the Numpy array created by SelectKBest and transforms it into a concise report that lists the relative importance, ranked descending, for all of the features in the glass database.</p>
<section id="note-that-we-also-evaluated-the-effect-of-the-features-empirically.-an-additional-experiment-to-run-this-program-with-a-range-of-values-for-k-si-fe-and-ref_ix-did-not-materially-alter-the-predictions-of-the-machine-learning-algorithm." class="level4">
<h4 class="anchored" data-anchor-id="note-that-we-also-evaluated-the-effect-of-the-features-empirically.-an-additional-experiment-to-run-this-program-with-a-range-of-values-for-k-si-fe-and-ref_ix-did-not-materially-alter-the-predictions-of-the-machine-learning-algorithm.">(*Note that we also evaluated the effect of the features empirically. An additional experiment to run this program with a range of values for K, Si, Fe, and Ref_ix did not materially alter the predictions of the machine learning algorithm.)</h4>
<div class="cell" data-scrolled="true" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>names<span class="op">=</span>[<span class="st">'Ref_ix'</span>,<span class="st">'Na'</span>,<span class="st">'Mg'</span>,<span class="st">'Al'</span>,<span class="st">'Si'</span>,<span class="st">'K'</span>,<span class="st">'Ca'</span>,<span class="st">'Ba'</span>,<span class="st">'Fe'</span>]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># feature extraction using univariate selection</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_classif, k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>fit <span class="op">=</span> test.fit(X, Y)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize scores</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert scores to dataframe</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>df_scores <span class="op">=</span> pd.DataFrame(fit.scores_)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert names list to dataframe</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>df_names <span class="op">=</span> pd.DataFrame(names) </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Join two dataframes by indexes</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>best_features<span class="op">=</span>pd.concat([df_scores, df_names], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co"># change the type of the columns from int to str</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>best_features.columns <span class="op">=</span> best_features.columns.astype(<span class="bu">str</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># rename the columns to have more sensible names</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>best_features.columns.values[<span class="dv">0</span>] <span class="op">=</span> <span class="st">"Score"</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>best_features.columns.values[<span class="dv">1</span>] <span class="op">=</span> <span class="st">"Feature_Name"</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the rows (features) by rank in descending order</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>best_features.sort_values(by<span class="op">=</span><span class="st">'Score'</span>, ascending<span class="op">=</span><span class="va">False</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a column for rank to the dataframe</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>best_features[<span class="st">'Rank'</span>] <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="bu">len</span>(best_features)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-order the columns</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>best_features <span class="op">=</span> best_features[[<span class="st">'Feature_Name'</span>, <span class="st">'Score'</span>,<span class="st">'Rank'</span>]]</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Format the dataframe to give the score two decimal places</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>best_features <span class="op">=</span> best_features.style.<span class="bu">format</span>({<span class="st">'Score'</span>: <span class="st">"</span><span class="sc">{:.2f}</span><span class="st">"</span>})</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="co">#print(type(best_features))</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>best_features<span class="op">=</span>best_features.data</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> color_relevant_gold(row):</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Takes a dataframe row and returns a string with</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="co">    the background-color property `'background-color: gold'` for relevant</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a><span class="co">    rows, black otherwise.</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (row.values[<span class="dv">0</span>] <span class="op">==</span> <span class="st">'K'</span>  <span class="kw">or</span> row.values[<span class="dv">0</span>] <span class="op">==</span> <span class="st">'Ref_ix'</span> <span class="kw">or</span> row.values[<span class="dv">0</span>] <span class="op">==</span> <span class="st">'Si'</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>        <span class="kw">or</span> row.values[<span class="dv">0</span>] <span class="op">==</span> <span class="st">'Fe'</span>):</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> <span class="st">'gold'</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> <span class="st">''</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="st">'background-color: </span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> color]<span class="op">*</span><span class="bu">len</span>(row.values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'      ----Best features----'</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>best_features.style.<span class="bu">apply</span>(color_relevant_gold, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      ----Best features----</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="16">
<style type="text/css">
#T_2ee44_row0_col0, #T_2ee44_row0_col1, #T_2ee44_row0_col2, #T_2ee44_row1_col0, #T_2ee44_row1_col1, #T_2ee44_row1_col2, #T_2ee44_row2_col0, #T_2ee44_row2_col1, #T_2ee44_row2_col2, #T_2ee44_row3_col0, #T_2ee44_row3_col1, #T_2ee44_row3_col2, #T_2ee44_row5_col0, #T_2ee44_row5_col1, #T_2ee44_row5_col2 {
  background-color: ;
}
#T_2ee44_row4_col0, #T_2ee44_row4_col1, #T_2ee44_row4_col2, #T_2ee44_row6_col0, #T_2ee44_row6_col1, #T_2ee44_row6_col2, #T_2ee44_row7_col0, #T_2ee44_row7_col1, #T_2ee44_row7_col2, #T_2ee44_row8_col0, #T_2ee44_row8_col1, #T_2ee44_row8_col2 {
  background-color: gold;
}
</style>

<table id="T_2ee44_" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th class="blank level0" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="col_heading level0 col0" data-quarto-table-cell-role="th">Feature_Name</th>
<th class="col_heading level0 col1" data-quarto-table-cell-role="th">Score</th>
<th class="col_heading level0 col2" data-quarto-table-cell-role="th">Rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_2ee44_level0_row0" class="row_heading level0 row0" data-quarto-table-cell-role="th">2</td>
<td id="T_2ee44_row0_col0" class="data row0 col0">Mg</td>
<td id="T_2ee44_row0_col1" class="data row0 col1">65.544521</td>
<td id="T_2ee44_row0_col2" class="data row0 col2">1</td>
</tr>
<tr class="even">
<td id="T_2ee44_level0_row1" class="row_heading level0 row1" data-quarto-table-cell-role="th">7</td>
<td id="T_2ee44_row1_col0" class="data row1 col0">Ba</td>
<td id="T_2ee44_row1_col1" class="data row1 col1">38.974602</td>
<td id="T_2ee44_row1_col2" class="data row1 col2">2</td>
</tr>
<tr class="odd">
<td id="T_2ee44_level0_row2" class="row_heading level0 row2" data-quarto-table-cell-role="th">3</td>
<td id="T_2ee44_row2_col0" class="data row2 col0">Al</td>
<td id="T_2ee44_row2_col1" class="data row2 col1">35.726676</td>
<td id="T_2ee44_row2_col2" class="data row2 col2">3</td>
</tr>
<tr class="even">
<td id="T_2ee44_level0_row3" class="row_heading level0 row3" data-quarto-table-cell-role="th">1</td>
<td id="T_2ee44_row3_col0" class="data row3 col0">Na</td>
<td id="T_2ee44_row3_col1" class="data row3 col1">28.548019</td>
<td id="T_2ee44_row3_col2" class="data row3 col2">4</td>
</tr>
<tr class="odd">
<td id="T_2ee44_level0_row4" class="row_heading level0 row4" data-quarto-table-cell-role="th">5</td>
<td id="T_2ee44_row4_col0" class="data row4 col0">K</td>
<td id="T_2ee44_row4_col1" class="data row4 col1">8.748128</td>
<td id="T_2ee44_row4_col2" class="data row4 col2">5</td>
</tr>
<tr class="even">
<td id="T_2ee44_level0_row5" class="row_heading level0 row5" data-quarto-table-cell-role="th">6</td>
<td id="T_2ee44_row5_col0" class="data row5 col0">Ca</td>
<td id="T_2ee44_row5_col1" class="data row5 col1">2.971426</td>
<td id="T_2ee44_row5_col2" class="data row5 col2">6</td>
</tr>
<tr class="odd">
<td id="T_2ee44_level0_row6" class="row_heading level0 row6" data-quarto-table-cell-role="th">4</td>
<td id="T_2ee44_row6_col0" class="data row6 col0">Si</td>
<td id="T_2ee44_row6_col1" class="data row6 col1">2.787330</td>
<td id="T_2ee44_row6_col2" class="data row6 col2">7</td>
</tr>
<tr class="even">
<td id="T_2ee44_level0_row7" class="row_heading level0 row7" data-quarto-table-cell-role="th">8</td>
<td id="T_2ee44_row7_col0" class="data row7 col0">Fe</td>
<td id="T_2ee44_row7_col1" class="data row7 col1">2.710882</td>
<td id="T_2ee44_row7_col2" class="data row7 col2">8</td>
</tr>
<tr class="odd">
<td id="T_2ee44_level0_row8" class="row_heading level0 row8" data-quarto-table-cell-role="th">0</td>
<td id="T_2ee44_row8_col0" class="data row8 col0">Ref_ix</td>
<td id="T_2ee44_row8_col1" class="data row8 col1">1.608955</td>
<td id="T_2ee44_row8_col2" class="data row8 col2">9</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
<section id="split-the-data-into-training-and-validation-sets." class="level3">
<h3 class="anchored" data-anchor-id="split-the-data-into-training-and-validation-sets.">Split the data into training and validation sets.</h3>
<p>To create the conditions under which we will have the ability to test our future model’s effectiveness at classifying unseen data, we can now split both X and Y into validation and training arrays. The “train” pair of arrays can then be used for training the model, and the “validation” pair of arrays can be used to demonstrate how effective the model is after we have trained it. To do this, we will use the train_test_split function from sklearn’s model_selection library. We will create an 80-20 split of the data by entering a test_size parameter of 0.20. The training set will be 80% of the data, and the validation set will be 20% of the data. We will stratify the data so that the relative proportion of glass types in both pairs of training and validation sets is equivalent, despite the fact that the arrays are of different overall sizes.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>validation_size <span class="op">=</span> <span class="fl">0.20</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>X_train, X_validation, Y_train, Y_validation <span class="op">=</span> train_test_split(X, Y,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span>validation_size, random_state<span class="op">=</span>seed, stratify<span class="op">=</span>Y)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(X_train))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
</div>
</section>
<section id="add-the-nikumaroro-jar-and-clear-facsimile-into-the-x-and-y-validation-datasets." class="level3">
<h3 class="anchored" data-anchor-id="add-the-nikumaroro-jar-and-clear-facsimile-into-the-x-and-y-validation-datasets.">Add the Nikumaroro jar and clear facsimile into the X and Y validation datasets.</h3>
<p>Next, we need to add our two jars (clear facsimile and Nikumaroro jar) to the validation arrays. The features of these two jars will be added to X_validation, and the target (glass type) of these two jars will be added to Y_validation. Having accomplished this, we will then have added two glass samples that the study authors could not possibly have anticipated when they built their database in 1987. This will be a great test of the skill of machine learning algorithms in general and of the completeness of their original dataset in particular.</p>
<p>To demonstrate that our jars have been added successfully, we will print out the shape of the validation arrays both before and after performing the append operations. Note that the shape returns a tuple, providing the number of rows, followed by the number of columns, separated by a comma.</p>
<p>The append operations for arrays are a little tricky. To do this, in addition to using numpy’s append method, we need to chain to that the reshape method to size the array appropriately prior to appending to it. The chained command executes from right to left, first reshaping the array to which we are appending, and then performing the append operation itself.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">#names=</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  ['Ref_ix','Na',  'Mg',   'Al', 'Si',  'K',  'Ca',  'Ba',  'Fe']</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>artifact_features<span class="op">=</span>[<span class="fl">1.52369</span>,  <span class="fl">13.1</span>,  <span class="fl">4.3</span>,  <span class="fl">.74</span>,   <span class="fl">72.37</span>,  <span class="fl">.24</span>,  <span class="fl">8.5</span>,  <span class="fl">.74</span>,  <span class="fl">.02</span>]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>facsimile_features<span class="op">=</span>[<span class="fl">1.51316</span>, <span class="fl">11.7</span>,  <span class="fl">2.4</span>,  <span class="fl">.85</span>,   <span class="fl">72.37</span>,  <span class="fl">.12</span>,  <span class="fl">3.6</span>,  <span class="fl">.37</span>,  <span class="fl">.01</span>]</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>artifact_features_array<span class="op">=</span>np.array(artifact_features)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>facsimile_features_array<span class="op">=</span>np.array(facsimile_features)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of X_validation before adding jars:'</span>,X_validation.shape)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>X_validation<span class="op">=</span>np.append(</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>X_validation,artifact_features_array).reshape(X_validation.shape[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>,<span class="dv">9</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>X_validation<span class="op">=</span>np.append(</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>X_validation,facsimile_features_array).reshape(X_validation.shape[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>,<span class="dv">9</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of X_validation after adding jars:'</span>,X_validation.shape)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 is equal to a container, the actual identity of the Nikumaroro jar and of the facsimile</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>artifact_identity<span class="op">=</span>[<span class="fl">5.0</span>]</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>facsimile_identity<span class="op">=</span>[<span class="fl">5.0</span>]</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>artifact_array<span class="op">=</span>np.array(artifact_identity)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>facsimile_array<span class="op">=</span>np.array(facsimile_identity)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of Y_validation before adding jars:'</span>,Y_validation.shape)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>Y_validation<span class="op">=</span>np.append(</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>Y_validation,artifact_array).reshape(Y_validation.shape[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>Y_validation<span class="op">=</span>np.append(</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>Y_validation,facsimile_array).reshape(Y_validation.shape[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of Y_validation after adding jars:'</span>,Y_validation.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>shape of X_validation before adding jars: (43, 9)
shape of X_validation after adding jars: (45, 9)
shape of Y_validation before adding jars: (43,)
shape of Y_validation after adding jars: (45,)</code></pre>
</div>
</div>
</section>
<section id="create-a-list-of-models-to-begin-the-testing-process-of-choosing-the-best-one." class="level3">
<h3 class="anchored" data-anchor-id="create-a-list-of-models-to-begin-the-testing-process-of-choosing-the-best-one.">Create a list of models to begin the testing process of choosing the best one.</h3>
<p>Now comes the heart of the machine learning program, the creation of models, also known as machine learning algorithms. There are many different kinds of machine learning algorithms. Each has its own strengths and weaknesses. The skill of machine learning algorithms on particular datasets will vary. Some will show very little skill in exposing the structure of the data, resulting in a model that cannot classify unseen glass examples very accurately. Others will show much greater skill. We want to test a variety of machine learning algorithms on the data so that we can select the model that is most likely to succeed in classifying the type of glass for this particular set of data.</p>
<p>Machine learning algorithms come in two basic varieties: regression and classification. Regression algorithms treat our target variable (glass type) as a continuous variable. This means that they consider the targets as floating point numbers rather than as discrete integers. Classification algorithms treat the target variable as discrete categories. There is every advantage in testing both varieties, rather than trying to anticipate in advance which type of model might perform best.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'LR'</span>, LinearRegression()))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'LASSO'</span>, Lasso()))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'EN'</span>, ElasticNet()))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'KNN'</span>, KNeighborsRegressor(n_neighbors<span class="op">=</span><span class="dv">5</span>)))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'CART Regressor'</span>, DecisionTreeRegressor()))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'SVR'</span>, SVR()))</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'KNClass'</span>,KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>)))</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>models.append((<span class="st">'RandomForest'</span>,RandomForestClassifier(</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>n_estimators<span class="op">=</span><span class="dv">100</span>, max_features<span class="op">=</span><span class="dv">9</span>,class_weight<span class="op">=</span><span class="st">'balanced'</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="score-the-models." class="level3">
<h3 class="anchored" data-anchor-id="score-the-models.">Score the models.</h3>
<p>There are many methods for scoring the effectiveness of different models so that they may be compared side by side. Some of these methods work optimally for regression models; others work optimally for classification models. The negative mean squared error, while not optimal for classification models, is acceptable for use in multi-class datasets such as this one, and it works very well for regression models. Thus, it is a good ‘all-purpose’ scoring method for our needs.</p>
<p>What this code snippet does is to evaluate the success of each of the models defined above by repeatedly testing them on different randomized ‘folds’ of the data. The results of each test will not be the same. Each will vary to a lesser or greater extent, depending on which fold is selected. The score is actually a composite result of repeated tests on many folds. The negative mean square error score is expressed as a negative number, such that the highest negative number, that which is closest to zero, will be considered to have the best score. The standard deviation, given in parentheses, provides a sense of how much variation to expect within a given score.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test options and evaluation metric</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> <span class="st">'neg_mean_squared_error'</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate each model in turn</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> []</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Evaluation of Mean Square Error results of different models'</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models:</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  kfold <span class="op">=</span> RepeatedStratifiedKFold(n_splits<span class="op">=</span><span class="dv">7</span>, n_repeats<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  cv_results <span class="op">=</span> cross_val_score(</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>  model, X_train, Y_train, cv<span class="op">=</span>kfold, scoring<span class="op">=</span>scoring)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>  results.append(cv_results)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>  names.append(name)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>  msg <span class="op">=</span> <span class="st">"</span><span class="sc">%s</span><span class="st">: </span><span class="sc">%f</span><span class="st"> (</span><span class="sc">%f</span><span class="st">)"</span> <span class="op">%</span> (name, cv_results.mean(), cv_results.std())</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(msg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Evaluation of Mean Square Error results of different models
LR: -1.206907 (0.388832)
LASSO: -2.496606 (0.292277)
EN: -2.112353 (0.293161)
KNN: -0.980076 (0.386747)
CART Regressor: -1.281429 (0.594722)
SVR: -4.767184 (0.520934)
KNClass: -1.683810 (0.841334)
RandomForest: -1.137222 (0.696663)</code></pre>
</div>
</div>
</section>
<section id="graph-the-score-results." class="level3">
<h3 class="anchored" data-anchor-id="graph-the-score-results.">Graph the score results.</h3>
<p>The test results above are complete, but they are a little hard to read. It would be far easier to evaluate a graphical representation of the data using a box plot. The box plot below takes each machine learning algorithm and places it alongside its neighbors for easy comparison. Note that there are tuning and scaling operations that might have incrementally increased the accuracy of our models, which have not been employed here.</p>
<p>Based on the graph, it would appear that the Random Forest Classifier, a model with an excellent reputation among classification models, is an effective model, but KNN and Cart Regressor also seem competitive. In actual practice, however, KNN and CART produced a far less accurate result. When testing these models, the resulting confusion matrix (see below for an explanation of the confusion matrix) showed that these models are actually fairly weak.</p>
<p>This illustrates an important point: Multiple scoring methods are often required to see which model offers the best results. Still, a box graph such as the one shown below can narrow the number of judgments that must be made.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a> <span class="co"># Compare Algorithms</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'UNTUNED Unscaled Algorithm Comparison'</span>,fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>plt.boxplot(results)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(names)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>plt.gcf().set_size_inches(<span class="dv">15</span>, <span class="dv">7</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>plt.yticks(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="oversampling-explained" class="level3">
<h3 class="anchored" data-anchor-id="oversampling-explained">Oversampling, explained</h3>
<p>Random Forest often pairs well with oversampling techniques, such as SMOTE (Synthetic Minority Over-sampling Technique). What SMOTE will do is to magnify certain classes that are sparsely represented in the training dataset. This magnification will allow the training process to resolve these under-represented classes more effectively, because it will have seen more examples than were initially presented. For example, tableware has only seven examples in the training dataset. By applying SMOTE to the training dataset, this number is magnified to 61. In fact, all of the glass types are ‘leveled’ to 61 examples after SMOTE is applied. This leveling will make it much easier for the Random Forest Classifier to classify unseen examples to a greater level of accuracy.</p>
<p>Now we can observe a comparison of counts for each of the glass types before and after the application of SMOTE. This is only a test to see what the counts would be. Later on, we will apply SMOTE to the definition of our model. Our dictionary will come in handy here to translate the numerical glass types found in the 1987 database to their English equivalents.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>counter<span class="op">=</span>Counter(Y_train)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Counts Before SMOTE:'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ele <span class="kw">in</span> counter:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">dict</span>[<span class="bu">int</span>(ele)],<span class="st">':'</span>,counter[ele])</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>oversample <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">42</span>,k_neighbors<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>X_trainsm, Y_trainsm <span class="op">=</span> oversample.fit_resample(X_train, Y_train)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>countersm<span class="op">=</span>Counter(Y_trainsm)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Counts After SMOTE:'</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ele <span class="kw">in</span> countersm:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">dict</span>[<span class="bu">int</span>(ele)],<span class="st">':'</span>,countersm[ele])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Counts Before SMOTE:
Window Float : 56
Headlamp : 23
Vehicle Float : 14
Window Non-Float : 61
Tableware : 7
Container : 10

Counts After SMOTE:
Window Float : 61
Headlamp : 61
Vehicle Float : 61
Window Non-Float : 61
Tableware : 61
Container : 61</code></pre>
</div>
</div>
</section>
<section id="visualizing-smote" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-smote">Visualizing SMOTE</h3>
<p>Seeing the change in counts before and after the application of SMOTE oversampling is an excellent way of visualizing what SMOTE does. Another way to do this is to create a scatter plot on two elements of the periodic table and compare where the points are plotted before and after the application of SMOTE. The graphs following this function encode each dot with a color. The legend in the upper right corner of the graph shows which glass type each color represents.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>names<span class="op">=</span>[<span class="st">'Ref_ix'</span>,<span class="st">'Na'</span>,<span class="st">'Mg'</span>,<span class="st">'Al'</span>,<span class="st">'Si'</span>,<span class="st">'K'</span>,<span class="st">'Ca'</span>,<span class="st">'Ba'</span>,<span class="st">'Fe'</span>]</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> smotegraph(title_suffix, var_suffix<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Author: Joe Cerniglia</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Date: September 17, 2022</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">    A function to plot two scatter graphs to show the distribution of glass types over two </span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">    elements of the periodic table.</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">    title_suffix: string that provides title text for which training dataset has been </span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">    graphed, original or SMOTE-oversampled.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">    var_suffix: string to indicate which training dataset to use in the plot</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">    returns None. The function produces graphs but does not return anything outside the function itself.</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># scatter plots of examples by class label: Aluminum (3) x Iron (8)</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label, _ <span class="kw">in</span> counter.items():</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># row_ix returns an array containing all the row numbers for a given label (color)</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> var_suffix<span class="op">==</span><span class="va">None</span>:</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>            row_ix <span class="op">=</span> where(Y_train <span class="op">==</span> label)[<span class="dv">0</span>] <span class="co">#slicing with zero allows access </span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>                <span class="co">#to the array embedded inside the tuple returned by the where</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            plt.scatter(</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>                X_train[row_ix, <span class="dv">3</span>], X_train[row_ix, <span class="dv">8</span>], label<span class="op">=</span><span class="bu">dict</span>[label], alpha<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            row_ix <span class="op">=</span> where(Y_trainsm <span class="op">==</span> label)[<span class="dv">0</span>]</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>            plt.scatter(</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>                X_trainsm[row_ix, <span class="dv">3</span>], X_trainsm[row_ix, <span class="dv">8</span>], label<span class="op">=</span><span class="bu">dict</span>[label], alpha<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>    plt.legend(markerscale<span class="op">=</span><span class="fl">1.5</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Glass of the UK database by Type"</span> <span class="op">+</span> title_suffix, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Al weight %"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Fe </span><span class="ch">\n</span><span class="st"> weight %"</span>,fontsize<span class="op">=</span><span class="dv">16</span>, rotation<span class="op">=</span><span class="dv">0</span>, labelpad<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>smotegraph(<span class="st">' - before SMOTE oversampling'</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>smotegraph(<span class="st">' - after SMOTE oversampling'</span>,<span class="st">'sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-24-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The second graph clearly shows the number of dots on the graph increased. SMOTE creates observations that did not exist in the original U.K. database but which are nearest neighbors. Note especially how the upper right quadrant now has a number of containers (brown dots) where only one existed before! These observations have moderate amounts of iron in them. Iron increases refractive index, which generally means that glass with high iron content is less transparent. Since containers, especially for products, once equated translucency with luxury, these synthetic observations created by SMOTE seem plausible. Note also the row of purple dots at the bottom of the graph. These are tableware samples created by SMOTE that have low iron content, and thus are highly transparent, which seems quite correct for tableware.</p>
</section>
<section id="build-data-pipelines-that-can-be-used-to-input-the-training-data." class="level3">
<h3 class="anchored" data-anchor-id="build-data-pipelines-that-can-be-used-to-input-the-training-data.">Build data pipelines that can be used to input the training data.</h3>
<p>Now that we have seen what the counts would be for an oversampled training dataset, we can create a data pipeline that combines SMOTE with our previously selected model, Random Forest Classifier. To test the efficacy of SMOTE, we should create two models. The first model we create will have the ability to apply a Random Forest Classifier to a non-oversampled dataset. The second will have the ability to apply Random Forest Classifier to an oversampled dataset. We will also want to add in a standard scaler to both model pipelines. The standard scaler takes features of different scale and makes them more similar in scale. This technique is very useful in further enhancing the model’s ability to classify. Note that even after we have defined our model pipelines, we have not yet supplied any data to them.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>modelnoSMOTE<span class="op">=</span>make_pipeline(</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>StandardScaler(),RandomForestClassifier(</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>n_estimators<span class="op">=</span><span class="dv">100</span>,max_features<span class="op">=</span><span class="dv">9</span>,class_weight<span class="op">=</span><span class="st">'balanced'</span>))</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>modelwithSMOTE<span class="op">=</span>make_pipeline(</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>StandardScaler(),SMOTE(</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>random_state<span class="op">=</span><span class="dv">42</span>, k_neighbors<span class="op">=</span><span class="dv">5</span>),RandomForestClassifier(</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>n_estimators<span class="op">=</span><span class="dv">100</span>,max_features<span class="op">=</span><span class="dv">9</span>,class_weight<span class="op">=</span><span class="st">'balanced'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="fit-the-non-smote-model-to-the-training-data-to-train-the-model." class="level3">
<h3 class="anchored" data-anchor-id="fit-the-non-smote-model-to-the-training-data-to-train-the-model.">Fit the non-SMOTE model to the training data to train the model.</h3>
<p>First, we will use and evaluate the non-SMOTE model. We will apply the non-SMOTE model to actual data by supplying it with our training data. The fit method applies the model by using the training features and targets to learn how to generalize to unseen validation data.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>modelnoSMOTE.fit(X_train, Y_train)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="apply-the-trained-non-smote-model-to-unseen-data." class="level3">
<h3 class="anchored" data-anchor-id="apply-the-trained-non-smote-model-to-unseen-data.">Apply the trained non-SMOTE model to unseen data.</h3>
<p>Now we can test the model’s performance by applying it to unseen data (X_validation) and generating a report on the accuracy of its performance.</p>
<p>The variable called predictions takes our validation features and attempts to classify what type of glass the features for each example represents. Recall that we added in the two jars to our validation dataset, and so these predictions will include them.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> modelnoSMOTE.predict(X_validation)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="how-to-read-a-confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="how-to-read-a-confusion-matrix">How to Read a Confusion Matrix</h3>
<p>The confusion matrix need not be confusing. It provides all the predictions in a matrix. Think of the columns and rows of the matrix as having been labeled with the names of the integer glass types, ordered left to right and top to bottom, in sequence. Row labels represent the true values, and column labels represent the predicted values. The intersection of true and predicted represents the status of any given cell in the matrix. Using this matrix, we can tell how many predictions were correct, how many were over-predicted, and how many were under-predicted. The diagonal line that can be drawn from the top left of the matrix to the bottom right, slicing the matrix into two equal diagonal halves, represents correct predictions (true=predicted). Each number in the same row as a correct prediction represents an under-prediction. In other words, the model under-predicts when the number of correct predictions for a category is less than the actual number in that category. By summing all of the numerals in any given row, we obtain the true number of glass types for each type. Each number in the same column as a correct prediction represents an over-prediction. In other words, the model over-predicts when the number of total predictions for a category in a given column is more than the correct predictions in that category.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest without SMOTE oversampling'</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(Y_validation, predictions))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest without SMOTE oversampling
[[13  1  0  0  0  0]
 [ 1 13  0  1  0  0]
 [ 1  1  1  0  0  0]
 [ 1  0  0  3  0  1]
 [ 0  1  0  0  1  0]
 [ 2  0  0  1  0  3]]</code></pre>
</div>
</div>
</section>
<section id="cohens-kappa" class="level3">
<h3 class="anchored" data-anchor-id="cohens-kappa">Cohen’s Kappa</h3>
<p>The Cohen’s Kappa statistic is a measure of inter-rater reliability. The “raters,” in this machine learning example, are the actual glass types and the predictions of those types. In addition to considering the extent of agreement between these two, it also includes statistical consideration of the probability that this agreement could have occurred by chance.</p>
<p>The kappa score for the non-SMOTE Random Forest model is considered reasonably good.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>kappa<span class="op">=</span>cohen_kappa_score(Y_validation, np.<span class="bu">round</span>(predictions,<span class="dv">0</span>))</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Kappa (1=perfect;0=chance;&lt;0=worse than chance): </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> kappa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Kappa (1=perfect;0=chance;&lt;0=worse than chance): 0.665314</code></pre>
</div>
</div>
</section>
<section id="mean-absolute-deviation" class="level3">
<h3 class="anchored" data-anchor-id="mean-absolute-deviation">Mean absolute deviation</h3>
<p>The mean absolute deviation, or MAD, is a measure of the average distance between each data value and the average value of the dataset. In terms of the confusion matrix above, it provides a sense of how far the values are distributed away from the central diagonal line.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>MAD <span class="op">=</span> mean_absolute_error(Y_validation, predictions)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'MAD (Mean Absolute Dev): </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> MAD)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MAD (Mean Absolute Dev): 0.711111</code></pre>
</div>
</div>
</section>
<section id="the-classification-report" class="level3">
<h3 class="anchored" data-anchor-id="the-classification-report">The classification report</h3>
<p>The classification report calculates precision and recall. Precision is the number of correct guesses in a class divided by the total guesses in a class. It can be obtained by observing the columns in the confusion matrix. Recall is the number of correct guesses in a class divided by the total number of actual members of the class. It can be obtained by observing the rows in the confusion matrix. The f1-score is the harmonic mean of precision and recall and is a good overall indicator of model effectiveness. Support is the true count for each type.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(Y_validation, predictions, zero_division<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         1.0       0.72      0.93      0.81        14
         2.0       0.81      0.87      0.84        15
         3.0       1.00      0.33      0.50         3
         5.0       0.60      0.60      0.60         5
         6.0       1.00      0.50      0.67         2
         7.0       0.75      0.50      0.60         6

    accuracy                           0.76        45
   macro avg       0.81      0.62      0.67        45
weighted avg       0.77      0.76      0.74        45
</code></pre>
</div>
</div>
</section>
<section id="observe-the-predictions-for-the-nikumaroro-jar-and-the-clear-facsimile." class="level3">
<h3 class="anchored" data-anchor-id="observe-the-predictions-for-the-nikumaroro-jar-and-the-clear-facsimile.">Observe the predictions for the Nikumaroro jar and the clear facsimile.</h3>
<p>Since we already added the clear facsimile and Nikumaroro jar to the validation dataset, the model has already made its predictions for these two samples. However, the evaluation thus far has not directly revealed what those predictions actually were. Now we will want to observe for ourselves how the model behaves toward these particular instances of the data. To do this, we can simply enter the array for each sample that we created above into the predict method of the noSMOTE model. We may obtain the underlying probabilities for these predictions by using the predict_proba method of the noSMOTE model.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> modelnoSMOTE.predict([artifact_features])</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Nikumaroro jar prediction:'</span>,<span class="bu">dict</span>[<span class="bu">int</span>(yhat)])</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>yhat2 <span class="op">=</span> modelnoSMOTE.predict([facsimile_features])</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'clear facsimile prediction:'</span>,<span class="bu">dict</span>[<span class="bu">int</span>(yhat2)])</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>yhat_probability <span class="op">=</span> modelnoSMOTE.predict_proba([artifact_features])</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>yhat2_probability <span class="op">=</span> modelnoSMOTE.predict_proba([facsimile_features])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nikumaroro jar prediction: Window Float
clear facsimile prediction: Headlamp</code></pre>
</div>
</div>
</section>
<section id="display-the-details-of-the-jar-prediction-probabilities-for-the-non-smote-model." class="level3">
<h3 class="anchored" data-anchor-id="display-the-details-of-the-jar-prediction-probabilities-for-the-non-smote-model.">Display the details of the jar prediction probabilities for the non-SMOTE model.</h3>
<p>Now we have the opportunity to use the utility display function we created above. Nesting calls to this function inside another function will allow the report to be generated with a single call when we next run a report for the SMOTE version of our model. We can also here include extra niceties such as colored text to highlight probabilities for the Container class and check marks to indicate where the prediction was correct.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> the_report():</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">dict</span>[yhat[<span class="dv">0</span>]]<span class="op">==</span><span class="st">'Container'</span>:</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">'Nikumaroro jar prediction:'</span>,Fore.RED <span class="op">+</span> <span class="bu">dict</span>[yhat[<span class="dv">0</span>]]<span class="op">+</span><span class="st">' '</span> <span class="op">+</span> <span class="st">u'</span><span class="ch">\u2713</span><span class="st">'</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">'Nikumaroro jar prediction:'</span>,Fore.BLACK <span class="op">+</span> <span class="bu">dict</span>[yhat[<span class="dv">0</span>]])</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    make_nicer_probability_output(yhat_probability,<span class="st">'artifact'</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">dict</span>[yhat2[<span class="dv">0</span>]]<span class="op">==</span><span class="st">'Container'</span>:</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">'clear facsimile prediction:'</span>,Fore.RED <span class="op">+</span> <span class="bu">dict</span>[yhat2[<span class="dv">0</span>]]<span class="op">+</span><span class="st">' '</span> <span class="op">+</span> <span class="st">u'</span><span class="ch">\u2713</span><span class="st">'</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(Fore.BLACK <span class="op">+</span> <span class="st">'clear facsimile prediction:'</span>,Fore.BLACK <span class="op">+</span> <span class="bu">dict</span>[yhat2[<span class="dv">0</span>]])</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    make_nicer_probability_output(yhat2_probability,<span class="st">'clear facsimile'</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>the_report()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nikumaroro jar prediction: Window Float
The artifact has a probability of:
46% to be a Window Float
28% to be a Window Non-Float
17% to be a Headlamp
9% to be a Vehicle Float
0% to be a Container
0% to be a Tableware

clear facsimile prediction: Headlamp
The clear facsimile has a probability of:
33% to be a Headlamp
26% to be a Container
19% to be a Window Non-Float
16% to be a Window Float
5% to be a Tableware
1% to be a Vehicle Float</code></pre>
</div>
</div>
</section>
<section id="run-the-report-again-for-the-smote-model." class="level3">
<h3 class="anchored" data-anchor-id="run-the-report-again-for-the-smote-model.">Run the report again for the SMOTE model.</h3>
<p>Having proceeded through a step-by-step analysis of the machine learning classification report we have built, we can now run the same report again, this time using the second SMOTE model we created. This model will, again: 1. oversample the data with SMOTE 2. scale the data with StandardScaler 3. classify each glass example with RandomForestClassifier and 4. output the numerical prediction and probabilities of glass type for the Nikumaroro jar and for the clear facsimile</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>modelwithSMOTE.fit(X_train, Y_train)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest with SMOTE oversampling'</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> modelwithSMOTE.predict(X_validation)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>ytrue<span class="op">=</span><span class="dv">5</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>predictions<span class="op">=</span>np.<span class="bu">round</span>(predictions, <span class="dv">0</span>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(Y_validation, predictions))</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>kappa<span class="op">=</span>cohen_kappa_score(Y_validation, np.<span class="bu">round</span>(predictions,<span class="dv">0</span>))</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>MAD <span class="op">=</span> mean_absolute_error(Y_validation, predictions)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Kappa (1=perfect;0=chance;&lt;0=worse than chance): </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> kappa)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'MAD (Mean Absolute Dev): </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> MAD)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(Y_validation, predictions, zero_division<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> modelwithSMOTE.predict([artifact_features])</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Nikumaroro jar prediction:'</span>,<span class="bu">dict</span>[<span class="bu">int</span>(yhat)])</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>yhat2 <span class="op">=</span> modelwithSMOTE.predict([facsimile_features])</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'clear facsimile prediction:'</span>,<span class="bu">dict</span>[<span class="bu">int</span>(yhat2)])</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>yhat_probability <span class="op">=</span> modelwithSMOTE.predict_proba([artifact_features])</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>yhat2_probability <span class="op">=</span> modelwithSMOTE.predict_proba([facsimile_features])   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Random Forest with SMOTE oversampling
[[12  2  0  0  0  0]
 [ 1 13  0  1  0  0]
 [ 1  0  2  0  0  0]
 [ 0  1  0  4  0  0]
 [ 0  0  0  0  2  0]
 [ 2  0  0  1  0  3]]
Kappa (1=perfect;0=chance;&lt;0=worse than chance): 0.730539
MAD (Mean Absolute Dev): 0.555556
              precision    recall  f1-score   support

         1.0       0.75      0.86      0.80        14
         2.0       0.81      0.87      0.84        15
         3.0       1.00      0.67      0.80         3
         5.0       0.67      0.80      0.73         5
         6.0       1.00      1.00      1.00         2
         7.0       1.00      0.50      0.67         6

    accuracy                           0.80        45
   macro avg       0.87      0.78      0.81        45
weighted avg       0.82      0.80      0.80        45

Nikumaroro jar prediction: Window Non-Float
clear facsimile prediction: Container</code></pre>
</div>
</div>
</section>
<section id="display-the-detail-of-jar-prediction-probabilities-for-the-smote-model." class="level3">
<h3 class="anchored" data-anchor-id="display-the-detail-of-jar-prediction-probabilities-for-the-smote-model.">Display the detail of jar prediction probabilities for the SMOTE model.</h3>
<p>Our final coding step will be to re-run the report of probabilities using our SMOTE-enhanced Random Forest Classifier model.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>the_report()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nikumaroro jar prediction: Window Non-Float
The artifact has a probability of:
52% to be a Window Non-Float
32% to be a Window Float
6% to be a Container
6% to be a Headlamp
4% to be a Vehicle Float
0% to be a Tableware

clear facsimile prediction: Container ✓
The clear facsimile has a probability of:
49% to be a Container
20% to be a Window Non-Float
14% to be a Headlamp
11% to be a Window Float
5% to be a Tableware
1% to be a Vehicle Float</code></pre>
</div>
</div>
</section>
<section id="analysis-procedure" class="level3">
<h3 class="anchored" data-anchor-id="analysis-procedure">Analysis Procedure</h3>
<p>Let us review the machine learning process that we have just created and executed. The following sequence of operations was performed:<br> 1) Create two arrays. The first contains the database features, all of the predictive columns of the original dataset. The second contains the target column, GlassType.<br> 2) Create a train-test, stratified split so that the model can be trained on 80% of the data and validated on a subsample of 20%.<br> 3) Add the two external glass sample jars to the validation (unseen) data, both for features (X) and for target (Y).<br> 4) Test a set of regression and classification models on the training dataset and report the MSE (mean squared error) results for each one, as well as the standard deviation of the MSE.<br> 5) Plot the preliminary effectiveness of each model on a box plot.<br> 6) Select the model most likely to succeed. In this case, Random Forest Classifier was selected.<br> 7) Build two models of Random Forest Classifier, one without oversampling and the other with oversampling. Use the standard scaler in both models to make the scale of the features more uniform with one another.<br> 8) Fit the models to the training dataset.<br> 9) Print a report on the performance of the models.<br> 10) For each model, print a report on the predicted result for both of the two unseen jars and the probability of the prediction for each glass type.<br></p>
</section>
<section id="findings" class="level3">
<h3 class="anchored" data-anchor-id="findings">Findings</h3>
<p>One of the most interesting findings was that when the SMOTE oversampling algorithm was used with the Random Forest Classifier, the resolving power of the model increased to the point that it was consistently able to identify the clear facsimile jar correctly as a container. Without the SMOTE oversampling algorithm applied, the model predicted the clear facsimile jar to be a headlamp. The prediction of container scored a distant third or fourth place.</p>
<p>The non-SMOTE model predicted the Nikumaroro jar to be a window, usually of the float variety*. This is very close to the prediction made by the model when SMOTE was used. Following application of SMOTE oversampling, the model predicted the Nikumaroro jar to be a non-float window.</p>
<p>These machine learning model predictions would seem to indicate that the clear facsimile jar can be correctly identified through prudent enhancement of the model with oversampling techniques. These techniques, however, are ineffective in increasing the ability of the model to predict the Nikumaroro jar to be what it is, a container.</p>
<p>Using a well-tuned model, the fact that a non-float window was predicted for the Nikumaroro jar demonstrates that this database of late 20th century glassware lacks enough relevant examples that would allow it to predict the Nikumaroro jar correctly.</p>
<p>* Because the model is stochastic in nature, results will not be exactly the same each time.</p>
</section>
<section id="covariate-drift-and-why-it-matters" class="level3">
<h3 class="anchored" data-anchor-id="covariate-drift-and-why-it-matters">Covariate Drift and Why it Matters</h3>
<p>This lack of relevant examples in the training data is a common phenomenon in machine learning, so common in fact that it has a name: covariate drift. Covariate drift is defined as a mismatch between the relative sizes or attributes of categories between the training and the validation data. There are many possible reasons for why covariate drift happens. The passage of time may have changed the population, or perhaps those assembling the training data did not obtain enough available samples from each population, or for whatever reason overlooked certain segments of a population altogether.</p>
<p>When extrapolating information about different populations, whether those populations be glass types, people, or some other collection, we want the categories we survey to be representative of the populations upon which we wish to generalize. If our validation data is not representative of the data we have used to train our model, that is a problem that needs to be addressed.</p>
<p>Before we can address it, however, we need to check whether it really exists, and, if so, how much it exists. We need to measure it.</p>
</section>
<section id="measuring-covariate-drift" class="level3">
<h3 class="anchored" data-anchor-id="measuring-covariate-drift">Measuring Covariate Drift</h3>
<p>Shikhar Gupta, a data scientist at Amazon, has provided an excellent program/algorithm for obtaining quantitative measurement of covariate drift. His paper is here: https://towardsdatascience.com/how-dis-similar-are-my-train-and-test-data-56af3923de9b. I have modified his work by placing it inside a function that can generate multiple reports for different testing situations, and I have adjusted it for the particular situation of testing the U.K. database. I have also added in an ROC curve graph and a short report detailing the various data transformations that are happening as the function progresses.</p>
<p>The basic idea behind this function is to combine the training features (X_train) and validation features (Y_train) dataset, stacking one on top of the other, into a new features dataset, x. We can then create a new target dataset, y, by creating, for every observation in x, a row in y with a new variable, is_train. The is_train variable is a boolean value indicating whether or not each observation came from the original training dataset (prior to combining training and validation). The values in y may be used to compare predicted results with the actual results from the new target dataset.</p>
<p>Having combined training and validation into a new features dataset, x, and created a corresponding target dataset, y, we need a way to separate x and y horizontally into new training and validation datasets. We can create our new training and validation datasets by using stratified Kfold. Stratified Kfold will ensure that each subsection of the data that is produced will have the same ratio of training values (original training data) to validation values (original validation data). By the time all the folds have been produced, we will have created an array of predictions for every single observation in x. Each of these observations in x will have generated a prediction of whether it came from the original training or from the original validation. In essence, we are creating a ‘blind test’ in which the random probability that any given observation from x came from training or from validation is roughly equal.</p>
<p>This procedure is somewhat different than the traditional one of creating a single validation “holdout” dataset we would then use to test the skill of a given model. Instead, what we are doing is fitting each component Kfold to our Random Forest model, assigning a prediction to each observation in the training set created, and adding those predictions to a master array containing a prediction for every observation in the original x.</p>
<p>Each of the predictions that we generate will also have a probability associated with it. The probability calculates how likely is it that that the observation came from the training dataset. This probability can be used to create weighting factors that measure quantitatively a unit-less “distance” between each observation in the final Kfold and the original training data.</p>
<p>To do this weighting, we filter the predictions on the row indexes of the training features dataset created from that last Kfold. This dataset of predictions contains the same ratio of training to validation as existed in the original x dataset. We can then plot the weights from this data onto a distribution plot. If the distribution of the weights in this Kfold shows a clear separation from the original training data, and if the number contained in that separated distribution is roughly equivalent to the number of observations in the Kfold that came from the original validation dataset, we will know then that the validation data we originally selected to test our model has caused covariate drift. If, on the other hand, the distribution shows a central tendency around the value of 1, we will know then that the validation data does not have covariate drift, that the origin of the data itself is not a predictive variable (as we would hope it would not be), and that in fact our validation data is well-enough matched to our training data that it may be used to generate valid, if not always accurate, predictions.</p>
<p>Here is the modification of Shikhar Gupta’s original program to measure covariate drift:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> covariate_drift(validation, title, splits):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Authors: Shikhar Gupta and Joe Cerniglia</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Date: September 17, 2022</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">    A function to create metadata that can be used to measure covariate drift.</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This function generates two graphs:</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">        1. a ROC curve showing the skill of the model at determining whether the data was</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">        from training or from validation</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co">        2. a distribution curve showing the degree to which training and validation differ.</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">    A report is also generated, including a confusion matrix.</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co">    validation: an array containing all of the validation examples we wish to compare</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co">    against the training data.</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co">    title: string to provide a title for the distribution curve</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co">    splits: integer number of splits for the stratifiedKfold. The reciprocal of this number will </span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="co">    determine the ratio of validation samples to training samples.</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="co">    returns None. The function produces graphs but does not return anything outside the function itself.</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># change validation to a pandas dataframe</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>    names<span class="op">=</span>[<span class="st">'Ref_ix'</span>,<span class="st">'Na'</span>,  <span class="st">'Mg'</span>,   <span class="st">'Al'</span>, <span class="st">'Si'</span>,  <span class="st">'K'</span>,  <span class="st">'Ca'</span>,  <span class="st">'Ba'</span>,  <span class="st">'Fe'</span>, <span class="st">'GlassType'</span>]</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>    validation<span class="op">=</span>pd.DataFrame(validation, columns<span class="op">=</span>names)</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'# training rows:'</span>,<span class="bu">len</span>(train))</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'# validation rows:'</span>,<span class="bu">len</span>(validation))</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#adding a column to identify whether a row comes from the training set or not</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>    validation[<span class="st">'is_train'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>    train[<span class="st">'is_train'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">#combining test and train data</span></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>    df_combine<span class="op">=</span>pd.concat([train, validation], axis<span class="op">=</span><span class="dv">0</span>, ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">#dropping 'target' column. We are creating a new target from the origin of the data: test or train</span></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>    df_combine <span class="op">=</span> df_combine.drop(<span class="st">'GlassType'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> df_combine.drop(<span class="st">'is_train'</span>, axis<span class="op">=</span><span class="dv">1</span>).values <span class="co">#leaves only independent variables</span></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df_combine[<span class="st">'is_train'</span>].values <span class="co">#labels: the new target variable is is_train</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Combined validation and train - feature variables (rows, columns):'</span>,x.shape)</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Combined validation and train - target variable (rows, columns):'</span>,y.shape)</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> RandomForestClassifier(n_jobs<span class="op">=-</span><span class="dv">1</span>, max_depth<span class="op">=</span><span class="dv">5</span>, min_samples_leaf<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> np.zeros(y.shape) <span class="co">#creating an empty prediction array that is as large as the original</span></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">#dataframe of combined values</span></span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">#SKF is stratified kfold</span></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>    skf<span class="op">=</span>SKF(n_splits<span class="op">=</span>splits, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Stratified kfold cross validation is an extension of regular kfold cross validation but </span></span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">#specifically for classification problems where rather than completely random splits, </span></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the ratio between the target classes (in this case, train/test) is the same in each fold as it </span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">#is in the full dataset.</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold, (train_idx, validation_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(skf.split(x,y)):</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>        X_train, X_validation <span class="op">=</span> x[train_idx], x[validation_idx] </span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>        y_train, y_validation <span class="op">=</span> y[train_idx], y[validation_idx]</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>        m.fit(X_train, y_train)</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> m.predict_proba(X_validation)[:, <span class="dv">1</span>] <span class="co">#calculating the probability that is_train = 1</span></span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each time through the loop a different set of rows in the training arrays is selected, fit to the</span></span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># model, and a different (smaller) set of rows in the validation array is assigned a prediction,</span></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># based on the fitted model.</span></span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># By the time the loop of 20 is finished, every observation in the combined training</span></span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and validation data has a prediction value.</span></span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The last fold will be the one selected for X_train and X_validation,</span></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y_train and y_validation, but these arrays will not be needed later, except for counts.</span></span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Also, the training indexes of the final fold will be needed to create the distribution plot.</span></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This last fold's weights, derived from predictions from the last training fold, will be the ones </span></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># used in the distribution plot.</span></span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>        predictions[validation_idx] <span class="op">=</span> probs</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Short report about the results</span></span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Confusion matrix for combined training + validation predictions:'</span>)</span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confusion_matrix(y.astype(<span class="bu">float</span>), np.<span class="bu">round</span>(predictions,<span class="dv">0</span>)))</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'# training rows after stratified kfold:'</span>,<span class="bu">len</span>(X_train))</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'# validation rows after stratified kfold:'</span>,<span class="bu">len</span>(X_validation))</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Case mix for X_train after Kfold:'</span>)</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a>    ylist <span class="op">=</span> [<span class="bu">str</span>(x) <span class="cf">for</span> x <span class="kw">in</span> y_train.tolist()]</span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a>    ylist <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.replace(<span class="st">'0'</span>,<span class="st">'from validation'</span>),ylist)</span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a>    ylist <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.replace(<span class="st">'1'</span>,<span class="st">'from training'</span>),ylist)</span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(Counter(<span class="bu">list</span>(ylist)))</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb54-81"><a href="#cb54-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ROC curve of the entire set of predictions</span></span>
<span id="cb54-82"><a href="#cb54-82" aria-hidden="true" tabindex="-1"></a>    fpr, tpr, thresholds <span class="op">=</span> metrics.roc_curve(y, predictions, pos_label<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb54-83"><a href="#cb54-83" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'ROC-Area Under Curve score for train and validation distributions:'</span>,metrics.auc(fpr, tpr))</span>
<span id="cb54-84"><a href="#cb54-84" aria-hidden="true" tabindex="-1"></a>    sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb54-85"><a href="#cb54-85" aria-hidden="true" tabindex="-1"></a>    plt.plot(fpr,tpr)</span>
<span id="cb54-86"><a href="#cb54-86" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'True Positive Rate'</span>,fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb54-87"><a href="#cb54-87" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'False Positive Rate'</span>,fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb54-88"><a href="#cb54-88" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb54-89"><a href="#cb54-89" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-90"><a href="#cb54-90" aria-hidden="true" tabindex="-1"></a>    sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb54-91"><a href="#cb54-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Distribution plot of the training weights</span></span>
<span id="cb54-92"><a href="#cb54-92" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb54-93"><a href="#cb54-93" aria-hidden="true" tabindex="-1"></a>    predictions_train <span class="op">=</span> predictions[train_idx] <span class="co"># Filter the predictions on the row indexes of the </span></span>
<span id="cb54-94"><a href="#cb54-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># training features dataset created from that last Kfold.</span></span>
<span id="cb54-95"><a href="#cb54-95" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb54-96"><a href="#cb54-96" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'# of total training samples in the graph:'</span>,<span class="bu">len</span>(predictions_train))</span>
<span id="cb54-97"><a href="#cb54-97" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>predictions_train) <span class="op">-</span> <span class="fl">1.</span> </span>
<span id="cb54-98"><a href="#cb54-98" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">/=</span> np.mean(weights) <span class="co"># Normalizing the weights</span></span>
<span id="cb54-99"><a href="#cb54-99" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Computed sample weight </span><span class="ch">\n</span><span class="st"> </span><span class="ch">\n</span><span class="st"> The higher the weight, the more dissimilar '</span>  <span class="op">+</span> </span>
<span id="cb54-100"><a href="#cb54-100" aria-hidden="true" tabindex="-1"></a>               <span class="st">'each kfold sample is to samples from the original training dataset'</span>,fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb54-101"><a href="#cb54-101" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'# Samples'</span>,fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb54-102"><a href="#cb54-102" aria-hidden="true" tabindex="-1"></a>    plt.xlim([<span class="bu">min</span>(weights), <span class="bu">max</span>(weights)])</span>
<span id="cb54-103"><a href="#cb54-103" aria-hidden="true" tabindex="-1"></a>    plt.figtext(<span class="fl">0.5</span>, <span class="dv">1</span>, title, wrap<span class="op">=</span><span class="va">True</span>, horizontalalignment<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb54-104"><a href="#cb54-104" aria-hidden="true" tabindex="-1"></a>    g<span class="op">=</span>sns.histplot(weights, kde<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'mediumpurple'</span>)</span>
<span id="cb54-105"><a href="#cb54-105" aria-hidden="true" tabindex="-1"></a>    g.set_xticks(g.get_xticks()[<span class="dv">1</span>:])</span>
<span id="cb54-106"><a href="#cb54-106" aria-hidden="true" tabindex="-1"></a>    g.set_xticklabels(g.get_xticks(), size <span class="op">=</span> <span class="dv">15</span>)</span>
<span id="cb54-107"><a href="#cb54-107" aria-hidden="true" tabindex="-1"></a>    g.set_xticks(<span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">int</span>(<span class="bu">round</span>(<span class="bu">max</span>(weights)<span class="op">+</span><span class="dv">1</span>,<span class="dv">0</span>)),<span class="dv">1</span>))<span class="op">;</span></span>
<span id="cb54-108"><a href="#cb54-108" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb54-109"><a href="#cb54-109" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-110"><a href="#cb54-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="selecting-the-validation-dataset-to-compare-with-the-training-set-for-covariate-drift" class="level3">
<h3 class="anchored" data-anchor-id="selecting-the-validation-dataset-to-compare-with-the-training-set-for-covariate-drift">Selecting the validation dataset to compare with the training set for covariate drift</h3>
<p>An important consideration in testing for coviariate drift will be how to build the validation dataset used to compare with the training dataset. The following function handles a variety of testing scenarios. One apparent problem in this testing is that we only have two jars with which to compare. They are the only validation examples that are of interest, since they are the ones that we know did not come from the original U.K. database.</p>
<p>To make this small validation set comparable in size to the original U.K. database, we will want to copy the data obtained from these jars over and over again, and append each copy to the validation, until it reaches the size of the training dataset. This function repeats the copy process a set number of times until that desired size is reached.</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_validation(validation, repeat, run_repeat, type_repeat<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Modifies a validation dataframe by appending to it repeated artifacts'''</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> run_repeat:</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(repeat):</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> type_repeat<span class="op">==</span><span class="st">'mix'</span>:</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>                validation.append(artifact_features<span class="op">+</span>artifact_identity)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>                validation.append(facsimile)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> type_repeat<span class="op">==</span><span class="st">'facsimiles'</span>:</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>                validation.append(facsimile_features<span class="op">+</span>facsimile_identity)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>                validation.append(facsimile)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> type_repeat<span class="op">==</span><span class="st">'artifacts'</span>:</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>                validation.append(artifact_features<span class="op">+</span>artifact_identity)</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>                validation.append(artifact_features<span class="op">+</span>artifact_identity)</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="testing-how-the-jar-from-nikumaroro-and-the-jar-from-ebay-stack-up-to-the-u.k.-database" class="level3">
<h3 class="anchored" data-anchor-id="testing-how-the-jar-from-nikumaroro-and-the-jar-from-ebay-stack-up-to-the-u.k.-database">Testing how the jar from Nikumaroro and the jar from eBay stack up to the U.K. database</h3>
<section id="elements-of-the-report" class="level4">
<h4 class="anchored" data-anchor-id="elements-of-the-report">Elements of the Report</h4>
<p>Now we can run our first test for covariate drift. After using the set_validation function to create a validation dataset of 215 observations comprised of data from the Nikumaroro jar and the clear facsimile, we will then run that validation data through the main program.</p>
<p>The first two lines returned by the report display the count of original U.K. database glass samples, which is 214. The number of validation rows created by the set_validation function is 215.</p>
<p>Our next two lines provide verification that x (training) and y (validation) have been combined, as well as the resulting count of that combination.</p>
<p>The next element in the report is the confusion matrix. This matrix can tell whether the model has been successful in separating training data from validation data. (See the section, How to Read a Confusion Matrix, for a better appreciation of how to interpret them.) If the validation and training row counts appear in the top left and lower right quadrants, respectively, then we know that covariate drift is present in the data.</p>
<p>After this, we see the counts obtained from the last stratified Kfold. The Kfold subsets a training dataset (95%) from the x. The remaining observations (5%) from x belong to the validation dataset.</p>
<p>We then want to verify that the ratio of training to validation in the original x and y (50:50) has remained in the stratified Kfold training dataset. The line that begins with the word ‘Counter’ will show that it has.</p>
<p>The ROC-Area Under Curve score visually represents how skillful the model was in separating training (U.K. database) from validation (Nikumaroro jar and clear facsimile). If the score is 1.0 and the curve is a right angle that intersects in the top left quadrant, we know then that the model has been completely skillful in this task.</p>
<p>The last exhibit is the distribution plot, which takes the training set from the last Kfold and bins the weights derived from that training set, which tells us how the observations in the training set compare with the original validation set. Those bins that center on 1.0 represent training examples that stand an equal chance of having come from original validation or training sets. (Ideally, all rows would hover at or near this point.) Those bins that center on 0.0 have a 100% probability of having come from the training set. The greater the positive distance the bins are from 1.0, the more unlike the original training set these observations are.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> dataset_ml.drop([<span class="st">'ID'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The artifact data are given as lists</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>validation <span class="op">=</span> [artifact_features<span class="op">+</span>artifact_identity]</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>facsimile <span class="op">=</span> facsimile_features<span class="op">+</span>facsimile_identity</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>set_validation(validation, <span class="dv">107</span>, <span class="va">True</span>,<span class="st">'mix'</span>)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>covariate_drift(validation,<span class="st">'Test of whether adding repeated Nikumaroro jars and clear facsimiles </span><span class="ch">\n</span><span class="st"> '</span> <span class="op">+</span> </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>                <span class="st">'as validation samples causes a covariate drift'</span>,<span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># training rows: 214
# validation rows: 215
Combined validation and train - feature variables (rows, columns): (429, 9)
Combined validation and train - target variable (rows, columns): (429,)
Confusion matrix for combined training + validation predictions:
[[215   0]
 [  0 214]]
# training rows after stratified kfold: 408
# validation rows after stratified kfold: 21
Case mix for X_train after Kfold:
Counter({'from training': 204, 'from validation': 204})
ROC-Area Under Curve score for train and validation distributions: 1.0

# of total training samples in the graph: 408</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-38-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-38-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="analysis-of-covariate-drift-effect-of-nikumaroro-jar-and-clear-facsimile" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-covariate-drift-effect-of-nikumaroro-jar-and-clear-facsimile">Analysis of covariate drift: Effect of Nikumaroro jar and clear facsimile</h3>
<p>The exhibits returned by the first test show unambiguously that the jar from Nikumaroro and the clear facsimile cause covariate drift in the analysis. The distribution plot shows two distinct groups, representing a clean division of samples between the U.K. database and the jars. The spread of the jar group is rather wide, indicating that some examples from the U.K. database are more similar to the jars than others.</p>
</section>
<section id="analysis-of-second-test-of-covariate-drift-effect-of-the-training-data-on-itself" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-second-test-of-covariate-drift-effect-of-the-training-data-on-itself">Analysis of second test of covariate drift: Effect of the training data on itself</h3>
<p>Our last test is a control experiment. We want to know whether the training data itself contains some inherent covariate drift of its own. Does the training data from the U.K. database show internal consistency with itself? This is also a good way to check for how a dataset potentially without covariate drift should appear.</p>
<p>This last test shows that the U.K. database is not perfectly consistent. Rather than the jagged bell curve that would appear in a highly normal distribution, our plot shows a bimodal distribution. This is most likely the result of two large categories of data within the U.K. database. Window and Non-window are the most likely separations that this plot is showing, with the smaller peaks toward the right possibly representing the less populated categories of tableware, headlamps, and containers.</p>
<p>The overall picture drawn by this plot, however, is of a database without covariate drift.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> dataset_ml.drop([<span class="st">'ID'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Test of whether the training dataset itself has covariate drift</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>validation2 <span class="op">=</span> train.copy()</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>comparison<span class="op">=</span>covariate_drift(validation2,<span class="st">'Test of whether training data itself has covariate drift'</span>,<span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># training rows: 214
# validation rows: 214
Combined validation and train - feature variables (rows, columns): (428, 9)
Combined validation and train - target variable (rows, columns): (428,)
Confusion matrix for combined training + validation predictions:
[[  4 210]
 [206   8]]
# training rows after stratified kfold: 407
# validation rows after stratified kfold: 21
Case mix for X_train after Kfold:
Counter({'from training': 204, 'from validation': 203})
ROC-Area Under Curve score for train and validation distributions: 0.003559262817713333

# of total training samples in the graph: 407</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-39-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-39-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="treating-covariate-drift" class="level3">
<h3 class="anchored" data-anchor-id="treating-covariate-drift">Treating Covariate Drift</h3>
<p>Many articles offer treatment options for addressing covariate drift. The only true measure of the success of treatment is the success of the machine learning models. Was a given model able to identify the jars correctly? We have already seen that, using SMOTE oversampling, we were able to identify the clear facsimile correctly, despite the covariate drift that was in the validation samples. Would additional techniques be able to identify the Nikumaroro jar as a container?</p>
<p>Albert Um, in an article for Medium, https://albertum.medium.com/covariate-shift-in-machine-learning-adf8d0077f79, has written a program that uses logistic regression to calculate weighting factors that may be used to weight the model to favor those training examples that are most like the validation set. The weights may then be used in the model to fit the original training examples and then predictions can be made on the jars that would presumably be more accurate than they would have been without the weights.</p>
</section>
<section id="setting-up-the-data" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-data">Setting up the data</h3>
<p>Here is how it works: The setup of the training dataset is identical to Shikhar Gupta’s algorithm, albeit with slightly different syntax. We take the original dataframe of the U.K. database and drop the ‘ID’ and ‘GlassType’ variables, since these will not be needed in the analysis. Then we add our ‘is_train’ column to the data, populating it with ‘1’, indicating that these observations came from training. We convert the training dataset to an array and split it vertically into features (X_train) and the target (Y_train), which is comprised solely of the is_train variable.</p>
<p>We then build our validation dataset exactly as we did previously, appending multiple copies to itself until we have a dataframe that is the same size as the training dataset. Then we add the is_train column to the validation dataset, populating it with a value of ‘0’. We then convert the validation dataset to an array and split that array vertically into features (X_validation) and the target (Y_validation), which is comprised solely of the is_train variable.</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Recover original training dataframe</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> dataset_ml.drop([<span class="st">'ID'</span>,<span class="st">'GlassType'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign new target</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>train[<span class="st">'is_train'</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert training dataframe to an array</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>array <span class="op">=</span> train.values</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split training into X and Y</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> array[:,<span class="dv">0</span>:<span class="dv">9</span>]</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> array[:,<span class="dv">9</span>]</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-create validation consisting of artifacts</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>validation <span class="op">=</span> [artifact_features <span class="op">+</span> artifact_identity]</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>facsimile <span class="op">=</span> facsimile_features <span class="op">+</span> facsimile_identity</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Enlarge the validation set by appending copies</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>set_validation(validation, <span class="dv">106</span>, <span class="va">True</span>, <span class="st">'mix'</span>)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the list to a dataframe</span></span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>names<span class="op">=</span>[<span class="st">'Ref_ix'</span>,<span class="st">'Na'</span>,  <span class="st">'Mg'</span>,   <span class="st">'Al'</span>, <span class="st">'Si'</span>,  <span class="st">'K'</span>,  <span class="st">'Ca'</span>,  <span class="st">'Ba'</span>,  <span class="st">'Fe'</span>, <span class="st">'GlassType'</span>]</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>validation<span class="op">=</span>pd.DataFrame(validation, columns<span class="op">=</span>names)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'# training rows:'</span>,<span class="bu">len</span>(train))</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'# validation rows:'</span>,<span class="bu">len</span>(validation))</span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the column to identify whether a row comes from train or not</span></span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>validation[<span class="st">'is_train'</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert validation dataframe to an array</span></span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>val_array<span class="op">=</span>validation.values</span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Split validation into X and Y</span></span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>X_validation <span class="op">=</span> val_array[:,<span class="dv">0</span>:<span class="dv">9</span>]</span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>Y_validation <span class="op">=</span> val_array[:,<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># training rows: 214
# validation rows: 213</code></pre>
</div>
</div>
</section>
<section id="creating-the-weights" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-weights">Creating the weights</h3>
<p>Now Mr.&nbsp;Um takes the two X and two Y arrays and stacks them using np.vstack and np.hstack. He creates two new arrays, named X_cov and y_cov, which combine, as previously, the training features and validation features (X_cov), and then, in a separate array, the training target and the validation target (y_cov). These functions create arrays in the format that logistic regression will be able to use in fitting the model.</p>
<p>Next, he fits the stacked arrays, X_cov and y_cov to a logistic regression model. This model uses both training (U.K. database) and validation (the jars) to create a new model that encompasses our “drifted” examples. With the new model in hand, he uses the decision_function of logistic regression to calculate confidence scores on the original training sample.</p>
<p>To reduce what Mr.&nbsp;Um calls “outrageous weight assignments,” he uses a cutoff value of 3 (or -3), which he applies conditionally to the array of confidence scores. All scores less than -3 are assigned a -3. All scores greater than 3 are assigned a 3. Mr.&nbsp;Um then exponentiates (raises Euler’s number, 2.71828, to the power of each weight), using the np.exp function.</p>
<p>We then have an array of weights that correspond with each row of the original training sample.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Albert Um</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fitting Logistic Regression to obtain weights similar to test</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># vertical stack feature matrix</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>X_cov <span class="op">=</span> np.vstack([X_train, X_validation])</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="co"># horizontal stack label array</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>y_cov <span class="op">=</span> np.hstack([Y_train, Y_validation])</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate a binary classifier for confidence scores</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>weights_logreg <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">2000000</span>)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>weights_logreg.fit(X_cov, y_cov)</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the weights for X_train, not to X_train + X_validation,</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a><span class="co"># but based on the fit of the covariate stacks, which include training + validation</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a><span class="co"># This brings the count to the original 214.</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence scores</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>conf_scores <span class="op">=</span> weights_logreg.decision_function(X_train)</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a><span class="co">#print(conf_scores)</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Clipping (C = Cutoff, originally set at 3)</span></span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>conditions <span class="op">=</span> [</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>  conf_scores <span class="op">&lt;</span> <span class="op">-</span>C,</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>  conf_scores <span class="op">&gt;</span> C</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>choices <span class="op">=</span> [</span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>  <span class="op">-</span>C,</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>  C</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>conf_scores <span class="op">=</span> np.select(conditions, choices, conf_scores)</span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a><span class="co"># exponentiate the confidence scores</span></span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.exp(conf_scores)</span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> weights.flatten()</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'# of weights:'</span>,<span class="bu">len</span>(weights))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># of weights: 214</code></pre>
</div>
</div>
</section>
<section id="fitting-the-model-with-the-weights-to-the-training-sample" class="level3">
<h3 class="anchored" data-anchor-id="fitting-the-model-with-the-weights-to-the-training-sample">Fitting the model with the weights to the training sample</h3>
<p>We can now bring back our original U.K. database as an array. Split the array into features (x) and target (y), using the original target variable, GlassType.</p>
<p>We then create stratified Kfolds of X_train (training features), X_validation (validation features), Y_train (training target), and Y_validation (validation target). We then fit our favorite model, Random Forests, to the training features and training target, using the sample weights we derived from the earlier fitting of the logistic regression model to the data that included both the U.K. dataset and the jars.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bring back the original dataset with the original target column: GlassType</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># so that we can apply the weights above to test their predictive ability</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>array <span class="op">=</span> dataset_ml.values</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> array[:,<span class="dv">1</span>:<span class="dv">10</span>]</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> array[:,<span class="dv">10</span>]</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co">#print('length X_train:',len(X_train))</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co">#print('length weights:',len(weights))</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We use a folded split to fit the model on the training data only, </span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co"># thus avoiding data leakage in the fitting of the model.</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="co"># The validation datasets must be naive to the training datasets to prevent data leakage.</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The splits=4 allows the validation set to be about 25% of the size of the training set.</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>skf<span class="op">=</span>SKF(n_splits<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, validation_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(skf.split(x,y)):</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        X_train, X_validation <span class="op">=</span> x[train_idx], x[validation_idx] </span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>        Y_train, Y_validation <span class="op">=</span> y[train_idx], y[validation_idx]</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="co">#print(len(X_train),len(X_validation),len(Y_train),len(Y_validation))</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a><span class="co">#print(Y_train)</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="co"># The idea is to weight more heavily those training examples that are </span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a><span class="co"># more similar to the artifacts</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a new model with the logistic regression's custom weights</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(n_jobs<span class="op">=-</span><span class="dv">1</span>,max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, Y_train, sample_weight <span class="op">=</span> weights[train_idx])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="assessing-the-skill-of-the-weighted-model" class="level3">
<h3 class="anchored" data-anchor-id="assessing-the-skill-of-the-weighted-model">Assessing the skill of the weighted model</h3>
<p>Now it is time to see how this new model with weights performs against covariate drift. This next step is rather simple. We can use the model fitted in the prior step to make predictions on the validation features (jars), then print a confusion matrix and a classification report to assess the results.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> model.predict(X_validation)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest with sample_weight'</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(Y_validation, yhat))</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(Y_validation, yhat, zero_division<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest with sample_weight
[[13  4  0  0  0  0]
 [ 4 15  0  0  0  0]
 [ 3  1  0  0  0  0]
 [ 0  3  0  1  0  0]
 [ 1  0  0  0  1  0]
 [ 0  0  0  0  0  7]]
              precision    recall  f1-score   support

         1.0       0.62      0.76      0.68        17
         2.0       0.65      0.79      0.71        19
         3.0       1.00      0.00      0.00         4
         5.0       1.00      0.25      0.40         4
         6.0       1.00      0.50      0.67         2
         7.0       1.00      1.00      1.00         7

    accuracy                           0.70        53
   macro avg       0.88      0.55      0.58        53
weighted avg       0.75      0.70      0.66        53
</code></pre>
</div>
</div>
<p>Unfortunately, our model performs a good deal worse than both our SMOTE and non-SMOTE models used earlier. On the Nikumaroro jar, it computes a minimal probability that the Nikumaroro jar is a container. On the clear facsimile, the results are very similar.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> model.predict([artifact_features])</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>yhat2 <span class="op">=</span> model.predict([facsimile_features])</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>yhat_probability <span class="op">=</span> model.predict_proba([artifact_features])</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>yhat2_probability <span class="op">=</span> model.predict_proba([facsimile_features])</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>the_report()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nikumaroro jar prediction: Window Float
The artifact has a probability of:
53% to be a Window Float
18% to be a Window Non-Float
17% to be a Headlamp
12% to be a Vehicle Float
0% to be a Container
0% to be a Tableware

clear facsimile prediction: Window Non-Float
The clear facsimile has a probability of:
27% to be a Window Non-Float
27% to be a Window Float
17% to be a Headlamp
11% to be a Container
10% to be a Tableware
9% to be a Vehicle Float</code></pre>
</div>
</div>
<p>Using the same Random Forests model without weights as a control, we find that the skill of this model is about the same as that of the model with weights, and the clear facsimile and Nikumaroro jar are still assigned a very low probability as containers.</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>modelnoWeight <span class="op">=</span> RandomForestClassifier(n_jobs<span class="op">=-</span><span class="dv">1</span>,max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>modelnoWeight.fit(X_train, Y_train)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> modelnoWeight.predict(X_validation)</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Random Forest without sample_weight'</span>)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(Y_validation,yhat))</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(Y_validation, yhat, zero_division<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest without sample_weight
[[12  5  0  0  0  0]
 [ 5 13  0  0  0  1]
 [ 2  2  0  0  0  0]
 [ 0  3  0  1  0  0]
 [ 0  1  0  0  1  0]
 [ 0  0  0  0  0  7]]
              precision    recall  f1-score   support

         1.0       0.63      0.71      0.67        17
         2.0       0.54      0.68      0.60        19
         3.0       1.00      0.00      0.00         4
         5.0       1.00      0.25      0.40         4
         6.0       1.00      0.50      0.67         2
         7.0       0.88      1.00      0.93         7

    accuracy                           0.64        53
   macro avg       0.84      0.52      0.55        53
weighted avg       0.70      0.64      0.61        53
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> modelnoWeight.predict([artifact_features])</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>yhat2 <span class="op">=</span> modelnoWeight.predict([facsimile_features])</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>yhat_probability <span class="op">=</span> modelnoWeight.predict_proba([artifact_features])</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>yhat2_probability <span class="op">=</span> modelnoWeight.predict_proba([facsimile_features])</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>the_report()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nikumaroro jar prediction: Window Float
The artifact has a probability of:
55% to be a Window Float
25% to be a Window Non-Float
10% to be a Headlamp
10% to be a Vehicle Float
0% to be a Tableware
0% to be a Container

clear facsimile prediction: Window Non-Float
The clear facsimile has a probability of:
33% to be a Window Non-Float
23% to be a Headlamp
21% to be a Window Float
10% to be a Vehicle Float
8% to be a Tableware
5% to be a Container</code></pre>
</div>
</div>
</section>
<section id="why-did-the-weighting-fail-to-improve-the-model" class="level3">
<h3 class="anchored" data-anchor-id="why-did-the-weighting-fail-to-improve-the-model">Why Did the Weighting Fail to Improve the Model?</h3>
<p>On an intuitive level, the low skill of this weighted model can be understood by looking back at the scatter plot we used to demonstrate SMOTE. Let’s recreate this scatter plot now.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>smotegraph(<span class="st">' - after SMOTE oversampling'</span>,<span class="st">'sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-14-Glass-ML-20th-Century_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>For the weighting model to be successful, two things need to happen.</p>
<p>First, samples in the original training set (U.K. database) that have somewhat similar features to the validation set (jars) must exist. Notice the upper left quadrant of the SMOTE graph above. There are only a few training examples in that region. If, for example, our jars have features that place them in the upper left corner of the graph, there will be no training examples there to weight in favor of the validation examples.</p>
<p>Second, at least a few of the samples in the original training set (U.K. database) that share similar features to the validation set (jars) must actually <strong>BE</strong> jars. If they are not jars, then weighting these examples will only cause the model to reinforce and amplify its misinterpretations.</p>
<p>Judging by the number of proposed treatment options, it would seem that there is no single magic bullet to the problem of covariate drift. The best approach is to experiment, with the realization that some validation samples may have drifted beyond the reach of rescue by a skillful model. In essence, the Nikumaroro jar might be regarded not as an example of covariate drift, but rather as a paradigm shift, a combination of features too unique to be generalizable to the world of glass types available in 1987.</p>
<p>The most basic treatment option, and the preferred one, for covariate drift is to find new data examples to add to the training set that will diversify it and make it more adaptable to training it for unanticipated validation examples.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Our task, then, would seem to be to supplement the 1987 U.K. database with chemical profiles of jars from the early twentieth century that are similar to the Nikumaroro jar.</p>
<p>But could these examples be found? Those of us who analyzed the Nikumaroro jar have searched for more than ten years and have not so far found any siblings that match that jar’s chemical profile. The search, however, continues.</p>
<p>I derive two lessons from this machine learning project: 1. Machine learning, at least of the type developed here, is not without failure or flaw. It can sometimes fail spectacularly.</p>
<ol start="2" type="1">
<li>Flawlessness, however, is neither very interesting nor very attainable, and in any case, it may not even be necessary. We can learn more sometimes by a model’s failure to learn than by its success.</li>
</ol>
<p>We have learned that our machine learning model can correctly characterize the clear facsimile, a jar of the same size and shape as the Nikumaroro jar, and possibly from the same era. The model cannot correctly characterize the semi-opaque Nikumaroro jar. One might suppose that this jar has a rare and original recipe that, in the absence of any information other than the data nourishing the model, is not readily identifiable as to the glass type, even with the powerful machine learning tools, such as random forests, available to us in 2022. While this may not be confirmative as to whether or not the jar was owned and brought to Nikumaroro Island by Amelia Earhart, this fact is further confirmation of its originality and rarity, perhaps even in its own time.</p>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<p>[1] <i>Wallenberger, Frederick T. and Bingham, Paul A., ed.&nbsp;Fiberglass and Glass Technology: Energy-Friendly Compositions and Applications. New York: Springer Science and Business Media, 2010, p.&nbsp;323.</i><br> [2] <i>Caddy, Brian, ed.&nbsp;Forensic Examination of Glass and Paint. London: Taylor &amp; Francis, 2001, p.&nbsp;61.</i><br> [3] <i>Kaur,&nbsp;Gurbinder.&nbsp;Bioactive Glasses: Potential Biomaterials for Future Therapy.&nbsp;Germany:&nbsp;Springer International Publishing,&nbsp;2017, p.&nbsp;107.</i></p>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>I owe much of what I have learned to date in machine learning to Dr.&nbsp;Jason Brownlee: https://machinelearningmastery.com/machine-learning-in-python-step-by-step/</p>
<p>I owe a debt of gratitude to reviewers from The Alan Turing Institute. Jennifer Ding, Research Application Manager and Eirini Zormpa, Community Manager of Open Collaboration, kindly offered numerous suggestions for clarifying both the code and the writing of this paper.</p>
<p>Thank you to all of the members of the Turing Data Stories Wednesday Meeting Group, who also offered many insights, camaraderie and support through many months of the process of creating new and interesting Turing Data Stories, and the work continues.</p>
<p>Thank you to Ric Gillespie of The International Group for Historic Aircraft Recovery for graciously allowing the sharing of this research with The Alan Turing Institute.</p>
<p>Thank you to Ian W. Evett and Ernest J. Spiehler, for making the data from their 1987 glass study available for students and experienced data scientists alike to use.</p>
<p>Thank you to my co-authors of the paper “A Freckle In Time,” which was written from 2010-2013: Greg George, Senior Staff Chemist at Persedo Spirits, Bill Lockhart, former assistant professor of sociology at New Mexico State University at Alamogordo, and Thomas Fulling King, former director of the United States Advisory Council on Historic Preservation.</p>
<p>Story preview photograph credit: ID <a href="https://www.dreamstime.com/editorial-photo-historic-lockheed-model-e-electra-airplane-seattle-museum-flight-display-silver-metal-sheeting-makes-dazzling-image98648556">98648556</a> © <a href="https://www.dreamstime.com/mickem_info">Mickem</a> | <a href="https://www.dreamstime.com/">Dreamstime.com</a></p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>