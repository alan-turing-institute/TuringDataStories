{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eurovision Turing Data Story\n",
    "\n",
    "> Exploring (non-musical) factors contributing to Eurovision votes in the past 25 years.\n",
    "\n",
    "- toc: false\n",
    "- categories: [data wrangling, data visualisation, bayesian modeling, random forest, eurovision]\n",
    "- author(s): Ed Chapman, Katriona Goldmann, Radka Jersakova, David Llewellyn-Jones, Joe Palmer, Camila Rangel Smith, Martin Stoffel, Jonathan Yong\n",
    "- image: TODO\n",
    "\n",
    "**Authors**\n",
    " - Ed Chapman\n",
    " - Katriona Goldmann\n",
    " - Radka Jersakova\n",
    " - David Llewellyn-Jones\n",
    " - Joe Palmer\n",
    " - Camila Rangel Smith\n",
    " - Martin Stoffel\n",
    " - Jonathan Yong\n",
    " \n",
    " **Reviewers:**\n",
    "- Reviewer 1\n",
    "- Reviewer 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Eurovision Song Contest, or *Eurovision* for short, is an annual competition featuring (mostly) European countries. It is in fact the longest running international televised music competitions, having started in 1951 as what was then seen as a technical experiment in transnational live broadcasting. The competition has continued every year since &mdash; except in 2020 when it was cancelled, [much to Iceland's chagrin](https://youtu.be/1HU7ocv3S2o), due to the pandemic &dash; and has grown in terms of number of countries participating, musical variety, visual flair and (some might say) fantastical preposterousness.\n",
    "\n",
    "Despite the name, the Eurovision Song Content is neither about Europe, nor Singing. According to the Eurovision website, the European Broadcasting Union which runs the contest is made up of \"56 countries and an additional 31 Associates in Asia, Africa, Australasia and the Americas\". The contest itself is billed as a *songwriting* competition.\n",
    "\n",
    "There are [strict requirements](https://eurovision.tv/about/rules) that songs and performers must comply with. Songs must be original; under three minutes long; sung live without lip syncing and with no live plugging instruments.\n",
    "\n",
    "Typically the winning country is expected to host the technically challenging and costly event the following year (a 'prize' that [allegedly led to](https://www.irelandbeforeyoudie.com/why-ireland-stopped-winning-eurovision/) Ireland entering sub-par acts for at least a decade in a deliberate attempt to avoid winning). As many readers will no-doubt be aware, in 2022 the competition was won by Kalush Orchestra from Ukraine, with their song [Stefania](https://youtu.be/F1fl60ypdLs). This would ordinarily mean the 2023 event would be held in Ukraine, however this was deemed unsafe in light of the Russian invasion. The UK (the 2022 runners-up) therefore stepped in, meaning that this year's contest was held in Liverpool, on 9–13 May 2023.\n",
    "\n",
    "The seventy-year history of the event means there is now a large body of data about it to work with, as well as a large number of great songs to listen to. As you work through this Turing Data Story, we strongly recommend you listen to some of the great Eurovision masterpieces as your backing track, from the likes of [Celine Dion](https://youtu.be/w6b7BHGkKQA), [Bucks Fizz](https://youtu.be/h4-lKMGII_k), [Lordi](https://youtu.be/gAh9NRGNhUU), [Loreen](https://youtu.be/Pfo-8z86x80) and of course [Abba](https://youtu.be/Vp1_OKawHYw).\n",
    "\n",
    "<center>\n",
    "    <img alt=\"meme comparing what I think I look like discussing eurovision: two men casually talking to sofa; to what I actually look like: Charlie Kelly looking wild-eyed in front of a peg board full of conspiracy connections\" src=\"https://i.imgur.com/WVNzMI7.jpeg\" />\n",
    "</center>\n",
    "\n",
    "In the current format, participating countries first take part in a semifinal; the top 10 from each semifinal qualify for the finals.\n",
    "The host country, as well as the \"Big Five\" (France, Germany, Italy, Spain, and the UK, which make greater financial contributions), directly qualify for the finals.\n",
    "\n",
    "Our aim with this Story has been to try to uncover some of the hidden mysteries of Eurovision voting, and also to make some predictions &mdash; based purely on this data &mdash; for which countries are likely to fare well, and which less well, at this year's contest.\n",
    "\n",
    "We have pooled data from a number of different sources with the aim of understanding trends in the contest voting patterns. We have also tried to make the data as accessible as possible. The curated data frame is available in the `data/df_main.csv` file, and the code to create this dataset is in `data.ipynb`.\n",
    "\n",
    "Our goal was to use this dataset to predict the winners of the [2023 event in Liverpool](https://eurovision.tv/event/liverpool-2023). Now that the event has taken place we've left this analysis as-is, so that you can compare our predictions with the actual results. But we've also now been able to add some post-match analysis to reflect on how our models did so that we can be even more accurate next year.\n",
    "\n",
    "In this notebook we'll go into some detail about all of our data collection, analysis and results. If you're here for instant gratification and just want to know how we did, check out our summary on the [Turing Institute Blog](https://www.turing.ac.uk/blog/can-data-science-help-us-predict-winner-eurovision-2023). But if you want to know all the gory data-sciency details, then read on!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing packages and setting up any configuration needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In order to explore the characteristics of Eurovision, with the aim of making predictions for the 2023 event, we first need to collect historical data about contests prior to 2023. We will obviously need to know what scores were assigned to which acts in previous years, but we also want to explore other factors that might potentially affect the success or otherwise of the performances.\n",
    "\n",
    "It has long [been claimed](https://www.datalytyx.com/eurovision-song-contest-regression-analysis-highlights-the-voting-patterns/) that there is a [measurable bias](https://www.tandfonline.com/doi/full/10.1080/02664763.2014.909792) in relation to which countries vote for which other countries at Eurovision. We should therefore gather data that exhibits these biases. We also consider other factors that may affect voting patterns, such as the gender of the performer or language of the song.\n",
    "\n",
    "The Eurovision Song Contest has undergone several changes to its voting system since it began in 1956. These changes were made to increase transparency and reduce the impact of regional bloc voting, while still allowing for a fair and entertaining competition.\n",
    "\n",
    "In the early years, the winning country was selected by a jury of experts from each participating country. In 1975, a new system was introduced that combined jury voting with telephone voting by viewers at home. Over time, the weight of the jury and viewer votes has shifted, with viewer votes becoming more influential. In 2016, the voting system was revised again, with separate scores given for jury and viewer votes, and a new system for calculating the final scores. \n",
    "\n",
    "In order to model uniform voting scores over time, we limited our analysis to contests from 1998 onwards, where the jury and televoting scores are equally weighted.\n",
    "\n",
    "In the following sections we'll describe our data-wrangling efforts in some depth. If you're more interested in the analysis, then you can safely skip forwards to the [Covariate visualisation section](#Covariate-visualisation) without losing the narrative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Voting Scores\n",
    "\n",
    "Our voting data was pulled from the following sources: \n",
    "\n",
    "- From the [Eurovision Song Contest Scores Kaggle dataset](https://www.kaggle.com/datasets/datagraver/eurovision-song-contest-scores-19752019), for the years 1998–2019. \n",
    "- The 2020 contest was cancelled due to Covid. \n",
    "- The [2021](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2021#Final_2) and [2022](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2022#Final_2) contest scores were scraped from Wikipedia.\n",
    "\n",
    "The jury votes have in the past been replaced with a substitute aggregate due to potential bias or 'irregularities'. In this case, new jury scores were calculated [\"based on the results of other countries with similar voting records\"](https://eurovision.tv/mediacentre/release/ebu-statement-voting-during-2022-shows). However, this is not very transparent and [not captured by our models](https://eurovisionworld.com/esc/here-is-the-proof-of-the-eurovision-voting-scandal-six-juries-cheated-and-voted-for-each-other).\n",
    "\n",
    "Unlike in earlier years, since 2016 the jury and televoting scores have been reported separately. Simply summing these would lead to scores that are unusually large compared to previous years. In order to bring this in line, both votes were summed and rescaled by mapping the highest result to 12 points, second-highest to 10 points, and so on.\n",
    "\n",
    "Let's load in the Kaggle data (we host our own copy to avoid having to negotiate with Kaggle's authentication wall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Read in data from the Kaggle dataset\n",
    "votes_1975_2019 = pd.read_excel('data/eurovision_song_contest_1975_2019.xlsx', engine='openpyxl')\n",
    "print('Number of entries: {}'.format(len(votes_1975_2019)))\n",
    "votes_1975_2019.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset requires some cleaning up before we can properly make use of it. Some of this is due to errors that have crept into the dataset such as incorrect spellings of country names, some is due to geopolitical changes during the period we're considering (we have mapped all entries by the entity 'Serbia & Montenegro' to Yugoslavia), and some is needed to get the dataset structured appropriately for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the column names\n",
    "votes_1975_2019.columns = [c.strip().lower().replace(' ', '_') for c in votes_1975_2019.columns.values.tolist()]\n",
    "\n",
    "# Select only finals votes, and only 1998 onwards (inclusive)\n",
    "votes_1998_2019 = votes_1975_2019[(votes_1975_2019['(semi-)_final'] == 'f') & (votes_1975_2019['year'] >= 1998)]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "votes_1998_2019 = votes_1998_2019[[\"year\", \"from_country\", \"to_country\", \"points\", \"jury_or_televoting\"]]\n",
    "\n",
    "# Clean up country names\n",
    "def standardise_country(c):\n",
    "    replacements = [('-', ' '), ('&', 'and'), ('Netherands', 'Netherlands'),\n",
    "                    # FYR Macedonia was formally renamed as North Macedonia in 2019\n",
    "                    ('F.Y.R. Macedonia', 'North Macedonia'), \n",
    "                    ('Russia', 'Russian Federation'), \n",
    "                    ('The Netherlands', 'Netherlands'), \n",
    "                    ('Czech Republic', 'Czechia'),\n",
    "                    # Yugoslavia dissolved in 2002; most of it became 'Serbia and Montenegro', until 2006, when Serbia and Montenegro split ways.\n",
    "                    ('Serbia and Montenegro', 'yugoslavia'),\n",
    "                    ('moldova', 'moldova, republic of')]\n",
    "    for r in replacements:\n",
    "        c = c.replace(r[0], r[1])\n",
    "    return c.lower()\n",
    "\n",
    "votes_1998_2019[['from_country', 'to_country']] = votes_1998_2019[['from_country', 'to_country']].applymap(standardise_country)\n",
    "\n",
    "# Drop columns which correspond to the same vote (there are two Belarus -> Russia in 2019, for example)\n",
    "votes_1998_2019 = votes_1998_2019.drop_duplicates(subset=['year', 'from_country', 'to_country', 'jury_or_televoting'])\n",
    "\n",
    "# Drop Lithuania in 2003 (they didn't participate - we don't know why it's still in the dataset)\n",
    "votes_1998_2019 = votes_1998_2019[~((votes_1998_2019['to_country'] == 'lithuania') & (votes_1998_2019['year'] == 2003))]\n",
    "\n",
    "# Drop \"votes\" from one country to itself\n",
    "votes_1998_2019 = votes_1998_2019[votes_1998_2019['from_country'] != votes_1998_2019['to_country']]\n",
    "\n",
    "votes_1998_2019.sample(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle dataset runs up to 2019, but we need data for 2021 and 2022 as well (recall that there was no contest in 2020). We collect these data from the tables in the Wikipedia pages for the [2021](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2021#Final_2) and [2022](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2022#Final_2) contests respectively.\n",
    "\n",
    "The process of downloading and cleaning up this data is rather long winded, so we're going to skip past that here and instead refer to the separate [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb) that we've put together for the inquisitve.\n",
    "\n",
    "So we'll short-circuit this by grabbing the output from the data collection notebook. Or, in the immortal words of the Blue Peter team, here's one I made earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "votes = pd.read_csv('data/votes.csv')\n",
    "votes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance language\n",
    "\n",
    "The next step is to collect together the languages used in the songs. We're particularly interested in whether the song is sung in the language of the performing country or some other language. Anecdotally there has been a tendency for songs to be sung in English if not a country's native language, and so we also keep track of whether a song includes English lyrics as well.\n",
    "\n",
    "When collecting the data, we have to bear in mind that songs are often sung in *more than one* language. The fields we've chosen to populate are therefore:\n",
    " - *whether the song contains English lyrics*,\n",
    " - *whether it contains non-English lyrics*,\n",
    " - *whether it contains the performing country's official language*,\n",
    " - *whether it contains the voting country's official language*, and\n",
    " - *whether it contains multiple languages*.\n",
    "\n",
    "We retreived information about the language of each performance from a dataset [available from Kaggle](https://www.kaggle.com/datasets/minitree/eurovision-song-lyrics?select=eurovision-lyrics-2022.json). This provides us with half of our needs; the other half relates to which languages are the official languages of each country.\n",
    "\n",
    "For the latter we scrape the list of official languages available [on Wikipedia](https://en.wikipedia.org/wiki/List_of_official_languages_by_country_and_territory).\n",
    "\n",
    "Again, the data needed considerable cleaning and structuring to suit our needs. We'll skip the code for doing this for brevity and refer the interested reader to our data collection notebook.\n",
    "\n",
    "Let's load the results saved out from that notebook here so we can see how things are progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_VL = pd.read_csv('data/language.csv')\n",
    "df_VL.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performer gender\n",
    "\n",
    "To establish the gender of the various performers we capture the *'gender'* property available from [Wikidata](https://www.wikidata.org/wiki/Wikidata:List_of_properties), using code that's based on Sam Van Stroud's [wikipeople script](https://github.com/samvanstroud/wikipeople/blob/master/wikipeople/wikipeople.py).\n",
    "\n",
    "Rather than referring directly to our [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb) we'll briefly go through the code here, since it demonstrates some nice techniques.\n",
    "\n",
    "Establishing gender isn't straightforward in all cases. For example, many Eurovision entries are group performances. For these we detect the *'instance of'* Wikidata property, checking whether it's equal to one of *group*, *duo*, *trio*, *music*, *band* or *ensemble*. There are also non-binary performers to consider.\n",
    "\n",
    "Searching Wikidata with a name often generates multiple hits, so we try to restrict our search to performers tagged as being musicians and tagged as having performed at Eurovision. Even with this restriction we don't always get a unique hit, or any hit at all. Some artists simply don't exist in the database, or are missing these tags. Out of the total 600 performers we are left having to manually assign genders to 55 of them.\n",
    "\n",
    "We've collapsed the code that collects the properties from Wikidata. Feel free to take a look, but the more interesting part follows below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def get_property(session, concept_id, property_id):\n",
    "    \"\"\"Async reimplementation of wikipeople.get_property\n",
    "    https://github.com/samvanstroud/wikipeople/blob/master/wikipeople/wikipeople.py\n",
    "\n",
    "    session is an aiohttp ClientSession instance.\n",
    "    concept_id refers to a person or a group, and can be obtained using the get_concept_id function.\n",
    "    property_id refers to a certain property of the given entity referred to by the concept_id.\n",
    "\n",
    "    Returns None if any of this can't be found for whatever reason.\n",
    "\n",
    "    e.g. \"Q219655\" is the concept_id for Carey Mulligan; \"P21\" is the property_id for gender.\n",
    "    So we have that get_property(session, \"Q219655\", \"P21\") -> \"female\".\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetclaims\",\n",
    "        \"entity\": concept_id,\n",
    "        \"property\": property_id,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        try:\n",
    "            res = await resp.json()\n",
    "        except Exception as e:\n",
    "            print(resp)\n",
    "            raise e\n",
    "\n",
    "    if property_id not in res[\"claims\"]:\n",
    "        return None\n",
    "    # This gives yet another 'id', and we then need to perform yet another HTTP\n",
    "    # request to find the actual *label* that this corresponds to.\n",
    "    else:\n",
    "        id = None\n",
    "        for prop in res[\"claims\"][property_id]:\n",
    "            try:\n",
    "                id = prop[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if id is None:\n",
    "            return None\n",
    "        else:\n",
    "            new_params = {\n",
    "                \"action\": \"wbgetentities\",\n",
    "                \"ids\": id,\n",
    "                \"languages\": \"en\",\n",
    "                \"format\": \"json\",\n",
    "                \"props\": \"labels\",\n",
    "            }\n",
    "            async with session.get(url, params=new_params) as resp:\n",
    "                try:\n",
    "                    res = await resp.json()\n",
    "                except Exception as e:\n",
    "                    print(resp)\n",
    "                    raise e\n",
    "            try:\n",
    "                return res[\"entities\"][id][\"labels\"][\"en\"][\"value\"]\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "async def get_concept_id(session, page_name):\n",
    "    \"\"\"\n",
    "    Get the concept_id corresponding to a particular Wikipedia page. For some odd reason, some Wikipedia\n",
    "    pages don't have concept IDs. In such a case, we return None.\n",
    "\n",
    "    e.g. get_concept_id(session, \"Carey Mulligan\") -> \"Q219655\"\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": page_name,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    music_markers = [\n",
    "        \"singer\",\n",
    "        \"artist\",\n",
    "        \"musician\",\n",
    "        \"music\",\n",
    "        \"band\",\n",
    "        \"group\",\n",
    "        \"duo\",\n",
    "        \"ensemble\",\n",
    "    ]\n",
    "\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        # Titles of WP pages that match the search query.\n",
    "        json = await resp.json()\n",
    "\n",
    "    result = json[\"search\"]\n",
    "\n",
    "    if len(result) == 0:\n",
    "        # Couldn't find a concept id for the person/group\n",
    "        return None\n",
    "\n",
    "    # By default, choose the first result from the list\n",
    "    target = 0\n",
    "    # But check the other results to see if any of them are musicians (as\n",
    "    # indicated by the markers) and Eurovision contestants\n",
    "    for i, res in enumerate(result):\n",
    "        if \"description\" in res[\"display\"]:\n",
    "            description = res[\"display\"][\"description\"][\"value\"]\n",
    "            if any(markers in description for markers in music_markers):\n",
    "                concept_id = res[\"id\"]\n",
    "                contestant_in = await get_property(session, concept_id, \"P1344\")\n",
    "                if contestant_in is not None and \"Eurovision\" in contestant_in:\n",
    "                    target = i\n",
    "    # Return the concept ID of the result found\n",
    "    return result[target][\"id\"]\n",
    "\n",
    "\n",
    "async def lookup_gender(session, page_name):\n",
    "    \"\"\"Find gender of a performing act, using the name associated with their\n",
    "    Wikipedia page. Returns None if could not be found.\n",
    "    \"\"\"\n",
    "    concept_id = await get_concept_id(session, page_name)\n",
    "    if concept_id is None:\n",
    "        return None\n",
    "\n",
    "    gender = await get_property(session, concept_id, \"P21\")\n",
    "    instance = await get_property(session, concept_id, \"P31\")\n",
    "    if gender is None and instance is None:\n",
    "        return None\n",
    "    elif gender is None and instance is not None:\n",
    "        group_checks = [\"group\", \"duo\", \"trio\", \"music\", \"band\", \"ensemble\"]\n",
    "        if any(x in instance for x in group_checks):\n",
    "            return \"group\"\n",
    "    else:\n",
    "        return gender\n",
    "\n",
    "\n",
    "async def get_pages(session, name):\n",
    "    \"\"\"Obtain a list of Wikipedia pages obtained by searching for a name.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"namespace\": \"0\",\n",
    "        \"search\": name,\n",
    "        \"limit\": \"10000\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        # Titles of WP pages that match the search query.\n",
    "        json = await resp.json()\n",
    "    return json[1]\n",
    "\n",
    "\n",
    "async def get_artist_gender(session, name):\n",
    "    gender = None\n",
    "    # Get the WP page for this person/group\n",
    "    pages = await get_pages(session, name)\n",
    "    # If there's one, try to get their gender from the first page\n",
    "    if len(pages) > 0:\n",
    "        gender = await lookup_gender(session, pages[0])\n",
    "    # Finally we use some heuristics to cover some edge cases\n",
    "    if gender is None:\n",
    "        if \"&\" in name or \"feat.\" in name:\n",
    "            return \"group\"\n",
    "\n",
    "    return gender"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we make use of the collapsed library functions. The code pulls the list of artists from the data we already collected above.  We use an async function to access Wikidata in batches of forty artists. This allows us to make multiple HTTP requests simultaneously and greatly decreases the overall time needed to work through the list.\n",
    "\n",
    "Having extracted as much as possible from Wikidata the code then performs the manual fixes to complete the missing entries, and to ensure transgender entries are regarded as simply *male* or *female* as appropriate. Finally the results are merged in with our existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_performers = df_VL['Artist'].unique().tolist()\n",
    "n_performers = len(all_performers)\n",
    "MAX_CONCURRENT = 40   # To stop Wikipedia from complaining about 'too many requests'\n",
    "USER_AGENT = 'Eurovision study @ The Alan Turing Institute mailto:jyong@turing.ac.uk'\n",
    "\n",
    "async def get_all_genders():\n",
    "    genders = []\n",
    "    print(f'We need to fetch the genders of {n_performers} performers, in batches of {MAX_CONCURRENT}. Hold tight...')\n",
    "    async with aiohttp.ClientSession(headers={'User-Agent': USER_AGENT}) as session:\n",
    "        start = 0\n",
    "        end = MAX_CONCURRENT\n",
    "        while start < n_performers:\n",
    "            print(f'Getting genders for performers #{start + 1} to #{end}... ', end='')\n",
    "            batch_tasks = asyncio.gather(*[get_artist_gender(session, p) for p in all_performers[start:end]])\n",
    "            batch_genders = await batch_tasks\n",
    "            print(f'Got {len(batch_genders)} results, {len([g for g in batch_genders if g is None])} of which were None.')\n",
    "            genders = genders + batch_genders\n",
    "            start = end\n",
    "            end = min(end + MAX_CONCURRENT, n_performers)\n",
    "            await asyncio.sleep(1.5)   # Put a pause between batches to avoid being timed out\n",
    "\n",
    "    assert len(genders) == n_performers  # Check for off-by-one errors\n",
    "    print('Finished downloading gender data.')\n",
    "    return dict(zip(all_performers, genders))\n",
    "\n",
    "gender_dict = await get_all_genders()\n",
    "\n",
    "# Manually assign missing entries (the Nones).\n",
    "male = ['Michael Hajiyanni', 'Charlie', 'Tüzmen', 'Mietek Szcześniak', 'Olexandr', 'Max', 'Brinck',\n",
    "        'Sakis Rouvas (2)', 'Gianluca', 'Frans', 'Chingiz', 'Mahmood', 'Serhat (2)', 'Miki', 'Stefan']\n",
    "female = ['Gunvor', 'Selma', 'Charlotte Nilsson (Perrelli)', 'Karolina', 'Laura', 'Rosa', 'Lou', 'Nicola',\n",
    "        'Karmen', 'Sanda', 'Ortal', 'Gracia', 'Chiara (2)', 'Hanna', 'Chiara (3)', 'Elena', 'Lena (2)',\n",
    "        'Birgit', 'Samra', 'ZAA Sanja Vučić', 'Anja', 'Alma', 'Netta', 'Michela', 'Efendi', 'Victoria',\n",
    "        'Destiny', 'Amanda Georgiadi Tenfjord', 'MARO']\n",
    "group = ['Eden', 'Voice', 'Taxi', 'One', 'Prime Minister', 'Fame', 'Regina (band)', 'ESDM',\n",
    "        'Tolmachevy Sisters', 'Minus One', 'AWS']\n",
    "for p in male:\n",
    "    gender_dict[p] = \"male\"\n",
    "for p in female:\n",
    "    gender_dict[p] = \"female\"\n",
    "for p in group:\n",
    "    gender_dict[p] = \"group\"\n",
    "\n",
    "# Wikipedia needs to learn that 'trans woman' is 'female'.\n",
    "for k, v in gender_dict.items():\n",
    "    if v == 'trans woman':\n",
    "        gender_dict[k] = 'female'\n",
    "\n",
    "# Add gender to the dataframe.\n",
    "df_VLG = df_VL.copy()\n",
    "df_VLG['gender'] = df_VLG['Artist'].map(gender_dict)\n",
    "df_VLG.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration data\n",
    "\n",
    "Migration data were gathered to report on the estimated migration betweeen each voting–performing country pair of countries that have previously competed. \n",
    "\n",
    "Data were collected from two sources. First we took migration data from [Our World in Data](https://ourworldindata.org/migration), under the 'Explore data on where people migrate from and to' section. The underlying data originate from the UN. The data show the total number of immigrants in each country split by country of origin in the years 1990–2020, recorded at intervals of every 5 years.\n",
    "\n",
    "We clean up the data so that the country names match those we're already using and filter on countries that participate in Eurovision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration = (pd.read_csv('data/migration-flows.csv')\n",
    "    .pipe(pd.melt, id_vars=['Country', 'Year'], var_name='Migration', value_name='Count')  # to long format\n",
    "    .loc[lambda x: x['Migration'].str.contains('Emigrants')]                               # filter for emigrant rows\n",
    "    .pipe(lambda x: x.rename(columns = {col: col.lower() for col in x.columns}))           # lowercase column names                                                         \n",
    "    .assign(migration = lambda x: x.migration.str.replace('Emigrants from ', ''))          # filter for emigrant rows                          \n",
    "    .rename(columns={'migration': 'emigrated_from', 'country': 'emigrated_to'})            # boil down to country name\n",
    "    .query('count >= 0')                                                                   # negative counts are just total emigrants from country\n",
    "    .pipe(lambda x: x.assign(count = x['count'].astype(int)))                              # convert count to int     \n",
    ")\n",
    "\n",
    "# Clean up country names\n",
    "for ft in ['from', 'to']:\n",
    "    migration[f'emigrated_{ft}'] = migration[f'emigrated_{ft}'].str.lower()\n",
    "    migration.loc[migration[f'emigrated_{ft}'] == 'moldova', f'emigrated_{ft}'] = 'moldova, republic of'\n",
    "    migration.loc[migration[f'emigrated_{ft}'] == 'russia', f'emigrated_{ft}'] = 'russian federation'\n",
    "\n",
    "# Remove non-Eurovision countries\n",
    "ev_countries = set(df_VLG['from_country']).union(set(df_VLG['to_country']))\n",
    "migration = migration[(migration['emigrated_to'].isin(ev_countries)) & (migration['emigrated_from'].isin(ev_countries))]\n",
    "\n",
    "# Add in country codes\n",
    "def get_country_codes(name):\n",
    "    return df_VLG[df_VLG['from_country'] == name].iloc[0][['from_code2', 'from_code3']]\n",
    "\n",
    "for ft in ['from', 'to']:\n",
    "    migration[f'emigrated_{ft}_code2'], migration[f'emigrated_{ft}_code3'] = zip(*migration[f'emigrated_{ft}'].map(get_country_codes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we're able to get data for all of the countries except Yugoslavia which is missing from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_countries = set(migration['emigrated_to']).union(set(migration['emigrated_from']))\n",
    "print('Eurovision countries without data: {}'.format(''.join(ev_countries - migration_countries)))  # No data for Yugoslavia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide a fairer comparison we convert absolute migration numbers into proportions of the overall population size of each country. For this we also need country population data, which we collected from the [World Bank](https://data.worldbank.org/indicator/SP.POP.TOTL?end=2021&start=2021&view=map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = (pd.read_csv('data/pop_sizes.csv')\n",
    "           .iloc[:, 3:]\n",
    "           .rename(columns=lambda x: x.lower().replace(' ', '_'))\n",
    "           .pipe(pd.melt, id_vars=['country_code'], var_name='year', value_name='population')\n",
    "           .assign(year=lambda x: x['year'].apply(lambda y: y.split('_')[0]))\n",
    "           .assign(year=lambda x: x['year'].astype(int))\n",
    "           .rename(columns={'country_code': 'code3'})\n",
    "           .dropna()\n",
    "           .assign(population=lambda x: pd.to_numeric(x['population'], errors='coerce'))\n",
    ")\n",
    "\n",
    "migration_and_pop = (migration.merge(pop_size, left_on=['year', 'emigrated_to_code3'], right_on=['year', 'code3'], how='left')\n",
    "                    .rename(columns={'population': 'population_to'})\n",
    "                    .assign(prop_emigrants=lambda x: x['count'] / x['population_to'])\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UN migration data are recorded in five year intervals, so for each year of Eurovision data we assign the most recent recording. In essence, we end up repeating each row of the migration data five times, once for each of the following years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_and_pop['migration_pop_year'] = migration_and_pop['year']\n",
    "\n",
    "total_migration_and_pop = migration_and_pop.copy()\n",
    "for i in range(1, 5):\n",
    "    next_migration_and_pop = migration_and_pop.copy()\n",
    "    next_migration_and_pop['year'] = next_migration_and_pop['year'] + i\n",
    "    total_migration_and_pop = pd.concat([total_migration_and_pop, next_migration_and_pop], ignore_index=True)\n",
    "\n",
    "total_migration_and_pop = total_migration_and_pop.sort_values(by=[\"emigrated_from\", \"emigrated_to\", \"year\"])\n",
    "total_migration_and_pop = total_migration_and_pop[['emigrated_from_code2', 'emigrated_to_code2', 'year', 'count', 'population_to', 'prop_emigrants', 'migration_pop_year']]\n",
    "\n",
    "# Output a sample of the data including only some of the more relevant columns\n",
    "total_migration_and_pop[['emigrated_to_code2', 'emigrated_from_code2', 'year', 'population_to', 'prop_emigrants']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we integrate this with our existing dataset. We'll skip this step for brevity and pull the data in direction from the [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb) as before instead.\n",
    "\n",
    "The result is our original dataframe with additional columns with migration and population data. Note the caveat that we're not working with perfect data here: we don't have data for Yugoslavia, and our data are repeated for the unrecorded years between the five year intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VLGM = pd.read_csv('data/migration.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition winners\n",
    "\n",
    "Traditionally, the winner of each Eurovision contest becomes the host country for the following year's contest, which is considered to be an expensive operation. This has led to a recurring argument that many countries do not want to win, and therefore put in less effort. At the same time, it has previously been proposed that the number of years since a country's last win influences how likely they are to do well due to how competitive their performance is.\n",
    "\n",
    "In order to investigate this, we calculated how long each performing country had gone without a win in Eurovision. To achieve this, the previous contest winners were gathered from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Eurovision_Song_Contest_winners). \n",
    "\n",
    "Finally we integrate this with our existing dataset. We'll skip this step for brevity and pull the data in direction from the [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb) as before instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VLGMC = pd.read_csv('data/comps_without_win.csv')\n",
    "\n",
    "# Output a sample of the data for 2022, including only some of the more relevant columns\n",
    "df_VLGMC[df_VLGMC['year'] == 2022][['to_country', 'comps_without_win']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared border\n",
    "\n",
    "It's been suggested that neighbouring countries may be *more* &mdash; or contrariwise *less* &mdash; likely to vote for one another than countries further away. Is this really the case, and if so, which is it? Or is it just a case of commentator cynicism and *post-hoc* rationalisation?\n",
    "\n",
    "We decided to find out by adding shared borders between countries into our model. We collected these country border data from [GeoDataSource](https://github.com/geodatasource/country-borders/). This only includes land borders, so sea borders are not considered. \n",
    "\n",
    "For the detailed process of cleaning up and merging the data into our dataset, see the [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents our final dataset for the historical data\n",
    "df = df_VLGMCB = pd.read_csv('data/df_main.csv')\n",
    "\n",
    "# Output a sample of the data including only some of the more relevant columns\n",
    "df_VLGMCB[['from_country', 'to_country', 'has_border']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Years covered: {} to {}'.format(df['year'].min(), df['year'].max()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023 performers\n",
    "\n",
    "Existing data sources have provided data covering all previous years up to 2022. As we noted above, we restrict our data to 1998 onwards so that we can focus on years in which both public and jury voting was used.\n",
    "\n",
    "Our model will make predictions based on the parameters we've provided &mdash; things like the country a performer is from, their gender and the language of the song &mdash; which means that in order to make predictions for 2023, we'll need to know the same characteristics for the 2023 entries. Our model can then use these characteristics in combination with the historical data, to predict the ranking, or scores, of the performances for 2023.\n",
    "\n",
    "The final step in the collection of our data was therefore to find information about the upcoming performances in 2023. These data were gathered from [Wikipedia](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2023). Some of the parameters for which we weren't able to obtain more recent data &mdash; such as migration numbers &mdash; we repeat the entries from previous years.\n",
    "\n",
    "The code for pulling this data together repeats many of the steps we described previously for the historical data, and so we won't go through the details again again here. You can see the full process in the [data collection notebook](https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/story_notebook/eurovision/notebooks/data.ipynb). As usual we will pull in the results as they were output from that notebook and just review a sample.\n",
    "\n",
    "This gives us all of the data we need. In the next section we'll examine this data and find out what it tells us about Eurovision entries and voting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = pd.read_csv('data/df_2023.csv')\n",
    "\n",
    "# Output a sample of the data including only some of the more relevant columns\n",
    "future[['to_country', 'gender', 'Contains_English', 'Contains_NonEnglish', 'comps_without_win']].sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualisations\n",
    "\n",
    "Before we start modelling, we explore patterns in the data through summary statistics and visualisations. The aim here is to gain an intuition about trends, relationships between variables, potential quirks and outliers and voting patterns between countries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best performing countries\n",
    "\n",
    "Let's first have a look at the winners in each year from 1998 to 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = (df.loc[df['rank'] == 1, ['to_country', 'total_points', 'rank', 'to_code2', 'year']]\n",
    "             .drop_duplicates()\n",
    "             )\n",
    "winners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winning Eurovision is hard! Only four countries have managed to win more than once since 1998, and only Ukraine 🇺🇦 and Sweden 🇸🇪  won three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_winners = winners['to_country'].value_counts().loc[lambda x: x > 1]\n",
    "multiple_winners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winning is one thing, but which countries perform best _on average_? To make this statistic a bit more robust, we only look at countries that have made the finals at least five times within the timeframe we're considering (see the `count` variable).\n",
    "Over this period, Italy 🇮🇹 has the highest average score, closely followed by Bulgaria 🇧🇬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_scorers = (df.loc[:, ['to_country', 'to_code2', 'total_points', 'year']]\n",
    "                   .drop_duplicates()\n",
    "                   .groupby('to_country')['total_points']\n",
    "                   .agg(['count', 'mean'])\n",
    "                   .query('count >= 5')\n",
    "                   .round({'mean': 2})\n",
    "                   .sort_values(by='mean', ascending=False)\n",
    "                   .head()\n",
    "                   )\n",
    "highest_scorers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While summary statistics like the average can be informative, it would be really interesting to see how well a country is doing over the years in a bit more detail.\n",
    "Are the top performing countries always much better than average, or do a few great perfomances lift up the average scores of these countries?\n",
    "\n",
    "To visualise this for different countries, we create a function to plot the scores of a country over time and mark specific years where the country either didn't perform in the final at all or where it has actually won the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "my_cmap = plt.get_cmap(\"magma_r\")\n",
    "rescale = lambda y: y / 26\n",
    "\n",
    "def plot_country_history(country, ax, df_plot):\n",
    "    df_country_entries = (df_plot\n",
    "                          .loc[df_plot['to_country'] == country]\n",
    "                          .sort_values('year', ascending=False)\n",
    "                          )\n",
    "    country_name = df_country_entries['to_country'].iloc[0].title()\n",
    "\n",
    "    # Add in NaN entries for years where the country did not perform in the finals\n",
    "    present_years = df_country_entries['year'].unique()\n",
    "    absent_years = list(set(range(1998, 2023)) - set(present_years))\n",
    "    df_absent_entries = pd.DataFrame(dict(year=absent_years, total_points=np.nan,\n",
    "                        rank= np.nan, to_country=country))\n",
    "    df_country_entries = pd.concat([df_country_entries, df_absent_entries], ignore_index=True)\n",
    "\n",
    "    # Plot data\n",
    "    ax.bar(df_country_entries['year'], df_country_entries['total_points'],\n",
    "            color=my_cmap(rescale(df_country_entries['rank'])))\n",
    "\n",
    "    # Annotate absent entries, with a special marker for 2020 (cancelled)\n",
    "    for year in absent_years:\n",
    "        marker = \"_\" if year == 2020 else \"x\"\n",
    "        ax.scatter(x=year, y=0.03, s=25, color='grey', marker=marker, transform=ax.get_xaxis_transform())\n",
    "\n",
    "    # Annotate winning entries\n",
    "    winning_entries = df_country_entries.loc[df_country_entries['rank'] == 1, ['year', 'total_points']].drop_duplicates()\n",
    "    if winning_entries.shape[0] > 0:\n",
    "        ax.scatter(x=winning_entries['year'], y=winning_entries['total_points'], s=50, color='gold', marker=\"*\", edgecolor='black', zorder=5)\n",
    "\n",
    "    ax.set_xlim(1997, 2023)\n",
    "    ax.set_title(country_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the top performers, let's first have a look at the United Kingdom 🇬🇧. Every couple of years, the UK has a good year with a top-ten position, but hasn't come first since 1997 with [Katrina and the Waves](https://www.youtube.com/watch?v=azw4Kh8Rqpw) (not in our dataset). 2022 was the most successful year since then, with Sam Ryder becoming second with the song [Space Man](https://www.youtube.com/watch?v=RZ0hqX_92zI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4), sharey=True, sharex=True, squeeze=True)\n",
    "\n",
    "plot_country_history('united kingdom', ax, df)\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='*', color='white', label='Winner',\n",
    "                          markerfacecolor='gold', markersize=12, markeredgecolor='black'),                          \n",
    "                  Line2D([0], [0], marker='x', color='white', label='Did not perform in final',\n",
    "                          markerfacecolor='grey', markersize=8, markeredgecolor='grey'), \n",
    "                  Line2D([0], [0], marker='_', color='white', label='Competition cancelled',\n",
    "                          markerfacecolor='grey', markersize=8, markeredgecolor='grey')]\n",
    "fig.legend(handles=legend_elements, loc='right', ncol=1, bbox_to_anchor=(0.9, -0.05))\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=plt.Normalize(vmin=1, vmax=26))\n",
    "cbaxes = fig.add_axes([0.2, -0.05, 0.2, 0.05]) # x y deltax deltay\n",
    "\n",
    "fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1, label='Position', \n",
    "             cax = cbaxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the UK, how do the _most successful_ countries perform over the years? We'll have a look at the top four countries, for both the total and the average scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "fig = plt.figure(layout='constrained', figsize=(12, 6))\n",
    "subfigs = fig.subfigures(2, 1, wspace=0.1)\n",
    "\n",
    "for subfig, country_group, title in zip(subfigs,\n",
    "                                        [multiple_winners, highest_scorers],\n",
    "                                        ['Multiple winners', 'Countries with highest average points']\n",
    "                                        ):\n",
    "    axs = subfig.subplots(1, 4, sharey=True, sharex=True, squeeze=True)\n",
    "    subfig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    for i, country in enumerate(country_group.index[:4]): \n",
    "        plot_country_history(country, axs[i], df)\n",
    "\n",
    "# Add in colorbar and legend\n",
    "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=plt.Normalize(vmin=1, vmax=26))\n",
    "cbaxes = fig.add_axes([0.3, -0.08, 0.2, 0.05]) # x y deltax deltay\n",
    "fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1, label='Position', cax=cbaxes)\n",
    "fig.legend(handles=legend_elements, loc='right', ncol=1, bbox_to_anchor=(0.7, -0.08))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a clear difference to the picture of the UK here.\n",
    "Ukraine 🇺🇦 and Sweden 🇸🇪 have not only won three times each, but have also placed within the top ten many times over the years (indicated by the lighter bars).\n",
    "Some countries haven't participated very often, but have done really well the few times they did, such as Bulgaria 🇧🇬.\n",
    "This is reflected in their second-place ranking in the table of average scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting patterns between country pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more to explore in the Eurovision data.\n",
    "Countries are performing but are also voting for each other.\n",
    "Do countries mainly vote for the actual performance or are the some general biases between countries?\n",
    "To dig deeper into this, we'll explore voting patterns between country pairs.\n",
    "\n",
    "We start by calculating the score each country has given to each other country, averaged over all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voting = (df.loc[:, ['from_country', 'points', 'to_country', 'year']]\n",
    "               .assign(average_vote=lambda x: x.groupby(['to_country', 'from_country'])['points'].transform('mean'))\n",
    "               .drop_duplicates(subset=['from_country', 'to_country'])\n",
    "               .loc[:, ['from_country', 'to_country', 'average_vote']]\n",
    "               .pivot(index='from_country', columns='to_country', values='average_vote')\n",
    "               )\n",
    "df_voting.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a *heatmap* to illustrate this data.\n",
    "In this color scheme we have chosen, darker squares indicate a *larger* average vote given by one country to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import scipy.spatial as sp\n",
    "import scipy.cluster.hierarchy as hc\n",
    "\n",
    "def plot_voting_heatmap(df_heatmap, cmap, suptitle, center=None):\n",
    "    # Reindex rows to be in the same order as columns\n",
    "    df_heatmap = df_heatmap.reindex(df_heatmap.columns)\n",
    "\n",
    "    row_dism = 1 - df_heatmap.T.corr()\n",
    "    row_linkage = hc.linkage(sp.distance.squareform(row_dism), method='complete')\n",
    "    plot = sns.clustermap(df_heatmap, row_linkage=row_linkage, col_linkage=row_linkage, \n",
    "                          figsize=(9, 8),\n",
    "                          mask=df_heatmap.isnull(), \n",
    "                          dendrogram_ratio=[0.15, 0.01],\n",
    "                          cbar_pos=(0.8, 1.01, 0.1, 0.019),\n",
    "                          cbar_kws={'orientation': 'horizontal'},\n",
    "                          cmap=cmap, \n",
    "                          xticklabels=1,\n",
    "                          yticklabels=1,\n",
    "                          **{'center': center} if center is not None else {})\n",
    "    plot.ax_col_dendrogram.set_visible(False) \n",
    "\n",
    "    plot.fig.suptitle(suptitle, fontsize=16, y=1.02)\n",
    "    plot.ax_heatmap.set_xlabel('Performing country')\n",
    "    plot.ax_heatmap.set_ylabel('Voting country')\n",
    "    plot.ax_heatmap.tick_params(axis='both', which='major', labelsize=9)\n",
    "    plot.ax_heatmap.xaxis.tick_bottom()   # otherwise x-axis ticks disappear, see: mwaskom/seaborn#2305\n",
    "    return plot\n",
    "\n",
    "plot_voting_heatmap(df_voting, cmap='magma_r', suptitle='Average score given to performing countries')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, countries are grouped using hierarchical clustering, ordering countries with similar voting patterns together. \n",
    "\n",
    "From here, we can see a few patterns emerging.\n",
    "Some countries tend to vote higher for each other, forming darker squares in the heatmap.\n",
    "One such square in the bottom right is around Slovenia 🇸🇮, Bosnia and Herzegovina 🇧🇦, Croatia 🇭🇷, and Serbia 🇷🇸, which all give each other relatively high votes on average.\n",
    "\n",
    "However, the *votes* alone don't say that much.\n",
    "As we've already seen, some countries simply tend to perform well at Eurovision; this would naturally make their average received scores larger!\n",
    "To extract this factor from the analysis, so that we can look into potential country-pair voting biases, we therefore look for *deviations* from a country's average performance score.\n",
    "Essentially, we are asking whether a certain country gives higher or lower votes to another country, compared to all other countries.\n",
    "\n",
    "We start by calculating the voting deviations from the mean each country receives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voting_deviation = (df.loc[:, ['from_country', 'points', 'to_country', 'year']]\n",
    "                          # This is the average vote from one country to another, the same as above.\n",
    "                         .assign(average_vote=lambda x: x.groupby(['to_country', 'from_country'])['points'].transform('mean'))\n",
    "                          # This is the average vote received by one country from all others.\n",
    "                         .assign(average_votes_all=lambda x: x.groupby(['to_country'])['points'].transform('mean'))\n",
    "                          # Calculate the deviation from the mean.\n",
    "                         .assign(vote_deviation=lambda x: x['average_vote'] - x['average_votes_all'])\n",
    "                          # We drop any voter-performer pairs for which there are fewer than three votes, as this is likely\n",
    "                          # not representative of the country's true voting pattern.\n",
    "                         .assign(count=lambda x: x.groupby(['from_country', 'to_country'])['vote_deviation'].transform('count'))\n",
    "                         .query('count >= 3')\n",
    "                         .drop_duplicates(subset=['from_country', 'to_country'])\n",
    "                         .loc[:, ['from_country', 'to_country', 'vote_deviation']]\n",
    "                         .pivot(index='from_country', columns='to_country', values='vote_deviation')\n",
    "                         )\n",
    "df_voting_deviation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the same heatmap, but with the deviations instead of the actual votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "plot = plot_voting_heatmap(df_voting_deviation, cmap='RdBu_r',\n",
    "                           suptitle='Average vote deviation from one country to another',\n",
    "                           center=0)\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Plot a box around the given entries, as specified by indices\n",
    "def highlight(xmin, ymin, dx, dy, color):\n",
    "    plot.ax_heatmap.add_patch(Rectangle((xmin, ymin), dx, dy, fill=False, edgecolor=color, linewidth=2))\n",
    "    # Plot the reverse as well, plus a connecting line, if the box is not symmetric about the diagonal\n",
    "    if xmin != ymin or dx != dy:\n",
    "        plot.ax_heatmap.add_patch(Rectangle((ymin, xmin), dy, dx, fill=False, edgecolor=color, linewidth=2))\n",
    "        plot.ax_heatmap.plot([xmin+dx, ymin], [ymin, xmin+dx], color=color, linewidth=1, linestyle='--')\n",
    "\n",
    "highlight(4, 4, 9, 9, 'black')                # ex-Soviet\n",
    "highlight(31, 31, 13, 13, 'black')            # Baltics\n",
    "highlight(13, 13, 7, 7, 'black')              # Balkans\n",
    "\n",
    "highlight(0, 27, 1, 4, 'cornflowerblue')      # Turkey to Germany, the Netherlands, Belgium, France\n",
    "highlight(20, 25, 4, 1, 'darkcyan')           # Italy to Malta, San Marino, Albania, Portugal\n",
    "highlight(13, 38, 1, 1, 'indigo')             # Serbia to Hungary\n",
    "highlight(17, 21, 1, 1, 'lightseagreen')      # North Macedonia to Albania\n",
    "highlight(23, 30, 1, 1, 'goldenrod')          # Portugal to France\n",
    "highlight(1, 3, 1, 1, 'fuchsia')              # Greece to Cyprus\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the colorscheme chosen assigns blue to negative deviations (i.e. a voting country gives lower-than-average votes to a performing country), and red to positive deviations.\n",
    "\n",
    "There do not appear to be any significant *negative* deviations.\n",
    "However, there are a number of interesting regions with positive deviations, which are highlighted by additional coloured boxes.\n",
    "For example:\n",
    "\n",
    "- The small blue boxes on the left indicate that Turkey's entries have consistently obtained higher-than-average scores from Germany, the Netherlands, Belgium, and France; but that this isn't reciprocated, as the other blue box on the top shows.\n",
    "  A possible hypothesis for this may be the fact that there is a significant Turkish diaspora in these countries; this reinforces our decision to include migration statistics in our collated dataset.\n",
    "\n",
    "- The large black boxes along the diagonal show clusters of countries who vote highly for each other and tend to be closer geographically.\n",
    "  For example, the cluster in the bottom-right contains many Nordic countries, the box in the centre Balkan countries, and the top-left box ex-Soviet countries.\n",
    "  Overall, this suggests that sharing a border and and other geopolitical relationships may influence how countries vote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Pair Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at these inter-country relationships using scatter plots.\n",
    "In the following plots, each point is an average voting score from one country to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "pair_grouped_points = df.groupby(['to_country', 'from_country'])['points']\n",
    "\n",
    "votes = (df.loc[:, ['to_country', 'from_country', 'points', 'year']]\n",
    "            .assign(\n",
    "                # Number of times the country performed in Eurovision finals\n",
    "                times_competed=lambda x: x.groupby(['to_country'])['year'].transform('nunique'),\n",
    "                # The total number of points country A has given to country B, across all years\n",
    "                total_points=pair_grouped_points.transform('sum'),\n",
    "                # The number of times country A has given (nonzero) points to country B, across all years\n",
    "                times_voted=pair_grouped_points.transform(lambda x: x[x > 0].count()),\n",
    "                # The average number of points country A has given to country B, across all years\n",
    "                average_points=pair_grouped_points.transform('mean'),\n",
    "                # The average number of points country B has gotten, across all voting countries and all years\n",
    "                overall_average_points=lambda x: x.groupby(['to_country'])['points'].transform('mean'),\n",
    "                # The deviation from the average points given by country A to country B\n",
    "                vote_deviation=lambda x: x['average_points'] - x['overall_average_points'])\n",
    "            .loc[:, ['to_country', 'from_country', 'times_competed', 'times_voted', 'total_points', 'average_points', 'overall_average_points', 'vote_deviation']]\n",
    "            .drop_duplicates(subset=['to_country', 'from_country'])\n",
    "            .sort_values(by=['to_country', 'from_country'])\n",
    "            .reset_index(drop=True)\n",
    ")\n",
    "votes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an *interactive* scatter plot, which will allow you to better explore the data yourself.\n",
    "Each data point represents one performer–voter pair; hovering over it will show specific information about each pair.\n",
    "\n",
    "The *x*-axis represents the total number of points which the voter has given the performer over the timeframe of our analysis, and the *y*-axis the average.\n",
    "Thus, for example, the upper-left corner contains country pairs which have not voted many times (leading to a small total), but with a large average (meaning that very high scores have been given on average).\n",
    "The top-leftmost point reveals that Serbia 🇷🇸 has only voted for Montenegro 🇲🇪 once, but gave them 12 points, the highest possible score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import plotly.express as px\n",
    "\n",
    "# The following lines are needed for Quarto to render the interactive plot\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "fig = px.scatter(votes, x='total_points', y='average_points')\n",
    "fig.update_traces(hovertemplate=('Performer: %{customdata[0]}'\n",
    "                                 '<br>Voter: %{customdata[1]}'\n",
    "                                 '<br>Total Eurovisions competed: %{customdata[2]}'\n",
    "                                 '<br>Total times voted for by selected country: %{customdata[3]}'\n",
    "                                 '<br>Total points given: %{customdata[4]}'\n",
    "                                 '<br>Average points: %{customdata[5]:.2f}'),\n",
    "                  customdata=votes,\n",
    "                  marker={'color': 'rgba(50, 50, 150, 0.1)', 'opacity': 0.5, 'size': 6,\n",
    "                          'line': {'color': 'rgba(50, 50, 150, 1.0)', 'width': 1}})\n",
    "fig.update_layout(hoverlabel_align='left', width=640, height=640, margin=dict(l=20, r=20, t=20, b=20),\n",
    "                  xaxis={'title': 'Total points given by one country to another'},\n",
    "                  yaxis={'title': 'Average points given by one country to another'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can visualise the vote deviations for each performer–voter pair as we did above.\n",
    "We reuse the same colour scheme here, with red implying a strong positive preference (i.e. a country that consistently gave higher-than-average votes to another) and blue indicating a negative preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "votes = (votes.assign(highest_average_point=lambda x: x.groupby(['to_country'])['average_points'].transform('max'))\n",
    "              .sort_values(by='highest_average_point', ascending=True))\n",
    "\n",
    "fig = px.scatter(votes, x='average_points', y='to_country', color='vote_deviation', \n",
    "                 color_continuous_scale=px.colors.diverging.RdBu_r,\n",
    "                 color_continuous_midpoint=0)\n",
    "fig.update_traces(hovertemplate=('Performer: %{customdata[0]}'\n",
    "                                 '<br>Voter: %{customdata[1]}'\n",
    "                                 '<br>Total Eurovisions competed: %{customdata[2]}'\n",
    "                                 '<br>Total times voted for by selected country: %{customdata[3]}'\n",
    "                                 '<br>Total points given: %{customdata[4]}'\n",
    "                                 '<br>Average points overall: %{customdata[7]:.2f}'),\n",
    "                  customdata=votes)\n",
    "fig.update_layout(hoverlabel_align='left', width=640, height=640, margin=dict(l=20, r=20, t=20, b=20),\n",
    "                  xaxis={'title': 'Average points received from each voter'},\n",
    "                  yaxis={'title': 'Performing country'}, \n",
    "                  coloraxis_colorbar=dict(title='Deviation from average points'))\n",
    "\n",
    "fig.update_yaxes(tickfont_size=8)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Country Friendships, Biases, and One-sided Relationships"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the broader patterns above, let's now dive into some specific country-pairs. \n",
    "We will look for the top country pairs which\n",
    "\n",
    "- give each other very high scores\n",
    "- give each other very low scores\n",
    "- have a one-sided relationship, where one country gives high scores to another, but receives low scores back.\n",
    "\n",
    "First, we'll filter out countries which haven't participated frequently enough, to ensure that our average voting score is a robust estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# how often did each country participate?\n",
    "n_participations = (df\n",
    "                    .groupby('to_country')['year']\n",
    "                    .nunique()\n",
    "                    .sort_values(ascending=True))\n",
    "# filter countries with less than 5 participations\n",
    "countries_to_remove = n_participations[n_participations < 5].index.tolist()\n",
    "# remove from df\n",
    "df_pairs = df[~df['to_country'].isin(countries_to_remove)]\n",
    "# get random 5 rows from df_pairs\n",
    "# df_pairs.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at more specific relationships, we'll calculate for each country pair how much their average votes for each differ. This value would be small if they both tend to give each other high or low scores, but it would be large if one country gives the other high scores, but gets low scores back. To check whether this worked, we have a look at the top five country pairs that give each other high votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Grouping by performer and voter, calculate mean votes and count number of years\n",
    "df_pairs = (df_pairs\n",
    "    .groupby(['from_country', 'to_country'])\n",
    "    .agg(votes = ('points', 'mean'), num_years = ('year', 'count'))\n",
    "    .reset_index()\n",
    "    .sort_values('votes', ascending=False)\n",
    ")\n",
    "\n",
    "# Merge original dataframe with its reverse\n",
    "df_pairs = (df_pairs\n",
    "    .merge(df_pairs.rename(columns={'from_country': 'to_country', \n",
    "                               'to_country': 'from_country'}), \n",
    "           on=['from_country', 'to_country'])\n",
    "    .drop_duplicates()\n",
    "    .query('from_country != to_country')\n",
    "    .assign(votes_diff = lambda x: abs(x['votes_x'] - x['votes_y']))\n",
    "    .reindex(columns=['from_country', 'to_country', 'num_years', 'votes_x', 'votes_y', 'votes_diff'])\n",
    ")\n",
    "\n",
    "# create combined column with pairs in alphabetical order\n",
    "df_pairs['country_pair'] = df_pairs[['from_country', 'to_country']].apply(lambda x: ' - '.join(sorted(x)), axis=1)\n",
    "# Remove duplicate country pairs and the temporary country_pair column\n",
    "df_pairs = (df_pairs\n",
    "            .drop_duplicates(subset=['country_pair'])\n",
    "            .drop(columns=['country_pair']))\n",
    "\n",
    "df_pairs.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for plotting, we filter for the top five country pairs that give each other high, low and unequal votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Number of country pairs to show for each category\n",
    "N = 5\n",
    "\n",
    "# top 5 country pairs with high votes for each other\n",
    "top_highs = (df_pairs\n",
    "    .query('votes_diff < 3')\n",
    "    .sort_values('votes_x', ascending=False).head(N)\n",
    ")\n",
    "\n",
    "# top 5 country pairs with low votes for each other\n",
    "top_lows = (df_pairs\n",
    "    .query('votes_diff < 3')\n",
    "    .sort_values('votes_x', ascending=True).head(N)\n",
    ")\n",
    "\n",
    "# top 5 unbalanced country pairs (one gives high votes to the other, but gets low votes back)\n",
    "top_one_sided = (df_pairs\n",
    "            .sort_values('votes_diff', ascending=False).head(N))\n",
    "\n",
    "# combine \n",
    "top_relationships = (pd.concat([top_highs, top_lows, top_one_sided]))\n",
    "# add grouping\n",
    "top_relationships['group'] = ['high'] * N + ['low'] * N + ['one-sided'] * N\n",
    "\n",
    "# sort by votes_x\n",
    "top_relationships = top_relationships.sort_values('votes_x')\n",
    "\n",
    "top_relationships['y'] = range(1, len(top_relationships) + 1)\n",
    "\n",
    "top_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Replace group names with numeric values for sorting\n",
    "group_order = {'low': 1, 'one-sided': 2, 'high': 3}\n",
    "top_relationships['group_order'] = top_relationships['group'].replace(group_order)\n",
    "\n",
    "# Create a new column representing the minimum value between 'votes_x' and 'votes_y'\n",
    "top_relationships['min_votes'] = top_relationships[['votes_x', 'votes_y']].min(axis=1)\n",
    "\n",
    "# Sort the DataFrame by 'group_order' and 'min_votes'\n",
    "top_relationships = top_relationships.sort_values(by=['group_order', 'min_votes'], ascending=[True, True])\n",
    "\n",
    "# Reassign the 'y' column after sorting\n",
    "top_relationships['y'] = range(1, len(top_relationships) + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.grid(axis='x', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# create a color map with 6 colors from red to blue\n",
    "colors = sns.color_palette(\"YlOrRd\", 6)[::-1]\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom\", colors, N=len(colors))\n",
    "\n",
    "# get color map with viridis colors for 6 bins\n",
    "cmap = plt.get_cmap('viridis' )\n",
    "\n",
    "#colors = [\"#b2182b\", \"#d1e5f0\", \"#92c5de\", \"#4393c3\", \"#2166ac\", \"#053061\" ]\n",
    "colors = [\"#d53e4f\", \"#e6f598\", \"#abdda4\", \"#66c2a5\", \"#3288bd\", \"#5e4fa2\"]\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom\", colors, N=len(colors))\n",
    "\n",
    "\n",
    "# define the range boundaries\n",
    "bins = np.arange(0, 13, 2)\n",
    "\n",
    "# add text labels and points\n",
    "for i, row in top_relationships.iterrows():\n",
    "    ax.text(row['votes_x']+0.2, row['y'] ,row['to_country'],  ha='left', va='center', fontsize=14)  # increase the value added to 'votes_x' for more space\n",
    "    ax.text(row['votes_y']-0.2, row['y'] , row['from_country'],  ha='right', va='center', fontsize=14)  # increase the value subtracted from 'votes_y' for more space\n",
    "\n",
    "    # plot points with colors based on x-value range\n",
    "    x_color = np.digitize(row['votes_x'], bins=bins, right=True)-1\n",
    "    y_color = np.digitize(row['votes_y'], bins=bins, right=True)-1\n",
    "    ax.scatter(row['votes_x'], row['y'], color=cmap(x_color/6), s=100, alpha=1, zorder=3)\n",
    "    ax.scatter(row['votes_y'], row['y'], color=cmap(y_color/6), s=100, alpha=1, zorder=3)\n",
    "\n",
    "    # add line between countries with slightly shortened length\n",
    "    line_buffer = 0.1\n",
    "    ax.plot([row['votes_x'] + line_buffer, row['votes_y'] - line_buffer], [row['y'], row['y']], color='black', alpha=0.7, zorder=2, linewidth=0.7)\n",
    "\n",
    "ax.set_xlabel('Average received vote of a country from its paired country', fontsize=14)\n",
    "ax.set_ylabel('')\n",
    "\n",
    "# Hide the y axis\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.spines[['top', 'right', 'left']].set_visible(False)\n",
    "\n",
    "# Add legend\n",
    "markers = [plt.Line2D([0,0],[0,0], color=cmap(i/6), marker='o', linestyle='') for i in range(6)]\n",
    "plt.legend(markers, ['{}-{}'.format(i, i+2) for i in bins[:-1]], numpoints=1, loc='lower right', title='Vote ranges')\n",
    "\n",
    "plt.title('Country pairs with high, low, and unequal votes for each other', fontsize=18,  loc='center', pad=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the top five country pairs which give each other high, low and unequal scores.\n",
    "The point next to a country name is the score it *received* from the other country. For example, in line with the previous figures, both France 🇫🇷 and Germany 🇩🇪 on average receive low scores from Turkey 🇹🇷, but vote highly for Turkey.\n",
    "Cyprus 🇨🇾  and Greece 🇬🇷  give each other high scores, and Albania 🇦🇱 and Lithuania 🇱🇹 tend to give each other low scores.\n",
    "\n",
    "The plot differs a bit from the results above, because it is looking at raw votes, not deviations from the mean vote.\n",
    "Therefore, one-sided voting does not necessarily indicate bias in voting: it may simply be because the country has historically performed very well (or very poorly!) in the Eurovision Song Contest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration Data\n",
    "\n",
    " **TODO** Maybe plot prop_emigrants versus the score deviation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our main goals was to predict the winner of the 2023 contest. On a finer scale, this means predicting how each country will vote for each other country. Arguably, countries with a large immigrant population from another country might give more favorable votes to that country. To test and incorporate this in our models, we used migration data from [Our World in Data](https://ourworldindata.org/migration). Specifically, we were interested in the proportion of immigrants in each country from each other country. We calculated both sides, i.e. proportion emigrants from the voter country in the perfomer country (v2p) and the other way round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migration data\n",
    "df.filter(like='migr').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "df_migration = df[['year', 'from_code2', 'to_code2', 'prop_emigrants_v2p', 'prop_emigrants_p2v', 'points']].copy()\n",
    "\n",
    "# Calculate score deviation per year\n",
    "df_migration['avg_votes'] = df_migration.groupby(['to_code2', 'year'])['points'].transform('mean')\n",
    "df_migration['deviation'] = df_migration['points'] - df_migration['avg_votes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12, 6))\n",
    "sns.regplot(x='prop_emigrants_v2p', y='deviation', data=df_migration, ax=ax1)\n",
    "sns.regplot(x='prop_emigrants_p2v', y='deviation', data=df_migration, ax=ax2)\n",
    "\n",
    "ax1.set_xlabel('Proportion of emigrants from voting country in performing country')\n",
    "ax2.set_xlabel('Proportion of emigrants from performing country in voting country')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "sns.lmplot(x='prop_emigrants_p2v', y='prop_emigrants_v2p', data=df_migration, scatter_kws={\"s\": 4})\n",
    "\n",
    "# log scale axes\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Proportion of emigrants from voting country in performing country\")\n",
    "plt.ylabel(\"Proportion of emigrants from performing country in voting country\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in three things:\n",
    "1. Does the song contain english lyrics: yes vs no &#x2705;\n",
    "2. What are the average votes for songs with english lyrics vs non-english lyrics &#x2705;\n",
    "3. Does the performer sing in their official language yes vs no (english) vs no (non-english) &#x2705;\n",
    "4. How many languages appear in the song vs votes? (can you hedge your bets?) &#x2705;\n",
    "5. Are the votes received affected by whether or not a song is sung in the voting country's official language? &#x2705;\n",
    "\n",
    "**TODO** Get language data cleaned up, see https://github.com/KatrionaGoldmann/Eurovision_TDS/blob/issue-5-exploratory_visualisations/eurovision/notebooks/language_visualistaions.ipynb.\n",
    "\n",
    "**TODO** Discussion of Eurovision language rules (this has changed over time).\n",
    "\n",
    "In this dataset, we have categorised the song language into one or more of: the country's official language (`own`), English (`eng`), or an entirely separate language (`other`).\n",
    "When English *is* one of the country's official languages, both `own` and `other` are selected.\n",
    "We can use this data to investigate how likely each country is to perform in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "df_performance = df[['year', 'Artist', 'to_country', \n",
    "       'total_points', \n",
    "       'rank', 'to_code2',\n",
    "       'Official_languages', 'Language_sung', 'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'gender']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.loc[df_performance['year'] == 2018]['to_country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# get the percentage of performances who contain own language\n",
    "print('Percentage of performances in own language:', round(sum(df_performance['Contains_Own_Language'])/len(df_performance) *100), '%')\n",
    "print('Percentage of performances in English:', round(sum(df_performance['Contains_English'])/len(df_performance) *100), '%')\n",
    "\n",
    "print('Percentage of performances by gender:')\n",
    "df_performance['gender'].value_counts()/len(df_performance) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "df_language = df_performance.copy()\n",
    "\n",
    "df_performance['English_only'] = (df_performance['Contains_English']) & (df_performance['Number_of_Languages'] == 1 )\n",
    "df_performance['No_English'] = ~df_performance['Contains_English'] \n",
    "df_performance['Some_English'] = (df_performance['Contains_English']) & (df_performance['Number_of_Languages'] > 1 )\n",
    "\n",
    "# for each country get the ratio of songs that contain only English, some English and no English\n",
    "# then sort by the ratio of songs that contain only English\n",
    "df_language = df_performance.groupby('to_country').agg({'English_only': 'mean', 'Some_English': 'mean', 'No_English': 'mean'})\n",
    "\n",
    "df_language = df_language.sort_values(by=['English_only', 'Some_English'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "colours = {\"English_only\":'royalblue', \"No_English\":'seagreen', \"Some_English\":'goldenrod'}\n",
    "\n",
    "df_language.plot(kind='bar', figsize=(15, 6), stacked=True, color=colours)\n",
    "\n",
    "plt.legend(['English only', 'Partly English', 'No English'], title=\"Performance languages\", loc=[1, 1], \n",
    "        fontsize=14,  bbox_to_anchor=(0.51, 0., 0.5, 0.5), title_fontsize=16)\n",
    "\n",
    "plt.title('How frequently countries sing in English', fontsize=20, pad=30)\n",
    "\n",
    "plt.text(df_language.shape[0]-1, 1.1, 'Countries more likely to sing in English →', ha='right', va='center', fontsize=14)\n",
    "plt.text(0.05, 1.1, '← Countries less likely to sing in English ', ha='left', va='center',  fontsize=14)\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Ratio of performances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# convert wide to long format\n",
    "df_long = df_performance[['English_only', 'No_English',\t'Some_English', 'total_points']]\n",
    "\n",
    "df_long = df_long.melt(id_vars=['total_points'], var_name='language', value_name='contains_language')\n",
    "\n",
    "df_long['contains_language'] = df_long['contains_language'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import scipy.stats as stats\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "# boxplots for each language type\n",
    "ax = sns.boxplot(x='language', y='total_points', \n",
    "    data=df_long.loc[df_long['contains_language'] > 0], \n",
    "    palette=colours, showfliers=False, \n",
    "    order=['English_only', 'Some_English', 'No_English'])\n",
    "sns.stripplot(x='language', y='total_points', \n",
    "    order=['English_only', 'Some_English', 'No_English'],\n",
    "    data=df_long.loc[df_long['contains_language'] > 0], \n",
    "    jitter=0.25, size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "plt.title('Average points for performances in different languages', fontsize=20, pad=30)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Total points')\n",
    "\n",
    "add_stat_annotation(ax, data=df_long.loc[df_long['contains_language'] > 0],\n",
    "                    x='language', y='total_points', \n",
    "                    order=['English_only', 'Some_English', 'No_English'],\n",
    "                    box_pairs=[(\"English_only\", \"No_English\")],\n",
    "                    test='Mann-Whitney', text_format='star', verbose=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "df_language = df_performance.copy()\n",
    "\n",
    "df_performance['Own_language'] = (df_performance['Contains_Own_Language']) \n",
    "df_performance['Other_language'] = ~df_performance['Contains_Own_Language'] \n",
    "\n",
    "# for each country get the ratio of songs that contain only English, some English and no English\n",
    "# then sort by the ratio of songs that contain only English\n",
    "df_language = df_performance.groupby('to_country').agg({'Other_language': 'mean', 'Own_language': 'mean'})\n",
    "\n",
    "df_language = df_language.sort_values(by=['Own_language', 'Other_language'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "colours = {\"Own_language\":'teal', \"Other_language\":'lightsteelblue'}\n",
    "\n",
    "df_language.plot(kind='bar', figsize=(15, 6), stacked=True, color=colours)\n",
    "\n",
    "plt.legend(['Other language', 'Own language'], title=\"Performance languages\", loc=[1, 1], \n",
    "        fontsize=14,  bbox_to_anchor=(0.51, 0., 0.5, 0.5), title_fontsize=16)\n",
    "\n",
    "plt.title('How frequently countries sing in their official languages', fontsize=20, pad=30)\n",
    "\n",
    "plt.text(df_language.shape[0]-1, 1.1, 'Countries more likely to sing in Official language →', ha='right', va='center', fontsize=14)\n",
    "plt.text(0.05, 1.1, '← Countries less likely to sing in Official language ', ha='left', va='center',  fontsize=14)\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Ratio of performances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically those on the left of this plot are countries who sing in english but english is not their official language. Those on the right are countries who sing in their official language exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "sns.boxplot(x='Number_of_Languages', y='total_points', data=df_performance, showfliers=False)\n",
    "sns.stripplot(x='Number_of_Languages', y='total_points', data=df_performance, jitter=0.25, size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "plt.title('Points Scored by Number of Languages appearing in Song', fontsize=20, pad=30)\n",
    "plt.xlabel('Number of Languages')\n",
    "plt.ylabel('Total points')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# violin plot\n",
    "sns.violinplot(x='Contains_Voting_Language', y='points', data=df, inner=None, color=\"cornflowerblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "gender_df = df[['year', 'to_country', 'gender']].copy().drop_duplicates()\n",
    "\n",
    "countries = gender_df.sort_values(by=['to_country'])['to_country'].drop_duplicates().to_list()\n",
    "years = gender_df['year'].drop_duplicates().to_list()\n",
    "\n",
    "gender_colours = {\n",
    "    'female': to_rgba('seagreen'),\n",
    "    'group': to_rgba('wheat'),\n",
    "    'male': to_rgba('indianred'),\n",
    "    'none': to_rgba('white')\n",
    "}\n",
    "\n",
    "gender = []\n",
    "for year in years:\n",
    "    missing = pd.DataFrame({'year': [year] * len(countries), 'to_country': countries, 'gender': ['none'] * len(countries)}).set_index('to_country')\n",
    "    missing.update(gender_df[gender_df['year'] == year].set_index('to_country'))\n",
    "    gender.append(list(map(lambda x: gender_colours[x], missing.sort_values(by=['to_country'])['gender'].to_list())))\n",
    "\n",
    "gender = list(map(list, zip(*gender)))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 10)\n",
    "img = plt.imshow(gender, aspect=0.7)\n",
    "ax.set_xticks(range(len(years)))\n",
    "ax.set_xticklabels(years)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(list(map(str.title, countries)))\n",
    "ax.grid(False)\n",
    "plt.title('Gender of Artists at Eurovision by Year and Performing Country\\n(countries ordered alphabetically)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "for gender in ['female', 'male', 'group']:\n",
    "    count = len(gender_df[gender_df['gender'] == gender])\n",
    "    print('Total {} performers: {} ({:.0%})'.format(gender, count, count/len(gender_df)))\n",
    "\n",
    "def ratio(gender, x): return len([a for a in x if a == gender]) / len(x)\n",
    "\n",
    "gender_df = gender_df.groupby('to_country').agg(\n",
    "    female=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('female', x)),\n",
    "    group=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('group', x)),\n",
    "    male=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('male', x)),\n",
    ")\n",
    "gender_df = gender_df.sort_values(by=['female', 'group'], ascending=False)\n",
    "\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "gender_colours = {\n",
    "    'female': to_rgba('seagreen'),\n",
    "    'group': to_rgba('wheat'),\n",
    "    'male': to_rgba('indianred')#,\n",
    "    #'none': to_rgba('white')\n",
    "}\n",
    "\n",
    "gender_df[gender_colours.keys()].plot(kind='bar', figsize=(15, 6), stacked=True, color=gender_colours)\n",
    "plt.text(gender_df.shape[0] - 1, 1.1, 'Countries less likely to pitch female singers →', ha='right', va='center', fontsize=12)\n",
    "plt.text(0.05, 1.1, '← Countries more likely to pitch female singers', ha='left', va='center',  fontsize=12)\n",
    "plt.legend(['Female', 'Group', 'Male'], title=\"Performers' gender\", loc=[1, 1], fontsize=8,  bbox_to_anchor=(0.51, 0., 0.5, 0.5))\n",
    "plt.title('Gender of Artists at Eurovision by Performing Country', fontsize=16, pad=30)\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Ratio of performances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "gender_df = df[['year', 'to_code2', 'gender']].copy().drop_duplicates()\n",
    "\n",
    "def ratio(gender, x): return len([a for a in x if a == gender]) / len(x)\n",
    "\n",
    "gender_df = gender_df.groupby('year').agg(\n",
    "    female=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('female', x)),\n",
    "    group=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('group', x)),\n",
    "    male=pd.NamedAgg(column='gender', aggfunc=lambda x: ratio('male', x)),\n",
    ")\n",
    "\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# subset gender_colours to genders in gender_df columns\n",
    "# gender_colours = gender_colours[gender_df.columns.to_list()]\n",
    "\n",
    "gender_df.plot(kind='bar', figsize=(15, 6), stacked=True, color=gender_colours)\n",
    "\n",
    "plt.legend(['Female', 'Group', 'Male'], title=\"Performers' gender\", loc=[1, 1], fontsize=8,  bbox_to_anchor=(0.51, 0., 0.5, 0.5))\n",
    "plt.title('Gender of Artists at Eurovision by Year', fontsize=16, pad=30)\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Ratio of performances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "gender_df = df[['year', 'to_code2', 'gender', 'points']].copy()\n",
    "genders = list(gender_colours.keys())[:3]\n",
    "votes = []\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for gender in genders:\n",
    "    votes_df = gender_df[gender_df['gender'] == gender]['points']\n",
    "    votes.append(votes_df)\n",
    "    plt.axvline(votes_df.mean(), color=gender_colours[gender], linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.hist(votes, density=True, bins=range(14), color=list(gender_colours.values())[:3], label=[gender.title() for gender in genders], align='left')\n",
    "plt.xlabel('Points awarded')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Points Awarded Categorised by Gender\\n(Dashed line indicates mean score)')\n",
    "plt.legend()\n",
    "plt.xticks(range(13))\n",
    "ax.set_xticklabels([str(i) for i in range(13)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check if male get higher average votes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Visualisations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Heatmap for correlation plot\n",
    "- Martin's plot\n",
    "- Geographical plots\n",
    "- Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "df_performance = df[['year', 'Artist', 'to_country', \n",
    "       'total_points', 'rank', 'to_code2', \n",
    "       'Official_languages', 'Language_sung', 'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'prop_emigrants_v2p', 'prop_emigrants_p2v', 'has_border',\n",
    "       'comps_without_win',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'gender']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import matplotlib\n",
    "\n",
    "# Define the variables of interest and their data types\n",
    "vars_of_interest = {\n",
    "    'Contains_Own_Language': 'binary',\n",
    "    'Contains_Voting_Language': 'binary',\n",
    "    'Contains_English': 'binary',\n",
    "    'Contains_NonEnglish': 'binary',\n",
    "    'prop_emigrants_v2p': 'numeric', \n",
    "    'prop_emigrants_p2v': 'numeric', \n",
    "    'has_border': 'binary',\n",
    "    'gender': 'categorical', \n",
    "    'comps_without_win': 'numeric'\n",
    "}\n",
    "\n",
    "# Define the figure and axes\n",
    "nc = int(np.ceil(len(vars_of_interest)/2))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=nc, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# Loop through the dictionary\n",
    "for i, (key, value) in enumerate(vars_of_interest.items()):\n",
    "    j, k = 0, i\n",
    "    if i > (nc - 1):\n",
    "        j, k = 1, i - nc\n",
    "\n",
    "    if value == 'categorical':\n",
    "        sns.violinplot(ax=axes[j, k], x=key, y='points', data=df, color='tab:blue', inner=None, showmeans=True)\n",
    "    elif value == 'binary':\n",
    "        sns.violinplot(ax=axes[j, k], x=key, y='points', data=df, color='tab:blue', inner=None, showmeans=True)\n",
    "    else:\n",
    "        sns.regplot(ax=axes[j, k], x=key, y='points', data=df, ci=95, \n",
    "                    color='tab:blue', scatter_kws={'alpha': 0.4, 'edgecolor': 'none', 's': 20}, \n",
    "                    line_kws={'color': 'tab:orange'})\n",
    "\n",
    "        # if key contains Prop then log scale x axis\n",
    "        if key.startswith('prop'):\n",
    "            axes[j, k].set_xscale('log')\n",
    "            axes[j, k].set_xticks([0.01, 0.1, 1, 10, 100])\n",
    "            axes[j, k].get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "            axes[j, k].set_xlim(0.01, 1)\n",
    "\n",
    "    axes[j, k].set_ylim(-1, 13)\n",
    "    axes[j, k].set_xlabel(key.capitalize())\n",
    "    axes[j, k].set_ylabel('Votes')\n",
    "    axes[j, k].set_title(f'Votes vs. {key.capitalize()}')\n",
    "\n",
    "fig.suptitle('Relationships between Covariates and Eurovision Votes', fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.25)\n",
    "\n",
    "# Shuffle the bottom row along there are an odd number of variables\n",
    "if len(vars_of_interest) % 2 == 1:\n",
    "    axes[1][-1].remove()\n",
    "    # distance between two axes\n",
    "    if len(vars_of_interest) >= 3:\n",
    "        dist = axes[0][1].get_position().x0 - axes[0][0].get_position().x0\n",
    "        for ax in axes[1]:\n",
    "            pos = ax.get_position()\n",
    "            ax.set_position([pos.x0 + (dist / 2), pos.y0, pos.width, pos.height])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# plot comps_without_win vs. points\n",
    "sns.violinplot(x='points', y='comps_without_win', data=df)\n",
    "# add jitter\n",
    "sns.stripplot(x='points', y='comps_without_win', data=df, jitter=True, color='black', alpha=0.2)\n",
    "\n",
    "plt.title('Votes vs. Number of Competitions without a Win')\n",
    "plt.ylabel('Number of Competitions without a Win')\n",
    "plt.xlabel('Votes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['points', 'comps_without_win']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# One hot encoding for the gender variable\n",
    "df2 = pd.concat([df, pd.get_dummies(df['gender'])], axis=1)\n",
    "\n",
    "# correlation plot of numeric and binary variables\n",
    "df_corr = df2[['points', 'rank',\n",
    "       'total_points', 'Contains_English',\n",
    "       'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "       'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language',\n",
    "       'female', 'male', 'group',\n",
    "       'prop_emigrants_v2p',  'prop_emigrants_p2v', 'has_border',\n",
    "       'comps_without_win']].corr()\n",
    "\n",
    "# heatmap of the correlation matrix\n",
    "sns.heatmap(df_corr.corr(), cmap='coolwarm', annot=True, fmt='.2f', annot_kws={'fontsize': 7})\n",
    "\n",
    "# replace _ in labels with space\n",
    "labels = [label.replace('_', ' ').title() for label in df_corr.columns]\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation=-45, ha='left', va='top')\n",
    "plt.yticks(np.arange(len(labels)) + 0.5, labels, rotation=0, va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "\n",
    "Having explored all the data we have, we now turn to the question of using it to predict the results for Eurovision 2023.\n",
    "\n",
    "At the time that we carried out this modelling, Eurovision had not taken place yet. (You will see that the predictions here are *almost* the same as those described in [the Turing's public blog post](https://www.turing.ac.uk/blog/can-data-science-help-us-predict-winner-eurovision-2023), published before the competition!)\n",
    "\n",
    "However, since the competition has now concluded, we have also performed a retrospective analysis of how well our models performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Baseline model\n",
    "\n",
    "Arguably the zeroth rule of data science is to *keep it simple*.\\*\n",
    "In other words, if we can capture patterns in the data using a less complicated model, this should be preferred over a more complex one.\n",
    "For this reason, the first analysis we did was to simply extrapolate the predicted rank from each country's prior performance in Eurovision.\n",
    "\n",
    "To do so, we assign each performance a *rescaled rank* ranging from 0 (last place) to 1 (first place):\n",
    "\n",
    "$$r_{\\text{rescaled}} = \\frac{n - r}{n - 1},$$\n",
    "\n",
    "where $n$ is the number of participants in the given year, and $r$ is the actual numeric rank. We calculated the average rescaled rank (across all final entries) for every participant of Eurovision 2023, and used this to rank the entrants.\n",
    "\n",
    "The 2022 data were excluded from the training set; we used this as validation data.\n",
    "\n",
    "<sup>\\* 'Zeroth' because we are in a Python notebook, and thus zero-indexing is mandatory.</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(excluded_years=None):\n",
    "    if excluded_years is None:\n",
    "        excluded_years = []\n",
    "        \n",
    "    # Get maximum rank in each year (= number of countries participating)\n",
    "    df_rank_max = (df[['year', 'rank']]\n",
    "                .query('year not in @excluded_years')\n",
    "                .groupby('year')\n",
    "                .agg('max')\n",
    "                .rename({'rank': 'rank_max'}, axis=1)\n",
    "                )\n",
    "    \n",
    "    # Rescale rank to go from 0 = last place (rank = rank_max) to 1 = first place (rank = 1).\n",
    "    df_rank = (df[['to_country', 'year', 'rank']]\n",
    "                .query('year <= 2022')\n",
    "                .drop_duplicates()\n",
    "                .join(df_rank_max, how='left', on='year')\n",
    "                .assign(rescaled_rank=lambda x: (x['rank_max'] - x['rank'])/(x['rank_max'] - 1))\n",
    "                .groupby('to_country')\n",
    "                .agg({'rescaled_rank': 'mean'})\n",
    "                .sort_values('rescaled_rank', ascending=False)\n",
    "    )\n",
    "    \n",
    "    return df_rank\n",
    "    \n",
    "ranks_without_2022 = train_baseline(excluded_years=[2022])\n",
    "ranks_without_2022.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows the average rescaled rank for *all* countries observed in our dataset.\n",
    "To make predictions, we simply need to restrict the table entries to that year's participants.\n",
    "Let's see how well our baseline model does on predicting the 2022 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_baseline(rescaled_ranks, year):\n",
    "    if year <= 2022:\n",
    "        countries = df.query('year == @year')['to_country'].unique()\n",
    "    elif year == 2023:\n",
    "        countries = future['to_country'].unique()\n",
    "    predictions = (rescaled_ranks\n",
    "                   .query('to_country in @countries')\n",
    "                   .sort_values('rescaled_rank', ascending=False)\n",
    "                   .reset_index())\n",
    "    predictions['predicted_rank'] = predictions.index + 1\n",
    "    predictions = predictions.set_index('to_country')\n",
    "    return predictions\n",
    "    \n",
    "def get_actual_ranks(year):\n",
    "    return (df.query('year == @year')[['to_country', 'rank']]\n",
    "                .drop_duplicates()\n",
    "                .set_index('to_country')\n",
    "                .sort_values('rank'))\n",
    "                \n",
    "def join_predictions_and_actual(predicted_ranks, actual_ranks):\n",
    "    both = predicted_ranks.join(actual_ranks, validate='one_to_one').astype(int)\n",
    "    both = both.rename({'rank': 'actual_rank'}, axis=1)\n",
    "    return both[['predicted_rank', 'actual_rank']]\n",
    "    \n",
    "def get_spearman(predicted_ranks, actual_ranks):\n",
    "    both_ranks = join_predictions_and_actual(predicted_ranks, actual_ranks)\n",
    "    return both_ranks['predicted_rank'].corr(both_ranks['actual_rank'], method='spearman')\n",
    "    \n",
    "predictions_baseline = predict_baseline(ranks_without_2022, 2022)\n",
    "actual = get_actual_ranks(2022)\n",
    "both = join_predictions_and_actual(predictions_baseline, actual)\n",
    "\n",
    "print(f'Spearman correlation coefficient for 2022 is {get_spearman(predictions_baseline, actual):.3f}')\n",
    "both.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman coefficient is rather like its more famous cousin, the Pearson correlation, but is more suitable for ranked data.\n",
    "A value of 1 indicates that we have perfectly predicted the actual rankings, and -1 means that we have gotten it horribly wrong!\n",
    "\n",
    "In this respect, 0.239 does not look like a particularly impressive score.\n",
    "However, it should be borne in mind that Eurovision results can vary wildly from year to year, and perhaps a more valuable estimate of the accuracy of the model can be obtained by cross-validation.\n",
    "\n",
    "To do this, we train the model $N$ times (where $N = 24$ is the total number of years for which we have data), each time leaving out one year's worth of data, and using the model to predict that year's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Slovakia only has one final entry in our dataset, in 1998.\n",
    "## So, when the model is 'trained' on all data excluding 1998, Slovakia doesn't get a rescaled_rank as there is no data to work with.\n",
    "## The 1998 predictions therefore don't include Slovakia (the Spearman coefficient is calculated based on all other entrants in 1998).\n",
    "\n",
    "all_years = df['year'].unique()\n",
    "\n",
    "def get_spearman_baseline_without(year):\n",
    "    rescaled_ranks = train_baseline(excluded_years=[year])\n",
    "    predictions_baseline = predict_baseline(rescaled_ranks, year)\n",
    "    actual = get_actual_ranks(year)\n",
    "    # Remove Slovakia from 1998\n",
    "    if year == 1998:\n",
    "        predictions_baseline = predictions_baseline.query('to_country != \"slovakia\"')\n",
    "        actual = actual.query('to_country != \"slovakia\"')\n",
    "    return(get_spearman(predictions_baseline, actual))\n",
    "    \n",
    "spearmans = [get_spearman_baseline_without(year) for year in all_years]\n",
    "\n",
    "print(f'Mean Spearman coefficient across {len(all_years)} years: {np.mean(spearmans)}')\n",
    "\n",
    "df_xv = pd.DataFrame({'year': all_years, 'spearman': spearmans})\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(df_xv['year'], df_xv['spearman'], color='#6495ed')\n",
    "ax.set(xlabel='Year', ylabel='Spearman coefficient', title='Cross-validation: baseline method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that *on average*, the historical results of a country at Eurovision are a half-decent indicator of how well they will perform in any given year.\n",
    "Of course, this is hardly a surprise.\n",
    "Interestingly, we can pick out years with lower Spearman correlations as being 'anomalies' in this regard, where an underdog won.\n",
    "The year with the lowest value (that's 2002) featured Latvia as the winner, and perhaps even more surprisingly, the UK finished third ([Jessica Garlick with *'Come Back'*](https://www.youtube.com/watch?v=2NSR5rSk0Cs)).\n",
    "When we write 'surprisingly', this is emphatically *not* us inserting our personal opinion: the baseline model in fact really dislikes the UK, assigning it the third-lowest average rescaled rank of all countries.\n",
    "Thus, any year where the UK performs well is a surprise to our model!\n",
    "\n",
    "Having looked at how good our model is at reproducing its own data, let's see what happens when we predict the 2023 results (now including all data from 1998 to 2022, inclusive, in our training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2023 = predict_baseline(train_baseline(), 2023)\n",
    "\n",
    "n = len(predictions_2023)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 8))\n",
    "ax.barh(width=predictions_2023['rescaled_rank'], y=np.arange(n), color='#4878D0')\n",
    "ax.set(\n",
    "    xlim=(0, predictions_2023['rescaled_rank'].max() * 1.1),\n",
    "    ylim=(-1, n),\n",
    "    yticks=np.arange(n),\n",
    "    yticklabels=predictions_2023.index.values,\n",
    "    xlabel='Average rescaled rank across all final appearances',\n",
    "    title='Baseline model: predicting 2023 results based on historical performance'\n",
    ")\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=11)\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model thus predicted Italy to come first, Sweden second, and Ukraine third, simply on the basis of them performing historically well at Eurovision.\n",
    "\n",
    "In 2023, the top three countries were Sweden (with [Loreen's *Tattoo*](https://www.youtube.com/watch?v=b3vJfR81xO0)), Finland (with [Käärijä's *Cha Cha Cha*](https://www.youtube.com/watch?v=znWi3zN8Ucg)), and Israel (with [Noa Kirel's *Unicorn*](https://www.youtube.com/watch?v=r4wbdKmM3bQ)).\n",
    "The full results of the final were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ranks_2023 = (future.loc[~np.isnan(future['rank'])]\n",
    "                     .drop_duplicates(subset=['to_country'])\n",
    "                     .loc[:, ['to_country', 'rank']]\n",
    "                     .set_index('to_country'))\n",
    "\n",
    "actual_ranks_2023.sort_values(by='rank').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the benefit of this hindsight, we can therefore calculate the Spearman correlation coefficient for 2023.\n",
    "The Spearman coefficient is calculated only for countries which made the finals, since contestants eliminated in the semi-finals cannot really be assigned a 'rank'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "\n",
    "finalists_2023 = actual_ranks_2023.index.unique()\n",
    "predictions_2023_finals = predictions_2023.query('to_country in @finalists_2023')\n",
    "\n",
    "print(f'Spearman correlation for 2023: {get_spearman(predictions_2023_finals, actual_ranks_2023):.3f}')\n",
    "\n",
    "both_2023 = join_predictions_and_actual(predictions_2023_finals, actual_ranks_2023)\n",
    "ax = both_2023.plot(kind='scatter', x='predicted_rank', y='actual_rank', figsize=(4, 4), color='#6495ed')\n",
    "ax.set(xlabel='Predicted rank (by baseline model)', ylabel='Actual rank', title='Baseline model: 2023 predictions vs. actual results')\n",
    "texts = []\n",
    "for i, txt in enumerate(both_2023.index):\n",
    "    t = ax.text(s=txt, x=both_2023.iloc[i]['predicted_rank'], y=both_2023.iloc[i]['actual_rank'],\n",
    "                ha='center', va='center', fontsize=8)\n",
    "    texts.append(t)\n",
    "ax.invert_xaxis(); ax.invert_yaxis()\n",
    "adjust_text(texts, avoid_self=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Round everything up, draw conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
